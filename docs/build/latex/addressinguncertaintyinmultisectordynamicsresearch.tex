%% Generated by Sphinx.
\def\sphinxdocclass{book}
\documentclass[letterpaper,10pt,english]{book}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsable pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

% Jupyter Notebook code cell colors
\definecolor{nbsphinxin}{HTML}{307FC1}
\definecolor{nbsphinxout}{HTML}{BF5B3D}
\definecolor{nbsphinx-code-bg}{HTML}{F5F5F5}
\definecolor{nbsphinx-code-border}{HTML}{E0E0E0}
\definecolor{nbsphinx-stderr}{HTML}{FFDDDD}
% ANSI colors for output streams and traceback highlighting
\definecolor{ansi-black}{HTML}{3E424D}
\definecolor{ansi-black-intense}{HTML}{282C36}
\definecolor{ansi-red}{HTML}{E75C58}
\definecolor{ansi-red-intense}{HTML}{B22B31}
\definecolor{ansi-green}{HTML}{00A250}
\definecolor{ansi-green-intense}{HTML}{007427}
\definecolor{ansi-yellow}{HTML}{DDB62B}
\definecolor{ansi-yellow-intense}{HTML}{B27D12}
\definecolor{ansi-blue}{HTML}{208FFB}
\definecolor{ansi-blue-intense}{HTML}{0065CA}
\definecolor{ansi-magenta}{HTML}{D160C4}
\definecolor{ansi-magenta-intense}{HTML}{A03196}
\definecolor{ansi-cyan}{HTML}{60C6C8}
\definecolor{ansi-cyan-intense}{HTML}{258F8F}
\definecolor{ansi-white}{HTML}{C5C1B4}
\definecolor{ansi-white-intense}{HTML}{A1A6B2}
\definecolor{ansi-default-inverse-fg}{HTML}{FFFFFF}
\definecolor{ansi-default-inverse-bg}{HTML}{000000}

% Define an environment for non-plain-text code cell outputs (e.g. images)
\makeatletter
\newenvironment{nbsphinxfancyoutput}{%
    % Avoid fatal error with framed.sty if graphics too long to fit on one page
    \let\sphinxincludegraphics\nbsphinxincludegraphics
    \nbsphinx@image@maxheight\textheight
    \advance\nbsphinx@image@maxheight -2\fboxsep   % default \fboxsep 3pt
    \advance\nbsphinx@image@maxheight -2\fboxrule  % default \fboxrule 0.4pt
    \advance\nbsphinx@image@maxheight -\baselineskip
\def\nbsphinxfcolorbox{\spx@fcolorbox{nbsphinx-code-border}{white}}%
\def\FrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\@empty}%
\def\FirstFrameCommand{\nbsphinxfcolorbox\nbsphinxfancyaddprompt\sphinxVerbatim@Continues}%
\def\MidFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\sphinxVerbatim@Continues}%
\def\LastFrameCommand{\nbsphinxfcolorbox\sphinxVerbatim@Continued\@empty}%
\MakeFramed{\advance\hsize-\width\@totalleftmargin\z@\linewidth\hsize\@setminipage}%
\lineskip=1ex\lineskiplimit=1ex\raggedright%
}{\par\unskip\@minipagefalse\endMakeFramed}
\makeatother
\newbox\nbsphinxpromptbox
\def\nbsphinxfancyaddprompt{\ifvoid\nbsphinxpromptbox\else
    \kern\fboxrule\kern\fboxsep
    \copy\nbsphinxpromptbox
    \kern-\ht\nbsphinxpromptbox\kern-\dp\nbsphinxpromptbox
    \kern-\fboxsep\kern-\fboxrule\nointerlineskip
    \fi}
\newlength\nbsphinxcodecellspacing
\setlength{\nbsphinxcodecellspacing}{0pt}

% Define support macros for attaching opening and closing lines to notebooks
\newsavebox\nbsphinxbox
\makeatletter
\newcommand{\nbsphinxstartnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vtop{{#1\par}}
    % reserve some space at bottom of page, else start new page
    \needspace{\dimexpr2.5\baselineskip+\ht\nbsphinxbox+\dp\nbsphinxbox}
    % mimick vertical spacing from \section command
      \addpenalty\@secpenalty
      \@tempskipa 3.5ex \@plus 1ex \@minus .2ex\relax
      \addvspace\@tempskipa
      {\Large\@tempskipa\baselineskip
             \advance\@tempskipa-\prevdepth
             \advance\@tempskipa-\ht\nbsphinxbox
             \ifdim\@tempskipa>\z@
               \vskip \@tempskipa
             \fi}
    \unvbox\nbsphinxbox
    % if notebook starts with a \section, prevent it from adding extra space
    \@nobreaktrue\everypar{\@nobreakfalse\everypar{}}%
    % compensate the parskip which will get inserted by next paragraph
    \nobreak\vskip-\parskip
    % do not break here
    \nobreak
}% end of \nbsphinxstartnotebook

\newcommand{\nbsphinxstopnotebook}[1]{%
    \par
    % measure needed space
    \setbox\nbsphinxbox\vbox{{#1\par}}
    \nobreak % it updates page totals
    \dimen@\pagegoal
    \advance\dimen@-\pagetotal \advance\dimen@-\pagedepth
    \advance\dimen@-\ht\nbsphinxbox \advance\dimen@-\dp\nbsphinxbox
    \ifdim\dimen@<\z@
      % little space left
      \unvbox\nbsphinxbox
      \kern-.8\baselineskip
      \nobreak\vskip\z@\@plus1fil
      \penalty100
      \vskip\z@\@plus-1fil
      \kern.8\baselineskip
    \else
      \unvbox\nbsphinxbox
    \fi
}% end of \nbsphinxstopnotebook

% Ensure height of an included graphics fits in nbsphinxfancyoutput frame
\newdimen\nbsphinx@image@maxheight % set in nbsphinxfancyoutput environment
\newcommand*{\nbsphinxincludegraphics}[2][]{%
    \gdef\spx@includegraphics@options{#1}%
    \setbox\spx@image@box\hbox{\includegraphics[#1,draft]{#2}}%
    \in@false
    \ifdim \wd\spx@image@box>\linewidth
      \g@addto@macro\spx@includegraphics@options{,width=\linewidth}%
      \in@true
    \fi
    % no rotation, no need to worry about depth
    \ifdim \ht\spx@image@box>\nbsphinx@image@maxheight
      \g@addto@macro\spx@includegraphics@options{,height=\nbsphinx@image@maxheight}%
      \in@true
    \fi
    \ifin@
      \g@addto@macro\spx@includegraphics@options{,keepaspectratio}%
    \fi
    \setbox\spx@image@box\box\voidb@x % clear memory
    \expandafter\includegraphics\expandafter[\spx@includegraphics@options]{#2}%
}% end of "\MakeFrame"-safe variant of \sphinxincludegraphics
\makeatother

\makeatletter
\renewcommand*\sphinx@verbatim@nolig@list{\do\'\do\`}
\begingroup
\catcode`'=\active
\let\nbsphinx@noligs\@noligs
\g@addto@macro\nbsphinx@noligs{\let'\PYGZsq}
\endgroup
\makeatother
\renewcommand*\sphinxbreaksbeforeactivelist{\do\<\do\"\do\'}
\renewcommand*\sphinxbreaksafteractivelist{\do\.\do\,\do\:\do\;\do\?\do\!\do\/\do\>\do\-}
\makeatletter
\fvset{codes*=\sphinxbreaksattexescapedchars\do\^\^\let\@noligs\nbsphinx@noligs}
\makeatother



\title{\textbf{Addressing Uncertainty in MultiSector Dynamics Research}}
\date{Sep 07, 2021}
\release{v0.1.0}
\author{Patrick M. Reed, Antonia Hadjimichael, Keyvan Malek\and Tina Karimi, Chris R. Vernon, Vivek Srikrishnan\and Rohini Gupta, David Gold, B. Lee, Klaus Keller, Jennie S. Rice}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Introduction}
\label{\detokenize{1_introduction:introduction}}\label{\detokenize{1_introduction:id1}}\label{\detokenize{1_introduction::doc}}
\sphinxAtStartPar
This guidance text has been developed in support of the Integrated Multisector Multiscale Modeling (IM3) Science Focus Area’s objective to formally integrate uncertainty into its research tasks. IM3 is focused on innovative modeling to explore how human and natural system landscapes in the United States co\sphinxhyphen{}evolve in response to short\sphinxhyphen{}term shocks and long\sphinxhyphen{}term influences. The project’s challenging scope is to advance our ability to study the interactions between energy, water, land, and urban systems, at scales ranging from local (\textasciitilde{}1km) to the contiguous United States, while consistently addressing influences such as population change, technology change, heat waves, and drought. Uncertainty and careful model\sphinxhyphen{}driven scientific insights are central to IM3’s key MultiSector Dynamics (MSD) science objectives shown below.

\sphinxAtStartPar
\sphinxstylestrong{IM3 key MSD science objectives include:}

\sphinxAtStartPar
\sphinxstyleemphasis{Develop flexible, open\sphinxhyphen{}source, and integrated modeling capabilities that capture the structure, dynamic behavior, and emergent properties of the multiscale interactions within and between human and natural systems.}

\sphinxAtStartPar
\sphinxstyleemphasis{Use these capabilities to study the evolution, vulnerability, and resilience of interacting human and natural systems and landscapes from local to continental scales, including their responses to the compounding effects of long\sphinxhyphen{}term influences and short\sphinxhyphen{}term shocks.}

\sphinxAtStartPar
\sphinxstyleemphasis{Understand the implications of uncertainty in data, observations, models, and model coupling approaches for projections of human\sphinxhyphen{}natural system dynamics.}

\sphinxAtStartPar
Addressing the objectives above poses a strong transdisciplinary challenge that heavily depends on a diversity of models and, more specifically, a consistent framing for making model\sphinxhyphen{}based science inferences. The term transdisciplinary science as used here formally implies a deep integration of disciplines to aid our hypothesis\sphinxhyphen{}driven understanding of coupled human\sphinxhyphen{}natural systems\textendash{}bridging differences in theory, hypothesis generation, modeling, and modes of inference {[}\hyperlink{cite.index:id2}{1}{]}. The IM3 MSD research focus and questions require a deep integration across disciplines, where new modes of analysis can emerge that rapidly synthesize and exploit advances for making decision\sphinxhyphen{}relevant insights that at minimum acknowledge uncertainty and more ideally promote a rigorous quantitative mapping of its effects on the generality of claimed scientific insights. More broadly, diverse scientific disciplines engaged in the science of coupled human\sphinxhyphen{}natural systems, ranging from natural sciences to engineering and economics, employ a diversity of numerical computer models to study and understand their underlying systems of focus. The utility of these computer models hinges on their ability to represent the underlying real systems with sufficient fidelity and enable the inference of novel insights. This is particularly challenging in the case of coupled human\sphinxhyphen{}natural systems where there exists a multitude of interdependent human and natural processes taking place that could potentially be represented. These processes usually translate into modeled representations that are highly complex, non\sphinxhyphen{}linear, and exhibit strong interactions and threshold behaviors {[}\hyperlink{cite.index:id3}{2}, \hyperlink{cite.index:id4}{3}, \hyperlink{cite.index:id5}{4}{]}. Model complexity and detail have also been increasing as a result of our improving understanding of these processes, the availability of data, and the rapid growth in computing power {[}\hyperlink{cite.index:id6}{5}{]}. As model complexity grows, modelers need to specify a lot more information than before: additional model inputs and relationships as more processes are represented, higher resolution data as more observations are collected, new coupling relationships and interactions as models are put together to answer multisector questions (e.g., the land\sphinxhyphen{}water\sphinxhyphen{}energy nexus). Typically, not all of this information is well known, nor is the impact of these many uncertainties on model outputs well understood. It is further especially difficult to distinguish the effects of individual as well as interacting sources of uncertainty when modeling coupled systems with multisector and multiscale dynamics {[}\hyperlink{cite.index:id7}{6}{]}.

\sphinxAtStartPar
Given the challenge and opportunity posed by the disciplinary diversity of IM3, we utilized a team\sphinxhyphen{}wide survey to allow the project’s membership to provide their views on how their areas typically address uncertainty, emphasizing key literature examples and domain\sphinxhyphen{}specific reviews. Our synthesis of this survey information in \hyperref[\detokenize{1_introduction:figure-1-1}]{Fig.\@ \ref{\detokenize{1_introduction:figure-1-1}}} summaries the team’s perspectives, enabling a summary of the commonalities and differences for how different disciplinary areas are typically addressing uncertainty. \hyperref[\detokenize{1_introduction:figure-1-1}]{Fig.\@ \ref{\detokenize{1_introduction:figure-1-1}}} highlights the non\sphinxhyphen{}trivial challenge posed by seeking to carefully consider uncertainty across an MSD focused transdisciplinary team. There are significant differences across the team’s contributing disciplines in terms of the methodological approaches and tools used in the treatment of uncertainty. The horizontal axis of the figure represents a conceptual continuum of methodological approaches, ranging from deterministic (no uncertainty) modeling to the theoretical case of fully engaging in modeling all sources of uncertainty. The vertical axis of plot maps the analysis tools that are used in the disciplines’ literature, spanning error\sphinxhyphen{}driven historical analyses to full uncertainty quantification. Given that \hyperref[\detokenize{1_introduction:figure-1-1}]{Fig.\@ \ref{\detokenize{1_introduction:figure-1-1}}} is a conceptual illustration, the mapping of each discipline’s boundaries is not meant to imply exactness. They encompass the scope of feedback attained in the team\sphinxhyphen{}wide survey responses. The color circles designate specific sources of uncertainty that could be considered. Within the mapped disciplinary approaches, the color circles distinguish those sources of uncertainty that are addressed in the bodies of literature reported by respondents. Note the complete absence of grey circles designating that, at present, few if any studies report results for understanding how model coupling relationships shape uncertainty. We can briefly distinguish the key terms of uncertainty quantification (UQ) and uncertainty characterization (UC). UQ refers to the formal focus on the full specification of likelihoods as well as distributional forms necessary to infer the joint probabilistic response across all modeled factors of interest {[}\hyperlink{cite.index:id11}{7}{]}. Alternatively, uncertainty characterization as defined here, refers to exploratory modeling of alternative hypotheses for the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path dependent changes in the form and function of modelled systems {[}\hyperlink{cite.index:id12}{8}, \hyperlink{cite.index:id13}{9}{]}. Uncertain factors are any model component which is affected by uncertainty: inputs, resolution levels, coupling relationships, model relationships and parameters. When a model has been established as a sufficiently accurate representation of the system some of these factors may reflect elements of the real\sphinxhyphen{}world system that the model represents (for example, a population level parameter would reflect a sufficiently accurate representation of the population level in the system under study). As discussed in later sections, the choice of UQ or UC depends on the specific goals of studies, the availability of data, the types of uncertainties (e.g., well\sphinxhyphen{}characterized or deep), the complexity of underlying models as well as the computational limits. Deep uncertainty (as opposed to well\sphinxhyphen{}characterized) refers to situations where expert opinions consulted on a decision do not know or cannot agree on system boundaries, or the outcomes of interest and their relative importance, or the prior probability distribution for the various uncertain factors present {[}\hyperlink{cite.index:id8}{10}, \hyperlink{cite.index:id9}{11}{]}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure1_1_state_of_the_science}.png}
\caption{State\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art in different modeling communities, as reported in the survey distributed to IM3 teams. \sphinxstyleemphasis{Deterministic Historical Evaluation}: model evaluation under fully determined conditions defined using historical observations; \sphinxstyleemphasis{Local Sensitivity Analysis}: model evaluation performed by varying uncertain factors around specific reference values; \sphinxstyleemphasis{Global Sensitivity Analysis}: model evaluation performed by varying uncertain factors throughout their entire feasible value space; \sphinxstyleemphasis{Uncertainty Characterization}: model evaluation under alternative factor hypotheses to explore their implications for model output uncertainty; \sphinxstyleemphasis{Uncertainty Quantification}: representation of model output uncertainty using probability distributions; \sphinxstyleemphasis{Traditional statistical inference}: use of analysis results to describe deterministic or probabilistic outcomes resulting from the presence of uncertainty; \sphinxstyleemphasis{Narrative scenarios}: use of a limited decision\sphinxhyphen{}relevant number of scenarios to describe (sets of) changing system outcomes; \sphinxstyleemphasis{Exploratory modeling for scenario discovery}: use of large ensembles of uncertain conditions to discover decision\sphinxhyphen{}relevant combinations of uncertain factors}\label{\detokenize{1_introduction:id10}}\label{\detokenize{1_introduction:figure-1-1}}\end{figure}

\sphinxAtStartPar
At present, there is no singular guide for confronting the computational and conceptual challenges of the multi\sphinxhyphen{}model, transdisciplinary workflows that characterize ambitious projects such as IM3 {[}\hyperlink{cite.index:id10}{12}{]}. The primary aim of this text is to begin to address this gap and provide guidance for facing these challenges. \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:diagnostic-modeling}]{Section \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:diagnostic-modeling}}} provides an overview of diagnostic modeling and the different perspectives for how we should evaluate our models, \hyperref[\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}}} summarizes basic methods and concepts for sensitivity analysis, and \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis}]{Section \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis}}} delves into more technical applications of sensitivity analysis to support diagnostic model evaluation and exploratory modeling. Finally, \hyperref[\detokenize{5_conclusion:conclusion}]{Section \ref{\detokenize{5_conclusion:conclusion}}} provides some concluding remarks across the UC and UQ topics covered in this text. The appendices of this text include a glossary of the key concepts as well as an overview of UQ methods.


\chapter{Diagnostic Modeling Overview and Perspectives}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:diagnostic-modeling-overview-and-perspectives}}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:diagnostic-modeling}}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives::doc}}
\sphinxAtStartPar
This text prescribes a formal model diagnostic approach to IM3 computational experimentation that is a deliberative and iterative combination of state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art UC and global sensitivity analysis techniques that progresses from observed history\sphinxhyphen{}based fidelity evaluations to forward looking resilience and vulnerability inferences {[}\hyperlink{cite.index:id46}{13}, \hyperlink{cite.index:id47}{14}{]}.


\section{Overview of model diagnostics}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:overview-of-model-diagnostics}}
\sphinxAtStartPar
Model diagnostics provide a rich basis for hypothesis testing, model innovation, and improved inferences when classifying what is controlling highly consequential results (e.g., vulnerability or resilience in coupled human\sphinxhyphen{}natural systems). \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}]{Fig.\@ \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}}}, adapted from {[}\hyperlink{cite.index:id6}{5}{]}, presents idealized illustrations of the relationship between UC and global sensitivity analysis (GSA) for two coupled simulation models. The figure illustrates how UC can be used to address how uncertainties in various modeling decisions (data inputs, parameters, model structures, coupling relationships, and elsewhere) can be sampled and simulated to yield the empirical model output distribution(s) of interest. Monte Carlo frameworks allow us to sample and propagate (or integrate) the ensemble response of the model(s) of focus. The first step of any UC analysis is the specification of the initial input distributions as illustrated in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}]{Fig.\@ \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}}}. The second step is to perform the Monte Carlo simulations. The question can then be raised, which of the modeling assumptions in our Monte Carlo experiment are the most responsible for the resulting output uncertainty. We can answer this question using “global sensitivity analysis” (GSA) as illustrated in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}]{Fig.\@ \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}}}. GSA can be defined as a formal Monte Carlo sampling and analysis of modeling choices (structures, parameters, inputs) to quantify their influence on direct model outputs (or output\sphinxhyphen{}informed metrics). UC experiments by themselves do not explain why a particular uncertain outcome is produced, but produce distributions of model outcomes, as portrayed by the yellow curve. The pie chart shown in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}]{Fig.\@ \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}}} is a conceptual representation of the results of GSA to identify those factors that are most dominantly influencing results, either individually or interactively {[}\hyperlink{cite.index:id48}{15}{]}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure2_1_idealized_uc}.png}
\caption{Idealized uncertainty characterization and global sensitivity analysis for two coupled simulation models. Uncertainty coming from various sources (inputs, model structures, coupling relationships, and elsewhere) is propagated through the coupled model(s) to generate empirical distributions of outputs of interest (uncertainty characterization). This model output uncertainty can be decomposed to its origins, by means of sensitivity analysis. Figure adapted from Saltelli \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id6}{5}{]}.}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:id22}}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:figure-2-1}}\end{figure}

\sphinxAtStartPar
UC and GSA are not independent modeling analyses. As illustrated here, any GSA requires an initial UC hypothesis in the form of statistical assumptions and representations for the modeling choices of focus (structural, parametric, and data inputs). Information from these two model diagnostic tools can then be used to inform data needs for future model runs, experiments to reduce the uncertainty present, or the simplification or enhancement of the model where necessary. Together UC and GSA provide a foundation for diagnostic exploratory modeling that has a consistent focus on the assumptions, structural model forms, alternative parameterizations, and input data sets that are used to characterize the behavioral space of one or more models.


\section{Perspectives on diagnostic model evaluation}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives-on-diagnostic-model-evaluation}}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}}
\sphinxAtStartPar
When we judge or diagnose models the terms “verification” and “validation” are commonly used. However, their appropriateness in the context of numerical models representing complex coupled human\sphinxhyphen{}natural systems is questionable {[}\hyperlink{cite.index:id54}{16}, \hyperlink{cite.index:id118}{17}{]}. The core issue relates to the fact that these systems are often not fully known or perfectly implemented when modeled. Rather, they are defined within specific system framings and boundary conditions in an evolving learning process with the goal of making continual progress towards attaining higher levels of fidelity. For example, observations used to evaluate the fidelity of parameterized processes are often measured at a finer resolution than is represented in the model and then must be scaled up for the evaluation. In other cases, numerical models may neglect or simplify system processes because the data is not available or the physical mechanisms are not fully known. If sufficient agreement between prediction and observation is not achieved, it is challenging to know whether these types of modeling choices are the cause, or if other issues, such as deficiencies in the input parameters and/or other modeling assumptions are the true cause of errors. Even if there is high agreement between prediction and observation, the model cannot necessarily be considered validated, as it is always possible that the right values were produced for the wrong reasons. For example, low error can stem from a situation where different errors in underlying assumptions or parameters cancel each other out (“compensatory errors”). Furthermore, coupled human\sphinxhyphen{}natural system models are often subject to “equifinality”, a situation where multiple parameterized formulations can produce similar outputs or equally acceptable representations of the observed data. There is therefore no uniquely “true” or validated model, and the common practice of selecting “the best” deterministic calibration set is more of an assumption than a finding {[}\hyperlink{cite.index:id49}{18}, \hyperlink{cite.index:id38}{19}{]}. The situation becomes even more tenuous when observational data is limited in its scope and/or quality to be insufficient to distinguish model representations or their performance differences.

\sphinxAtStartPar
These limitations on model verification undermine any purely positivist treatment of model validity: that a model should correctly and precisely represent reality to be valid. Under this perspective, closely related to empiricism, statistical tests should be used to compare the model’s output with observations and only through empirical verification can a model or theory be deemed credible. A criticism to this viewpoint (besides the aforementioned challenges for model verification) is that it reduces the justification of a model to the single criterion of predictive ability and accuracy {[}\hyperlink{cite.index:id119}{20}{]}. Authors have argued that this ignores the explanatory power held in models and other procedures, which can also advance scientific knowledge {[}\hyperlink{cite.index:id120}{21}{]}. These views gave rise to relativist perspectives of science, which instead place more value on model utility in terms of fitness for a specific purpose or inquiry, rather than representational accuracy and predictive ability {[}\hyperlink{cite.index:id121}{22}{]}. This viewpoint appears to be most prevalent among practitioners seeking decision\sphinxhyphen{}relevant insights (i.e., inspire new views vs. predict future conditions). The relativist perspective argues for the use of models as heuristics that can enhance our understanding and conceptions of system behaviors or possibilities {[}\hyperlink{cite.index:id64}{23}{]}. In contrast, natural sciences favor a positivist perspective, emphasizing similarity between simulation and observation even in application contexts where it is clear that projections are being made for conditions that have never been observed and the system of focus will have evolved structurally beyond the model representation being employed (e.g., decadal to centennial evolution of human\sphinxhyphen{}natural systems).

\sphinxAtStartPar
These differences in prevalent perspectives are mirrored in how model validation is defined by the two camps: From the relativist perspective, validation is seen as a process of incremental “confidence building” in a model as a mechanism for insight {[}\hyperlink{cite.index:id122}{24}{]}, whereas in natural sciences validation is framed as a way to classify a model as having an acceptable representation of physical reality {[}\hyperlink{cite.index:id118}{17}{]}. Even though the relativist viewpoint does not dismiss the importance of representational accuracy, it does place it within a larger process of establishing confidence through a variety of tools. These tools, not necessarily quantitative, include communicating information between practitioners and modelers, interpreting a multitude of model outputs, and contrasting preferences and viewpoints.

\sphinxAtStartPar
On the technical side of the argument, differing views on the methodology of model validation appear as early as in the 1960’s. Naylor and Finger {[}\hyperlink{cite.index:id123}{25}{]} argue that model validation should not be limited to a single metric or test of performance (e.g., a single error metric), but should rather be extended to multiple tests that reflect different aspects of a model’s structure and behavior. This and similar arguments are made in literature to this day {[}\hyperlink{cite.index:id46}{13}, \hyperlink{cite.index:id52}{26}, \hyperlink{cite.index:id51}{27}, \hyperlink{cite.index:id124}{28}, \hyperlink{cite.index:id125}{29}{]} and are primarily founded on two premises. First, that even though modelers widely recognize that their models are abstractions of the truth, they still make truth claims based on traditional performance metrics that measure the divergence of their model from observation {[}\hyperlink{cite.index:id125}{29}{]}. Second, that the natural systems mimicked by the models contain many processes that exhibit significant heterogeneity at various temporal and spatial scales. This heterogeneity is lost when a single performance measure is used, as a result of the inherent loss of process information occurring when transitioning from a highly dimensional and interactive system to the dimension of a single metric {[}\hyperlink{cite.index:id54}{16}{]}. These arguments are further elaborated in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis}]{Section \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis}}}.

\sphinxAtStartPar
Multiple authors have proposed that the traditional reliance on single measures of model performance should be replaced by the evaluation of several model signatures (characteristics) to identify model structural errors and achieve a sufficient assessment of model performance {[}\hyperlink{cite.index:id46}{13}, \hyperlink{cite.index:id126}{30}, \hyperlink{cite.index:id168}{31}, \hyperlink{cite.index:id167}{32}{]}. There is however a point of departure here, especially when models are used to produce inferences that can inform decisions. When agencies and practitioners use models of their systems for public decisions, those models have already met sufficient conditions for credibility (e.g., acceptable representational fidelity), but may face broader tests on their salience and legitimacy in informing negotiated decisions {[}\hyperlink{cite.index:id64}{23}, \hyperlink{cite.index:id127}{33}, \hyperlink{cite.index:id128}{34}{]}. This presents a new challenge to model validation, that of selecting decision\sphinxhyphen{}relevant performance metrics, reflective of the system’s stakeholders’ viewpoints, so that the most consequential uncertainties are identified and addressed {[}\hyperlink{cite.index:id129}{35}{]}. For complex multisector models at the intersection of climatic, hydrologic, agricultural, energy, or other processes, the output space is made up of a multitude of states and variables, with very different levels of salience to the system’s stakeholders and to their goals being achieved. This is further complicated when such systems are also institutionally and dynamically complex. As a result, a broader set of qualitative and quantitative performance metrics is necessary to evaluate models of such complex systems, one that embraces the plurality of value systems, agencies and perspectives present. For IM3, even though the goal is to develop better projections of future vulnerability and resilience in co\sphinxhyphen{}evolving human\sphinxhyphen{}natural systems and not to provide decision support per se, it is critical for our multisector, multiscale model evaluation processes to represent stakeholders’ adaptive decision processes credibly.

\sphinxAtStartPar
As a final point, when a model is used in a projection mode, its results are also subject to additional uncertainty, as there is no guarantee that the model’s functionality and predictive ability will stay the same as the baseline, where the verification and validation tests were conducted. This challenge requires an additional expansion of the scope of model evaluation: a broader set of uncertain conditions needs to be explored, spanning beyond historical observation and exploring a wide range of unprecedented conditions. This perspective on modeling, termed exploratory {[}\hyperlink{cite.index:id15}{36}{]}, views models as computational experiments that can be used to explore vast ensembles of potential scenarios to identify those with consequential effects. Exploratory modeling literature explicitly orients experiments toward stakeholder consequences and decision\sphinxhyphen{}relevant inferences and shifts the focus from predicting future conditions to \sphinxstyleemphasis{discovering} which conditions lead to undesirable or desirable consequences.

\sphinxAtStartPar
This evolution in modeling perspectives can be mirrored by the IM3 family of models in a progression from evaluating models relative to observed history to advanced formalized analyses to make inferences on multisector, multiscale vulnerabilities and resilience. Exploratory modeling approaches can help fashion experiments with large numbers of alternative hypotheses on the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path\sphinxhyphen{}dependent changes in the form and function of human\sphinxhyphen{}natural systems {[}\hyperlink{cite.index:id130}{37}{]}. The aim of this text is to therefore guide the reader through the use of sensitivity analysis (SA) methods across these perspectives on diagnostic and exploratory modeling.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{The} \PYG{n}{following} \PYG{n}{contributions} \PYG{n}{are} \PYG{n}{suggested} \PYG{k}{as} \PYG{n}{fundamental} \PYG{n}{reading} \PYG{k}{for} \PYG{n}{the} \PYG{n}{information} \PYG{n}{presented} \PYG{o+ow}{in} \PYG{n}{this} \PYG{n}{section}\PYG{p}{:}
\PYG{o}{*} \PYG{n}{Naomi} \PYG{n}{Oreskes}\PYG{p}{,} \PYG{n}{Kristin} \PYG{n}{Shrader}\PYGZbs{}\PYG{n}{sphinxhyphen}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{n}{Frechette}\PYG{p}{,} \PYG{o+ow}{and} \PYG{n}{Kenneth} \PYG{n}{Belitz}\PYG{o}{.} \PYG{n}{Verification}\PYG{p}{,} \PYG{n}{Validation}\PYG{p}{,} \PYG{o+ow}{and} \PYG{n}{Confirmation} \PYG{n}{of} \PYG{n}{Numerical} \PYG{n}{Models} \PYG{o+ow}{in} \PYG{n}{the} \PYG{n}{Earth} \PYG{n}{Sciences}\PYG{o}{.} \PYGZbs{}\PYG{n}{sphinxstyleemphasis}\PYG{p}{\PYGZob{}}\PYG{n}{Science}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{l+m+mi}{263}\PYG{p}{(}\PYG{l+m+mi}{5147}\PYG{p}{)}\PYG{p}{:}\PYG{l+m+mi}{641}\PYGZbs{}\PYG{n}{textendash}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{l+m+mi}{646}\PYG{p}{,} \PYG{n}{February} \PYG{l+m+mf}{1994.} \PYG{n}{URL}\PYG{p}{:} \PYGZbs{}\PYG{n}{sphinxurl}\PYG{p}{\PYGZob{}}\PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{science}\PYG{o}{.}\PYG{n}{sciencemag}\PYG{o}{.}\PYG{n}{org}\PYG{o}{/}\PYG{n}{content}\PYG{o}{/}\PYG{l+m+mi}{263}\PYG{o}{/}\PYG{l+m+mi}{5147}\PYG{o}{/}\PYG{l+m+mi}{641}\PYG{p}{\PYGZcb{}} \PYG{p}{(}\PYG{n}{visited} \PYG{n}{on} \PYG{l+m+mi}{2020}\PYGZbs{}\PYG{n}{sphinxhyphen}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{l+m+mi}{04}\PYGZbs{}\PYG{n}{sphinxhyphen}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{l+m+mi}{15}\PYG{p}{)}\PYG{p}{,} \PYGZbs{}\PYG{n}{sphinxhref}\PYG{p}{\PYGZob{}}\PYG{n}{https}\PYG{p}{:}\PYG{o}{/}\PYG{o}{/}\PYG{n}{doi}\PYG{o}{.}\PYG{n}{org}\PYG{o}{/}\PYG{l+m+mf}{10.1126}\PYG{o}{/}\PYG{n}{science}\PYG{l+m+mf}{.263}\PYG{l+m+mf}{.5147}\PYG{l+m+mf}{.641}\PYG{p}{\PYGZcb{}}\PYG{p}{\PYGZob{}}\PYG{n}{doi}\PYG{p}{:}\PYG{l+m+mf}{10.1126}\PYG{o}{/}\PYG{n}{science}\PYG{l+m+mf}{.263}\PYG{l+m+mf}{.5147}\PYG{l+m+mf}{.641}\PYG{p}{\PYGZcb{}}\PYG{o}{.}
\PYG{o}{*} \PYG{n}{Keith} \PYG{n}{Beven}\PYG{o}{.} \PYG{n}{Towards} \PYG{n}{a} \PYG{n}{coherent} \PYG{n}{philosophy} \PYG{k}{for} \PYG{n}{modelling} \PYG{n}{the} \PYG{n}{environment}\PYG{o}{.} \PYGZbs{}\PYG{n}{sphinxstyleemphasis}\PYG{p}{\PYGZob{}}\PYG{n}{Proceedings} \PYG{n}{of} \PYG{n}{the} \PYG{n}{royal} \PYG{n}{society} \PYG{n}{of} \PYG{n}{London}\PYG{o}{.} \PYG{n}{Series} \PYG{n}{A}\PYG{p}{:} \PYG{n}{mathematical}\PYG{p}{,} \PYG{n}{physical} \PYG{o+ow}{and} \PYG{n}{engineering} \PYG{n}{sciences}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{l+m+mi}{458}\PYG{p}{(}\PYG{l+m+mi}{2026}\PYG{p}{)}\PYG{p}{:}\PYG{l+m+mi}{2465}\PYGZbs{}\PYG{n}{textendash}\PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}\PYG{l+m+mi}{2484}\PYG{p}{,} \PYG{l+m+mf}{2002.}
\end{sphinxVerbatim}


\chapter{Sensitivity Analysis: The Basics}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}}\label{\detokenize{3_sensitivity_analysis_the_basics:id1}}\label{\detokenize{3_sensitivity_analysis_the_basics::doc}}

\section{Global Versus Local Sensitivity}
\label{\detokenize{3_sensitivity_analysis_the_basics:global-versus-local-sensitivity}}\label{\detokenize{3_sensitivity_analysis_the_basics:global-vs-local}}
\sphinxAtStartPar
Out of the several definitions for sensitivity analysis presented in literature, the most widely used has been proposed by Saltelli \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id17}{38}{]} as “the study of how uncertainty in the output of a model (numerical or otherwise) can be apportioned to different sources of uncertainty in the model input”. In other words, sensitivity analysis explores the relationship between the model’s \(N\) input variables, \(x=[x_1,x_2,...,x_N]\), and \(M\) output variables, \(y=[y_1,y_2,...,y_M]\) with \(y=g(x)\), where \(g\) is the model that maps the model inputs to the outputs {[}\hyperlink{cite.index:id22}{39}{]}. Therefore, sensitivity analysis provides us with a set of alternatives to conducting large empirical experiments which are costly and often, in practice, next to impossible.

\sphinxAtStartPar
Historically, there have been two broad categories of sensitivity analysis techniques: local and global. Local sensitivity analysis is performed by varying model parameters around specific reference values, with the goal of exploring how small input perturbations influence model performance. Due to its convenience, this approach has been widely used in literature, but has important limitations {[}\hyperlink{cite.index:id18}{40}, \hyperlink{cite.index:id19}{41}{]}. If the model is not linear, the results of local sensitivity analysis can be heavily biased, as they will vary depending on the range of the chosen input (e.g., Tang \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id20}{42}{]}). If the model’s factors interact, local sensitivity analysis will underestimate their importance, as it does not account for those effects (e.g., {[}\hyperlink{cite.index:id23}{43}{]}). In general, as local sensitivity analysis only partially and locally explores the parametric space, it is not considered a valid approach for nonlinear models {[}\hyperlink{cite.index:id24}{44}{]}. This is illustrated in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}}} (a\sphinxhyphen{}b), presenting contour plots of a model response (\(y\)) with an additive linear model (a) and with a nonlinear model (b). In a linear model without interactions between the terms \(x_1\) and \(x_2\), local sensitivity analysis can produce appropriate sensitivity indices (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}}} (a)). If however, factors \(x_1\) and \(x_1\) interact, the local and partial consideration of the space can not properly account for each factor’s effects on the model response (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}}} (b)), as it is only informative at the base point where it is applied. In contrast, a global sensitivity analysis varies uncertain factors within the entire feasible space of variability (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}}} (c)). This approach reveals the global effects of each parameter on the model output, including any interactive effects. For models that cannot be proven linear, global sensitivity analysis is preferred and this text is primarily discussing global sensitivity analysis methods. In general, whenever we use the term sensitivity analysis we are referring to its global application.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure3_1_global_versus_local}.png}
\caption{Treatment of a two\sphinxhyphen{}dimensional space of variability by local (panels a\sphinxhyphen{}b) and global (panel c) sensitivity analyses. Local sensitivity analysis is only an appropriate approach to sensitivity in the case of linear models without interactions between terms, for example in panel (a), where \(y=3x_1+5x_2\). In the case of more complex models, for example in panels (b\sphinxhyphen{}c), where \(y={1 \above 1pt e^{x^2_1+x^2_2}} + {50 \above 1pt e^{(0.1x_1)^2+(0.1x_2)^3}}\) local sensitivity will miscalculate sensitivity indices (b), and global sensitivity methods should be used instead (c). The points in panel (c) are generated using a uniform random sample of \(n=50\), but many other methods are available.}\label{\detokenize{3_sensitivity_analysis_the_basics:id93}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-1}}\end{figure}


\section{Why Perform Sensitivity Analysis}
\label{\detokenize{3_sensitivity_analysis_the_basics:why-perform-sensitivity-analysis}}
\sphinxAtStartPar
It is important to understand the many ways in which a SA might be of use to your modeling effort which can help shape the framing of model\sphinxhyphen{}informed study. Most commonly, one might be motivated to perform sensitivity analysis for the following reasons:

\sphinxAtStartPar
\sphinxstyleemphasis{Model evaluation}: Sensitivity analysis can be used to gauge model inferences when assumptions about the structure of the model or its parameterization are dubious or have changed. For instance, consider a numerical model that uses a set of calibrated parameter values to produce outputs, which we then use to inform decisions about the real\sphinxhyphen{}world system represented. One might like to know if small changes in these parameter values significantly change this model’s output and the decisions it informs or if, instead, our parameter inferences yield stable model behavior regardless of the uncertainty present in the specific parameterized processes or properties. This can either discredit or lend credence to the model at hand, as well as any inferences drawn that are founded on its accurate representation of the system. Sensitivity analysis can identify which uncertain model factors cause this undesirable model behavior.

\sphinxAtStartPar
\sphinxstyleemphasis{Model simplification}: Sensitivity analysis can also be used to identify factors or components of the model that appear to have limited effects on direct outputs or metrics of interest. Consider a model that has been developed in an organization for the purposes of a specific research question and is later used in the context of a different application. Some processes represented in significant detail might no longer be of the same importance while consuming significant data or computational resources, as different outputs might be pertinent to the new application. Sensitivity analysis can be used to identify unimportant model components and simplify them to nominal values and reduced model forms. Model complexity and computational costs can therefore be reduced, and, by extension, monetary investments.

\sphinxAtStartPar
\sphinxstyleemphasis{Model refinement}: Alternatively, sensitivity analysis can reveal the factors or processes that are highly influential to the outputs or metrics of interest, by assessing their relative importance. In the context of model evaluation, this can inform which model components warrant additional investigation or measurement so the uncertainty surrounding them and the resulting model outputs or metrics of interest can be reduced.

\sphinxAtStartPar
\sphinxstyleemphasis{Exploratory modeling}: When sufficient credence has been established in the model, sensitivity analysis can be applied to a host of other inquiries. Inferences about the factors and processes that most (or least) control a model’s outputs of interest can be extrapolated to the real system they represent and be used in a heuristic manner to inform model\sphinxhyphen{}based inferences. On this foundation, a model paired with the advanced techniques presented in this text can be used to “discover” decision relevant and highly consequential outcomes (i.e., scenario discovery, {[}\hyperlink{cite.index:id15}{36}, \hyperlink{cite.index:id16}{45}{]}).

\sphinxAtStartPar
The nature and context of the model shapes the specific objectives of applying a sensitivity analysis, as well as methods and tools most appropriate and defensible for each application setting {[}\hyperlink{cite.index:id17}{38}, \hyperlink{cite.index:id25}{46}{]}. The three most common sensitivity analysis modes (\sphinxstyleemphasis{Factor Prioritization}, \sphinxstyleemphasis{Factor Fixing}, and \sphinxstyleemphasis{Factor Mapping}) are presented below, but the reader should be aware that other uses have been proposed in the literature (e.g., {[}\hyperlink{cite.index:id26}{47}, \hyperlink{cite.index:id27}{48}{]}).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor prioritization}: This sensitivity analysis application mode (also referred to as \sphinxstyleemphasis{factor ranking}) refers to when one would like to identify the uncertain factors which, when fixed to their true value (i.e., if there were no uncertainty regarding their value), would lead to the greatest reduction in output variability {[}\hyperlink{cite.index:id28}{49}{]}. Information from this type of analysis can be crucial to model improvement as these factors can become the focus of future measurement campaigns or numerical experiments so that uncertainty in the model output can be reduced. The impact of each uncertain input on the variance of the model output is often used as the criterion for factor prioritization. \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}}} (a) shows the effects of three uncertain variables (\(X_1\), \(X_2\), and \(X_3\)) on the variance of output \(Y\). \(V(E(Y|X_i))\) indicates the variance in \(Y\) if factor \(X_i\) is left to vary freely while all other factors remain fixed to nominal values. In this case, factor \(X_2\) makes the largest contribution to the variability of output \(Y\) and it should therefore be prioritized. In the context of risk analysis, factor prioritization can be used to reduce output variance to below a given tolerable threshold (also known as variance cutting).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor fixing}: This mode of sensitivity analysis (also referred to as \sphinxstyleemphasis{factor screening}) aims to identify the model components that have a negligible effect or make no significant contributions to the variability of the outputs or metrics of interest (usually referred to as non\sphinxhyphen{}influential {[}\hyperlink{cite.index:id28}{49}{]}). In the stylized example of \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}}} (a), \(X_1\) makes the smallest contribution to the variability of output \(Y\) suggesting that the uncertainty in its value could be negligible and the factor itself fixed in subsequent model executions. Eliminating these factors or processes in the model or fixing them to a nominal value can help reduce model complexity as well as the unnecessary computational burden of subsequent model runs, results processing, or other sensitivity analyses (the fewer uncertain factors considered, the fewer runs are necessary to illuminate their effects on the output). Significance of the outcome can be gauged in a variety of manners, depending on the application. For instance, if applying a variance\sphinxhyphen{}based method, a minimum threshold value of contribution to the variance could be considered as a significance ‘cutoff’, and factors with indices below that value can be considered non\sphinxhyphen{}influential. Nb: conclusions about factor fixing should be made based on total\sphinxhyphen{}order effects, i.e., considering all the effects a factor has, individually and in interaction with other factors (explained in more detail in the \hyperref[\detokenize{3_sensitivity_analysis_the_basics:variance-based-methods}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:variance-based-methods}}}).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor mapping}: Finally, factor mapping can be used to pinpoint which values of uncertain factors lead to model outputs within a given range of the output space {[}\hyperlink{cite.index:id28}{49}{]}. In the context of model diagnostics, it is possible that the model’s output changes in ways considered impossible based on the represented processes, or other observed evidence. In this situation, factor mapping can be used to identify which uncertain model factors cause this undesirable model behavior by ‘filtering’ model runs that are considered ‘non\sphinxhyphen{}behavioral’ {[}\hyperlink{cite.index:id29}{50}, \hyperlink{cite.index:id30}{51}, \hyperlink{cite.index:id31}{52}{]}. In \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}}} (b), region \(B\) of the output space denotes the set of behavioral model outcomes, which can be traced back to input space \(X\) (e.g., using Monte Carlo Filtering or pre\sphinxhyphen{}calibration, see \hyperref[\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}]{Section \ref{\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}}}).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{figure3_2_factor_mapping}.png}
\caption{Factor prioritization, factor fixing and factor mapping settings of sensitivity analysis.}\label{\detokenize{3_sensitivity_analysis_the_basics:id94}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}}\end{figure}

\sphinxAtStartPar
The language used above reflects a use of sensitivity analysis for model fidelity evaluation and refinement. However, as previously mentioned, when a model has been established as a sufficiently accurate representation of the system, sensitivity analysis can produce additional inferences. For instance, under the factor mapping use, the analyst can now focus on undesirable system states and discover which factors are most responsible for them: for instance, “population growth of above 25\% would be responsible for unacceptably high energy demands”. Factor prioritization and factor fixing can be used to make equivalent inferences, such as “growing populations and increasing temperatures are the leading factors for changing energy demands” (prioritizing of factors) or “changing dietary needs are inconsequential to increasing energy demands for this region” (a factor that can be fixed in subsequent model runs). All these inferences hinge on the assumption that the real system’s stakeholders consider the model states faithful enough representations of system states. As elaborated in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}]{Section \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}}}, this view on sensitivity analysis is founded on a relativist perspective on modeling, which tends to place more value on model usefulness rather than strict accuracy of representation in terms of error. As such, sensitivity analysis performed with decision\sphinxhyphen{}making relevance in mind will focus on model outputs or metrics that are consequential and decision relevant (e.g., energy demand in the examples above).


\section{Design of Experiments}
\label{\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}}\label{\detokenize{3_sensitivity_analysis_the_basics:id15}}
\sphinxAtStartPar
Before conducting a sensitivity analysis, the first element that needs to be clarified is the uncertainty space of the model {[}\hyperlink{cite.index:id30}{51}, \hyperlink{cite.index:id111}{53}{]}. In other words, how many and which factors making up the mathematical model are considered uncertain and can potentially affect the model output and the inferences drawn from it. Uncertain factors can be model parameters, model structures, inputs, or alternative model resolution levels (scales), all of which can be assessed through the tools presented in this text. Depending on the kind of factor, its variability can be elicited through various means: expert opinion, values reported in the literature, historical observations, its physical meaning (e.g., population values in a city can never be negative), or through the use of more formal UQ methods (\hyperref[\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}]{Section \ref{\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}}}). The model uncertainty space represents the entire space of variability present in each of the uncertain factors of a model. The complexity of most real\sphinxhyphen{}world models means that the response function, \(y=g(x)\), mapping inputs to outputs, is hardly ever available in an analytical form and therefore analytically computing the sensitivity of the output to each uncertain factor becomes impossible. In these cases, sensitivity analysis is only feasible through numerical procedures that employ different strategies to sample the uncertainty space and calculate sensitivity indices.

\sphinxAtStartPar
A sampling strategy is often referred to as a \sphinxstyleemphasis{design of experiments} and represents a methodological choice made before conducting any sensitivity analysis. Experimental design was first introduced by Fisher {[}\hyperlink{cite.index:id112}{54}{]} in the context of laboratory or field\sphinxhyphen{}based experiments. Its application in sensitivity analysis is similar to setting up a physical experiment in that it is used to discover the behavior of a system under specific conditions. An ideal design of experiments should provide a framework for the extraction of all plausible information about the impact of each factor on the output of the numerical model. The design of experiments is used to set up a simulation platform with the minimum computational cost to answer specific questions that cannot be readily drawn from the data through analytical or common data mining techniques. Models representing coupled human\sphinxhyphen{}natural systems usually have a large number of inputs, state variables and parameters, but not all of them exert fundamental control over the numerical process, despite their uncertainty, nor have substantial impacts on the model output, either independently or through their interactions. Each factor influences the model output in different ways that need to be discovered. For example, the influence of a parameter on model output can be linear or non\sphinxhyphen{}linear and can be continuous or only be active during specific times or at particular states of the system {[}\hyperlink{cite.index:id53}{55}, \hyperlink{cite.index:id113}{56}{]}. An effective and efficient design of experiments allows the analyst to explore these complex relationships and evaluate different behaviors of the model for various scientific questions {[}\hyperlink{cite.index:id114}{57}{]}.

\sphinxAtStartPar
There are a few different approaches to the design of experiments, closely related to the chosen sensitivity analysis approach, which is in turn shaped by the research motivations, scientific questions, and computational constraints at hand (additional discussion of this can be found at the end of \hyperref[\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}}}). For example, in a sensitivity analysis using perturbation and derivatives methods, the model input parameters vary from their nominal values one at a time, something that the design of experiments needs to reflect. If, instead, one were to perform sensitivity analysis using a multiple\sphinxhyphen{}starts perturbation method, the design of experiments needs to consider that multiple points across the factor space are used. The design of experiments specifically defines two key characteristics of samples that are fed to the numerical model: the number of samples and the range of each factor.

\sphinxAtStartPar
Generally, sampling can be performed randomly or by applying a stratifying approach. In random sampling, such as Monte Carlo {[}\hyperlink{cite.index:id115}{58}{]}, samples are randomly generated by a pseudo\sphinxhyphen{}random number generator with an a\sphinxhyphen{}priori assumption about the distribution of parameters and their possible ranges. Random seeds can also be used to ensure consistency and higher control over the random process. However, this method could leave some holes in the parameter space and cause clustering in some spaces, especially for a large number of parameters {[}\hyperlink{cite.index:id116}{59}{]}. Most sampling strategies use stratified sampling to mitigate these disadvantages. Stratified sampling techniques divide the domain of each factor into subintervals, often of equal lengths. From each subinterval, an equal number of samples is drawn randomly, or based on the specific locations within the subintervals {[}\hyperlink{cite.index:id28}{49}{]}. The rest of this section overviews some of the most commonly used designs of experiments. Table 1 summarizes the designs discussed.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Summary of designs of experiments overviewed in this section. * Depends on the sample size.}\label{\detokenize{3_sensitivity_analysis_the_basics:id95}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Design of experiments}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Factor interactions considered}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Treatment of factor domains}
\\
\hline
\sphinxAtStartPar
One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (OAT)
&
\sphinxAtStartPar
No \sphinxhyphen{} main effects only
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\sphinxAtStartPar
Full Factorial Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects
&
\sphinxAtStartPar
Discrete (levels)
\\
\hline
\sphinxAtStartPar
Fractional Factorial Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} only lower\sphinxhyphen{}order effects*
&
\sphinxAtStartPar
Discrete (levels)
\\
\hline
\sphinxAtStartPar
Latin Hypercube (LH) Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects*
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\sphinxAtStartPar
Quasi\sphinxhyphen{}Random Sampling with Low\sphinxhyphen{}Discrepancy Sequences
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects*
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (OAT)}
\label{\detokenize{3_sensitivity_analysis_the_basics:one-at-a-time-oat}}
\sphinxAtStartPar
In this approach, only one model factor is changed at a time while all others are kept fixed across each iteration in a sampling sequence. The OAT method assumes that model factors of focus are linearly independent (i.e., there are no interactions) and can analyze how factors individually influence model outputs or metrics of interest. While highly popular given its ease of implementation, OAT  is ultimately highly limited in its exploration of a model’s sensitivities {[}\hyperlink{cite.index:id28}{49}{]}. It is primarily used with local sensitivity techniques with similar criticisms: applying this sampling scheme on a system with nonlinear and interactive processes will miss important information on the effect uncertain factors have on the model. OAT samplings can be repeated multiple times in a more sophisticated manner and across different locations of the parameter space to overcome some of these challenges, which would increase computational costs and negate the main reasons for its selection.


\subsection{Full and Fractional Factorial Sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:full-and-fractional-factorial-sampling}}
\sphinxAtStartPar
In full factorial sampling, each factor is treated as being discrete, by considering two or more levels (or intervals). The sampling process then generates samples within each possible combination of levels, corresponding to each parameter. This scheme produces a more comprehensive sampling of the factors’ variability space, as it accounts for all candidate combinations of factor levels (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (a)). If the number of levels is the same across all factors, the number of generated samples is estimated using nk, where n is the number of levels and k is the number of factors. For example, \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (a) presents a full factorial sampling of three uncertain factors \((x_1,\) \(x_2,\) and \(x_3)\), each considered as having four discrete levels. The total number of samples necessary for such an experiment is \(4^3=64\). As the number of factors increases, the number of simulations necessary can also grow exponentially, making full factorial sampling computationally burdensome (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (b)). As a result, literature has commonly applied full factorial sampling at only two levels per factor, typically the two extremes {[}\hyperlink{cite.index:id32}{60}{]}. This significantly reduces computational burden but is only considered appropriate in cases where factors can indeed only assume two discrete values (e.g., when testing the effects of epistemic uncertainty and comparing between model structure A and model structure B). In the case of physical parameters on continuous distributions (e.g., when considering the effects of measurement uncertainty in a temperature sensor), discretizing the range of a factor to only extreme levels can bias its estimated importance.

\sphinxAtStartPar
Fractional factorial design is a widely used alternative to full factorial that allows the analyst to significantly reduce the number of simulations by confounding the main effects of a factor with its interactive effects {[}\hyperlink{cite.index:id28}{49}{]}. In other words, if one can reasonably assume that higher\sphinxhyphen{}order interactions are negligible, information about the main effects and lower\sphinxhyphen{}order interactions can be obtained using a fraction of the full factorial design. Traditionally, fractional factorial design has also been limited to two levels {[}\hyperlink{cite.index:id32}{60}{]}, referred to as Fractional Factorial designs 2k\sphinxhyphen{}p {[}\hyperlink{cite.index:id34}{61}{]}. Recently, Generalized Fractional Factorial designs have also been proposed that allow for the structured generation of samples at more than two levels per factor {[}\hyperlink{cite.index:id33}{62}{]}. Consider a case where the modeling team dealing with the problem in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (a) cannot afford to perform 64 simulations of their model. They can afford 32 runs for their experiment and instead decide to fractionally sample the variability space of their factors. A potential design of such a sampling strategy is presented in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (c).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{figure3_3_alternative_designs}.png}
\caption{Alternative designs of experiments and their computational costs for three uncertain factors \((x_1,\) \(x_2,\) and \(x_3)\). (a) Full factorial design sampling of three factors at four levels, at a total of 64 samples; (b) exponential growth of necessary number of samples when applying full factorial design at four levels; (c) fractional factorial design of three factors at four levels, at a total of 32 samples; and (d) Latin Hypercube sample of three factors with uniform distributions, at a total of 32 samples.}\label{\detokenize{3_sensitivity_analysis_the_basics:id96}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}\end{figure}


\subsection{Latin Hypercube (LH) Sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:latin-hypercube-lh-sampling}}
\sphinxAtStartPar
Latin hypercube sampling {[}\hyperlink{cite.index:id91}{63}{]} is one of the most common methods in space\sphinxhyphen{}filling experimental designs. With this sampling technique, for \(N\) uncertain factors, an \(N\)\sphinxhyphen{}dimensional hypercube is generated, with each factor divided into an equal number of levels depending on the sample to be generated. Equal numbers of samples are then randomly generated at each level, across all factors. In this manner, LH design guarantees sampling from every level of the variability space and without any overlaps. When the number of samples generated is much larger than the number of uncertain factors, LH sampling can be very effective in examining the effects of each factor {[}\hyperlink{cite.index:id28}{49}{]}. LH sampling is an attractive technique, because it guarantees a diverse coverage of the space, through the use of subintervals, without being constrained to discrete levels for each factor \sphinxhyphen{} compare \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (c) with \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-3}}} (d) for the same number of samples.

\sphinxAtStartPar
LH sampling is less effective when the number of samples is not much larger than the number of uncertain factors, and the effects of each factor cannot be appropriately distinguished. The samples between factors can also be highly correlated, biasing any subsequent sensitivity analysis results. To address this, the sampling scheme can be modified to control for the correlation in parameters while maximizing the information derived. An example of such modification is through the use of orthogonal arrays {[}\hyperlink{cite.index:id21}{64}{]}.


\subsection{Low\sphinxhyphen{}Discrepancy Sequences}
\label{\detokenize{3_sensitivity_analysis_the_basics:low-discrepancy-sequences}}
\sphinxAtStartPar
Low\sphinxhyphen{}discrepancy sequences is another sampling technique that employs a pseudo\sphinxhyphen{}random generator for Monte Carlo sampling {[}\hyperlink{cite.index:id81}{65}, \hyperlink{cite.index:id92}{66}{]}. These quasi\sphinxhyphen{}Monte Carlo methods eliminate potential gaps and clusters between samples by minimizing discrepancy when generating uniformly distributed random samples within the hypercube. The discrepancy property is mathematically measured by characterizing the lumpiness of a sequence of samples in a multidimensional space, which results in evenly distributed samples {[}\hyperlink{cite.index:id81}{65}{]}. Discrepancy can be quantitatively measured using the deviations of sampled points from the uniform distribution {[}\hyperlink{cite.index:id95}{67}{]}. Low\sphinxhyphen{}discrepancy sequences ensure that the number of samples in any subspace of the variability hypercube is approximately the same. This is not something guaranteed by LH sampling, and even though the design can be improved through optimization with various criteria, such adjustments are limited to small sample sizes and low dimensions {[}\hyperlink{cite.index:id95}{67}, \hyperlink{cite.index:id93}{68}, \hyperlink{cite.index:id94}{69}, \hyperlink{cite.index:id96}{70}, \hyperlink{cite.index:id97}{71}{]}. The Sobol sequence {[}\hyperlink{cite.index:id98}{72}, \hyperlink{cite.index:id99}{73}{]}, one of the most widely used sampling techniques, utilizes the low\sphinxhyphen{}discrepancy approach to uniformly fill the sampled factor space. A core advantage of this style of sampling is that it takes far fewer samples (i.e., simulations) to attain a much lower level of error in estimating model output statistics (e.g., the mean and variance of outputs).

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Put this into practice! Click the following link to try out an interactive tutorial on implementing a Sobol SA using SALib:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fsa\_saltelli\_sobol\_ishigami.ipynb}{Sobol SA using SALib Jupyter Notebook}
\end{sphinxadmonition}


\subsection{Other types of sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:other-types-of-sampling}}
\sphinxAtStartPar
The sampling techniques mentioned so far are general sampling methods useful for a variety of applications beyond sensitivity analysis. There are however techniques that have been developed for specific sensitivity analysis methods. Examples of these methods include the Morris One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time {[}\hyperlink{cite.index:id132}{74}{]}, Fourier Amplitude Sensitivity Test (FAST; {[}\hyperlink{cite.index:id100}{75}{]}), Extended FAST {[}\hyperlink{cite.index:id101}{76}{]}, and Extended Sobol methods {[}\hyperlink{cite.index:id102}{77}{]}. For example, the Morris sampling strategy builds a number of trajectories (usually referred to as repetitions and denoted by \(r\)) in the input space each composed of \(N+1\) factor points, where \(N\) is the number of uncertain factors. The first point of the trajectory is selected randomly and the subsequent \(N\) points are generated by moving one factor at a time by a fixed amount. Each factor is perturbed once along the trajectory, while the starting points of all of the trajectories are randomly and uniformly distributed. Several variations of this strategy also exist in the literature; for more details on each approach and their differences the reader is directed to Pianosi \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id30}{51}{]}.


\subsection{Synthetic generation of input time series}
\label{\detokenize{3_sensitivity_analysis_the_basics:synthetic-generation-of-input-time-series}}
\sphinxAtStartPar
Models often have input time series or processes with strong temporal and/or spatial correlations (e.g., streamflow, energy demand, price of commodities, etc.) that, while they might not immediately come to mind as factors to be examined in sensitivity analysis, can be treated as such. Synthetic input time series are used for a variety of reasons, for example, when observations are not available or are limited, or when past observations are not considered sufficiently representative to capture rare or extreme events of interest {[}\hyperlink{cite.index:id103}{78}, \hyperlink{cite.index:id104}{79}{]}. Synthetic generation of input time series provides a valuable tool to consider non\sphinxhyphen{}stationarity and incorporate potential stressors, such as climate change impacts into input time series {[}\hyperlink{cite.index:id105}{80}{]}. For example, a century of record will be insufficient to capture very high impact rare extreme events (e.g., persistent multi\sphinxhyphen{}year droughts). A large body of statistical literature exists focusing on the topics of synthetic weather {[}\hyperlink{cite.index:id74}{81}, \hyperlink{cite.index:id75}{82}{]} and streamflow {[}\hyperlink{cite.index:id76}{83}, \hyperlink{cite.index:id77}{84}{]} generation that provides a rich suite of approaches for developing history\sphinxhyphen{}informed, well\sphinxhyphen{}characterized stochastic process models to better estimate rare individual or compound (hot, severe drought) extremes. It is beyond the scope of this text to review these methods, but readers are encouraged to explore the studies cited above as well as the following publications for discussions and comparisons of these methods: {[}\hyperlink{cite.index:id103}{78}, \hyperlink{cite.index:id105}{80}, \hyperlink{cite.index:id106}{85}, \hyperlink{cite.index:id107}{86}, \hyperlink{cite.index:id108}{87}, \hyperlink{cite.index:id109}{88}, \hyperlink{cite.index:id110}{89}{]}. The use of these methods for the purposes of exploratory modeling, especially in the context of well\sphinxhyphen{}characterized versus deep uncertainty, is further discussed in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios}]{Section \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios}}}.


\section{Sensitivity Analysis Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-methods}}\label{\detokenize{3_sensitivity_analysis_the_basics:id47}}
\sphinxAtStartPar
In this section, we describe some of the most widely applied sensitivity analysis methods along with their mathematical definitions. We also provide a detailed discussion on applying each method, as well as a comparison of and their features and limitations.


\subsection{Derivative\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:derivative-based-methods}}
\sphinxAtStartPar
Derivative\sphinxhyphen{}based methods explore how model outputs are affected by perturbations in a single model input around a particular input value. These methods are local and are performed using OAT sampling. For simplicity of mathematical notations, let us assume that the model \(g(X)\) only returns one output. Following {[}\hyperlink{cite.index:id117}{90}{]} and {[}\hyperlink{cite.index:id30}{51}{]}, the sensitivity index, \(S_i\) , of the model’s \sphinxstyleemphasis{i}\sphinxhyphen{}th input factor, \(x_i\) , can be measured using the partial derivative evaluated at a nominal value, \(\bar{x}\), of the vector of inputs:
\begin{equation*}
\begin{split}S_i (\bar{x}) = \frac{\partial g}{\partial x} |_{\bar{x}{^c{_i}}}\end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{c}$_{\text{i}}$ is the scaling factor. In most applications however, the relationship \(g(X)\) is not fully known in its analytical form, and therefore the above partial derivative is usually approximated:
\begin{equation*}
\begin{split}S_i (\bar{x}) = \frac{g(\bar{x}_1,...\bar{x}_i+\Delta_i,...\bar{x}_N)-g(\bar{x}_1,...\bar{x}_i,...\bar{x}_N)}{\Delta_i}c_i\end{split}
\end{equation*}
\sphinxAtStartPar
Using this approximation, the \sphinxstyleemphasis{i}\sphinxhyphen{}th input factor is perturbed by a magnitude of \(\Delta_i\), and its relative importance is calculated. Derivative\sphinxhyphen{}based methods are some of the oldest sensitivity analysis methods as they only require \(N+1\) model evaluations to estimate indices for \(N\) uncertain factors. As described above, being computationally very cheap comes at the cost of not being able to explore the entire input space, but only (local) perturbations to the nominal value. Additionally, as these methods examine the effects of each input factor one at a time, they cannot assess parametric interactions or capture the interacting nature of many real systems and the models that abstract them.


\subsection{Elementary Effect Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:elementary-effect-methods}}
\sphinxAtStartPar
Elementary effect (EE) SA methods provide a solution to the local nature of the derivative\sphinxhyphen{}based methods by exploring the entire parametric range of each input parameter {[}\hyperlink{cite.index:id131}{91}{]}. However, EE methods still use OAT sampling and do not vary all input parameters simultaneously while exploring the parametric space. The OAT nature of EEs methods therefore prevents them from properly capturing the interactions between uncertain factors. EEs methods are computationally efficient compared to their All\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (AAT) counterparts, making them more suitable when computational capacity is a limiting factor, while still allowing for some inferences regarding factor interactions.
The most popular EE method is the Method of Morris {[}\hyperlink{cite.index:id132}{74}{]}. Following the notation by {[}\hyperlink{cite.index:id30}{51}{]}, this method calculates global sensitivity using the mean of the EEs (finite differences) of each parameter at different locations:
\begin{equation*}
\begin{split}S_i = \mu_i^* = \frac{1}{r}\sum_{j=1}^r EE^j_i = \frac{1}{r}\sum_{j=1}^r \frac{g(\bar{x}_1,...\bar{x}_i+\Delta_i,...\bar{x}_N)-g(\bar{x}_1,...\bar{x}_i,...\bar{x}_N)}{\Delta_i}c_i\end{split}
\end{equation*}
\sphinxAtStartPar
with \(r\) representing the number of repetitions (or trajectories) in the input space, usually set between 4 and 10 {[}\hyperlink{cite.index:id17}{38}{]}. Each xj represents the points of each trajectory, with \(j=1,…, r\), selected as described in the sampling strategy for this method, found above. This method also produces the standard deviation of the EEs:
\begin{equation*}
\begin{split}\sigma_i = \sqrt{\frac{1}{r}\sum_{j=1}^r(EE_i^j-\frac{1}{r}\sum_{j=1}^r EE^j_i)^2}\end{split}
\end{equation*}
\sphinxAtStartPar
which is a measure of parametric interactions. Higher values of \(\sigma_i\) suggest model responses at different levels of factor \(x_i\) are significantly different, which indicates considerable interactions between that and other uncertain factors. The values of \(\mu_i^*\) and \(\sigma_i\) for each factor allow us to draw several different conclusions, illustrated in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-4}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-4}}}, following the example by {[}\hyperlink{cite.index:id131}{91}{]}. In this example, factors \(x_1\), \(x_2\), \(x_4\), and \(x_5\) can be said to have an influence on the model outputs, with \(x_1\), \(x_4\), and \(x_5\) having some interactive or non\sphinxhyphen{}linear effects. Depending on the orders of magnitude of \(\mu_i^*\) and \(\sigma_i\) one can indirectly deduce whether the factors have strong interactive effects, for example if a factor \(\sigma_i << \mu_i^*\) then the relationship between that factor and the output can be assumed to be largely linear (nb: this is still an OAT method and assumptions on factor interactions should be strongly caveated). Extensions of the Method of Morris have also been developed specifically for the purposes of factor fixing and explorations of parametric interactions (e.g., {[}\hyperlink{cite.index:id27}{48}, \hyperlink{cite.index:id133}{92}, \hyperlink{cite.index:id134}{93}{]}).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure3_4_morris_method}.png}
\caption{Illustrative results of the Morris Method. Factors \(x_1\), \(x_2\), \(x_4\), and \(x_5\) have an influence on the model outputs, with \(x_1\), \(x_4\), and \(x_5\) having interactive or non\sphinxhyphen{}linear effects.}\label{\detokenize{3_sensitivity_analysis_the_basics:id97}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-4}}\end{figure}


\subsection{Regression\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:regression-based-methods}}
\sphinxAtStartPar
Regression analysis is another one of the oldest ways of investigating parametric importance and sensitivity {[}\hyperlink{cite.index:id17}{38}{]}. Here, we describe some of the most popular regression\sphinxhyphen{}based sensitivity indices. One of the main sensitivity indices of this category is the standardized regression coefficient (SRC). To calculate SRC, a linear regression relationship needs to be fitted between the input vector, \(x\), and the model output of interest by using a least\sphinxhyphen{}square minimizing method:
\begin{equation*}
\begin{split}y = b_0 + \sum_{i=1}^N b_ix_i\end{split}
\end{equation*}
\sphinxAtStartPar
where \(b_0\) and \(b_i\) (corresponding to the \sphinxstyleemphasis{i}\sphinxhyphen{}th model input) are regression coefficients. The following relationship can then be used to calculate the SRCs for different input values:
\begin{equation*}
\begin{split}S_i=SRC_i=b_i\frac{\sigma_i}{\sigma_y}\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\sigma_i\) and \(\sigma_y\) are standard deviations of \sphinxstyleemphasis{i}\sphinxhyphen{}th model input and output, respectively.

\sphinxAtStartPar
Several other regression\sphinxhyphen{}based indices explore the correlation between input and output parameters as a proxy to model parametric sensitivity {[}\hyperlink{cite.index:id131}{91}, \hyperlink{cite.index:id135}{94}, \hyperlink{cite.index:id136}{95}{]}. The Pearson correlation coefficient (PCC) can be used when a linear relationship exists between an uncertain factor, \(x_i\), and the output \(y\):
\begin{equation*}
\begin{split}S_i=PCC=\frac{cov(x_i,y)}{\sigma_i\sigma_y}\end{split}
\end{equation*}
\sphinxAtStartPar
In cases when there are outliers in the data or the relationship between the uncertain factors and the output is not linear, rank\sphinxhyphen{}based correlation coefficients are preferred, for example, Spearman’s rank correlation coefficient (SRCC):
\begin{equation*}
\begin{split}S_i=SRCC=\frac{cov(rx_i,ryi)}{\sigma_{ri}\sigma_{ry}}\end{split}
\end{equation*}
\sphinxAtStartPar
where the raw values of \(x_i\) and \(y\) and converted to ranks \(rx_i\) and \(ry\) respectively, which instead represents a measurement of the strength of the monotonic relationship, rather than linear relationship, between the input and output. Other regression\sphinxhyphen{}based metrics include the partial correlations coefficient, the partial rank correlations coefficient, and the Nash\sphinxhyphen{}Sutcliffe coefficient, more discussion on which can be found in {[}\hyperlink{cite.index:id22}{39}, \hyperlink{cite.index:id131}{91}{]}.

\sphinxAtStartPar
Tree\sphinxhyphen{}based regression techniques have also been used for sensitivity analysis in an effort to address the challenges faced with nonlinear models {[}\hyperlink{cite.index:id137}{96}{]}. Examples of these methods include the Patient Rule Induction Method (PRIMl; {[}\hyperlink{cite.index:id79}{97}{]}) and Classification And Regression Trees (CART; {[}\hyperlink{cite.index:id80}{98}{]}). CART\sphinxhyphen{}based approaches also include their boosting and bagging extensions {[}\hyperlink{cite.index:id89}{99}, \hyperlink{cite.index:id138}{100}{]}. These methods are particularly useful when sensitivity analysis is used for factor mapping (i.e., when trying to identify which uncertain model factors produce a certain model behavior). \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios}]{Section \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios}}} elaborates on the use of these methods. Regression\sphinxhyphen{}based sensitivity analysis methods are global by nature and can explore the entire space of variables. However, the true level of comprehensiveness depends on the design of experiments and the number of simulations providing data to establish the regression relationships. Although they are usually computationally efficient, they do not produce significant information about parametric interactions {[}\hyperlink{cite.index:id17}{38}, \hyperlink{cite.index:id22}{39}{]}.


\subsection{Regional Sensitivity Analysis}
\label{\detokenize{3_sensitivity_analysis_the_basics:regional-sensitivity-analysis}}
\sphinxAtStartPar
Another method primarily applied for basic factor mapping applications is Regional Sensitivity Analysis (RSA; {[}\hyperlink{cite.index:id157}{101}{]}). RSA is a global sensitivity analysis method that is typically implemented using standard sampling methods such as LH sampling. It is performed by specifying a condition on the output space (e.g., an upper threshold) and classifying outputs that meet the condition as behavioral and the ones that fail it as non\sphinxhyphen{}behavioral (illustrated in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-2}}} (b)). Note that the specified threshold depends on the nature of the problem, model, and the research question. It can reflect model\sphinxhyphen{}performance metrics (such as errors) or consequential decision\sphinxhyphen{}relevant metrics (such as unacceptable system outcomes). The behavioral and non\sphinxhyphen{}behavioral outputs are then traced back to their originating sampled factor sets, where differences between the distributions of samples can be used to determine their significance in producing each part of the output. The Kolmogorov\sphinxhyphen{}Smirnov divergence is commonly used to quantify the difference between the distribution of behavioral and non\sphinxhyphen{}behavioral parameters {[}\hyperlink{cite.index:id30}{51}{]}.
\begin{equation*}
\begin{split}S_i=|F_{x_i|y_b} (y \in Y_b)-F_{x_i|y_{nb}} (y \in Y_{nb})|\end{split}
\end{equation*}
\sphinxAtStartPar
where \(Y_b\) represents the set of behavioral outputs, and \(F_{x_i|y_b}\) is the empirical cumulative distribution function of the values of \(x_i\) associated with values of \(y\) that belong in the behavioral set. The nb notation indicates the equivalent elements related to the non\sphinxhyphen{}behavioral set. Large differences between the two distributions indicate stronger effects by the parameters on the respective part of the output space.

\sphinxAtStartPar
Used in a factor mapping setting, RSA can be applied for scenario discovery {[}\hyperlink{cite.index:id71}{102}, \hyperlink{cite.index:id70}{103}{]}, the Generalized Likelihood Uncertainty Estimation method (GLUE; {[}\hyperlink{cite.index:id38}{19}, \hyperlink{cite.index:id158}{104}, \hyperlink{cite.index:id159}{105}{]}) and other hybrid sensitivity analysis methods (e.g., {[}\hyperlink{cite.index:id160}{106}, \hyperlink{cite.index:id161}{107}{]}). The fundamental shortcomings of RSA are that, in some cases, it could be hard to interpret the difference between behavioral and non\sphinxhyphen{}behavioral sample sets, and that insights about parametric correlations and interactions cannot always be uncovered {[}\hyperlink{cite.index:id17}{38}{]}. For more elaborate discussions and illustrations of the RSA method, readers are directed to Tang \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id20}{42}{]}, Saltelli \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id28}{49}{]}, Young {[}\hyperlink{cite.index:id162}{108}{]} and references therein.


\subsection{Variance\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:variance-based-methods}}\label{\detokenize{3_sensitivity_analysis_the_basics:id71}}
\sphinxAtStartPar
Variance\sphinxhyphen{}based sensitivity analysis methods hypothesize that various specified model factors contribute differently to the variation of model outputs; therefore, decomposition and analysis of output variance can determine a model’s sensitivity to input parameters {[}\hyperlink{cite.index:id17}{38}, \hyperlink{cite.index:id102}{77}{]}. The most popular variance\sphinxhyphen{}based method is the Sobol method, which is a global sensitivity analysis method that takes into account complex and nonlinear factor interaction when calculating sensitivity indices, and employs more sophisticated sampling methods (e.g., the Sobol sampling method). The Sobol method is able to calculate three types of sensitivity indices that provide different types of information about model sensitivities. These indices include first\sphinxhyphen{}order, higher\sphinxhyphen{}order (e.g., second\sphinxhyphen{}, third\sphinxhyphen{}, etc. orders), and total\sphinxhyphen{}order sensitivities.

\sphinxAtStartPar
The first\sphinxhyphen{}order sensitivity index indicates the percent of model output variance contributed by a factor individually (i.e., the effect of varying \(x_i\) alone) and is obtained using the following {[}\hyperlink{cite.index:id102}{77}, \hyperlink{cite.index:id148}{109}{]}.
\begin{equation*}
\begin{split}S_i^1=\frac{V_{x_i}[E_{x_{\sim i}}(x_i)]}{V(y)}\end{split}
\end{equation*}
\sphinxAtStartPar
with \(E\) and \(V\) denoting the expected value and the variance, respectively. \(x_{\sim i}\) denotes all factors expect from \(x_i\). The first\sphinxhyphen{}order sensitivity index (\(S_i^1\)) can therefore also be thought of as the portion of total output variance (\(V_y\)) that can be reduced if the uncertainty in factor \(x_i\) is eliminated {[}\hyperlink{cite.index:id149}{110}{]}. First\sphinxhyphen{}order sensitivity indices are usually used to understand the independent effect of a factor and to distinguish its individual versus interactive influence. It would be expected for linearly independent factors that they would only have first order indices (no interactions) that should correspond well with sensitivities obtained from simpler methods using OAT sampling.

\sphinxAtStartPar
Higher\sphinxhyphen{}order sensitivity indices explore the interaction between two or more parameters that contribute to model output variations. For example, a second\sphinxhyphen{}order index indicates how interactions between a pair of parameter input variables can lead to change in model output variations and is calculated using the following relationship:
\begin{equation*}
\begin{split}S_i^2=\frac{V_{x_{i,j}}[E_{x_{\sim i,j}}(x_i,x_j)]}{V(y)}\end{split}
\end{equation*}
\sphinxAtStartPar
with \(i \ne j\). Higher order indices can be calculated by similar extensions (i.e., fixing additional operators together), but it is usually computationally expensive in practice. There are some software packages that calculate indices for orders higher than second; for example, the “senssobol” R package calculates third\sphinxhyphen{}order indices (more information can be found in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:software-toolkits}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:software-toolkits}}}).

\sphinxAtStartPar
The total sensitivity analysis index represents the entire influence of an input factor on model outputs including all of its interactions with other factors {[}\hyperlink{cite.index:id150}{111}{]}. In other words, total SA indices include first\sphinxhyphen{}order and all higher\sphinxhyphen{}order interactions associated with each factor and can be estimated calculated using the following:
\begin{equation*}
\begin{split}S_i^T= \frac{E_{x_{\sim i}}[V_{x_i}(x_{\sim i})]}{V(y)} = 1 - \frac{V_{x_{\sim i}}[E_{x_{i}}(x_{\sim i})]}{V(y)}\end{split}
\end{equation*}
\sphinxAtStartPar
This index reveals the expected portion of variance that remains if uncertainty is eliminated in all factors but \(x_i\) {[}\hyperlink{cite.index:id149}{110}{]}. The total sensitivity index is the overall best measure of sensitivity as it captures the full individual and interactive effects of model factors.

\sphinxAtStartPar
Besides the Sobol method, there are some other variance\sphinxhyphen{}based sensitivity analysis methods, such as the Fourier amplitude sensitivity test (FAST; {[}\hyperlink{cite.index:id100}{75}, \hyperlink{cite.index:id151}{112}{]}) and extended\sphinxhyphen{}FAST {[}\hyperlink{cite.index:id152}{113}, \hyperlink{cite.index:id153}{114}{]}, that have been used by the scientific community. However, Sobol remains by far the most common method of this class. Variance\sphinxhyphen{}based techniques have been widely used and have proved to be powerful in a variety of applications. Despite their popularity, some authors have expressed concerns about the methods’ appropriateness in some settings. Specifically, the presence of heavy\sphinxhyphen{}tailed distributions or outliers, or when model outputs are multimodal can bias the sensitivity indices produced by these methods {[}\hyperlink{cite.index:id154}{115}, \hyperlink{cite.index:id155}{116}, \hyperlink{cite.index:id156}{117}{]}. Moment\sphinxhyphen{}independent measures, discussed below, attempt to overcome these challenges.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Put this into practice! Click the following badge to try out an interactive tutorial on performing a sensitivity analysis to discover important factors:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Ffishery\_dynamics.ipynb}{Factor Discovery Jupyter Notebook}
\end{sphinxadmonition}


\subsection{Analysis of Variance (ANOVA)}
\label{\detokenize{3_sensitivity_analysis_the_basics:analysis-of-variance-anova}}
\sphinxAtStartPar
Analysis of Variance (ANOVA) was first introduced by Fisher and others {[}\hyperlink{cite.index:id144}{118}{]} and has since become a popular factor analysis method in physical experiments. ANOVA can be used as a sensitivity analysis method in computational experiments (referred to as factorial ANOVA) with a factorial design of experiment. Note that Sobol can also be categorized as an ANOVA sensitivity analysis method, and that is why Sobol is sometimes referred to as a functional ANOVA {[}\hyperlink{cite.index:id145}{119}{]}. Factorial ANOVA methods are particularly suited for models and problems that have discrete input spaces, significantly reducing the computational time. More information about these methods can be found in {[}\hyperlink{cite.index:id145}{119}, \hyperlink{cite.index:id146}{120}, \hyperlink{cite.index:id147}{121}{]}.


\subsection{Moment\sphinxhyphen{}Independent (Density\sphinxhyphen{}Based) Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:moment-independent-density-based-methods}}
\sphinxAtStartPar
These methods typically compare the entire distribution (i.e., not just the variance) of input and output parameters in order to determine the sensitivity of the output to a particular input variable. Several moment\sphinxhyphen{}independent sensitivity analysis methods have been proposed in recent years. The delta (\(\delta\)) moment\sphinxhyphen{}independent method calculates the difference between unconditional and conditional cumulative distribution functions of the output. The method was first introduced by {[}\hyperlink{cite.index:id142}{122}, \hyperlink{cite.index:id141}{123}{]} and has become widely used in various disciplines. The \(\delta\) sensitivity index is defined as follows:
\begin{equation*}
\begin{split}S_i=\delta_i=\frac{1}{2}E_{x_i}|f_y(y)-f_{y|x_i}(y)|dy\end{split}
\end{equation*}
\sphinxAtStartPar
where \(f_y(y)\) is the probability density function of the entire model output \(y\), and \(f_{y|x_i}(y)\) is the conditional density of \(y\), given that factor \(x_i\) assumes a fixed value. The \(\delta_i\) sensitivity indicator therefore represents the normalized expected shift in the distribution of \(y\) provoked by \(x_i\). Moment\sphinxhyphen{}independent methods are advantageous in cases where we are concerned about the entire distribution of events, such as when uncertain factors lead to more extreme events in a system {[}\hyperlink{cite.index:id47}{14}{]}. Further, they can be used with a pre\sphinxhyphen{}existing sample of data, without requiring a specific sampling scheme, unlike the previously reviewed methods {[}\hyperlink{cite.index:id143}{124}{]}. The \(\delta\) sensitivity index does not include interactions between factors and it is therefore akin to the first order index produced by the Sobol method. Interactions between factors can still be estimated using this method, by conditioning the calculation on more than one uncertain factor being fixed {[}\hyperlink{cite.index:id141}{123}{]}.


\section{How To Choose A Sensitivity Analysis Method: Model Traits And Dimensionality}
\label{\detokenize{3_sensitivity_analysis_the_basics:how-to-choose-a-sensitivity-analysis-method-model-traits-and-dimensionality}}
\sphinxAtStartPar
\hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}}} presents a graphical synthesis of the methods overviewed in this section, with regards to their appropriateness of application based on the complexity of the model at hand and the computational limits on the number of model evaluations afforded. The bars below each method also indicate the sensitivity analysis purposes they are most appropriate to address, which are in turn a reflection of the motivations and research questions the sensitivity analysis is called to address. Computational intensity is measured as a multiple of the number of model factors that are considered uncertain (\(d\)). Increasing model complexity mandates that more advanced sensitivity analysis methods are applied to address potential nonlinearities, factor interactions and discontinuities. Such methods can only be performed at increasing computational expense. For example, computationally cheap linear regression should not be used to assess factors’ importance if the model cannot be proven linear and the factors independent, because important relationships will invariably be missed (recall the example in \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}}}). When computational limits do constrain applications to make simplified assumptions and sensitivity techniques, any conclusions in such cases should be delivered with clear statements of the appropriate caveats.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure3_5classificationofmethods}.png}
\caption{Classification of the sensitivity analysis methods overviewed in this section, with regards to their computational cost (horizontal axis), their appropriateness to model complexity (vertical axis), and the purpose they can be used for (colored bars). d: number of uncertain factors considered; ANOVA: Analysis of Variance; FAST: Fourier Amplitude Sensitivity Test; PRIM: Patient Rule Induction Method; CART: Classification and Regression Trees; SRCC: Spearman’s rank correlation coefficient: NSE: Nash\textendash{}Sutcliffe efficiency; SRC: standardized regression coefficient; PCC: Pearson correlation coefficient}\label{\detokenize{3_sensitivity_analysis_the_basics:id98}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}}\end{figure}

\sphinxAtStartPar
The reader should also be aware that the estimates of computational intensity that are given here are indicative of magnitude and would vary depending on the sampling technique, model complexity and the level of information being asked. For example, a Sobol sensitivity analysis typically requires a sample of size \(n * d+2\) to produce first\sphinxhyphen{} and total\sphinxhyphen{}order indices, where \(d\) is the number of uncertain factors and \(n\) is a scaling factor, selected ad hoc, depending on model complexity {[}\hyperlink{cite.index:id25}{46}{]}. The scaling factor \(n\) is typically set to at least 1000, but it should most appropriately be set on the basis of index convergence. In other words, a prudent analyst would perform the analysis several times with increasing \(n\) and observe at what level the indices converge to stable values {[}\hyperlink{cite.index:id140}{125}{]}. The level should be the minimum sample size used in subsequent sensitivity analyses of the same system. Furthermore, if the analyst would like to better understand the degrees of interaction between factors, requiring second\sphinxhyphen{}order indices, the sample size would have to increase to \(n * 2d+2\) {[}\hyperlink{cite.index:id25}{46}{]}.

\sphinxAtStartPar
Another important consideration is that methods that do not require specific sampling schemes, can be performed in conjunction with others without requiring additional model evaluations. None of the regression\sphinxhyphen{}based methods, for example, require samples of specific structures or sizes, and can be combined with other methods for complementary purposes. For instance, one could complement a Sobol analysis with an application of CART, using the same data, but to address questions relating to factor mapping (e.g., we know factor \(x_i\)  is important for a model output, but we would like to also know which of its values specifically push the output to undesirable states). Lastly, comparing results from different methods performed together can be especially useful in model diagnostic settings. For example, {[}\hyperlink{cite.index:id47}{14}{]} used \(\delta\) indices, first\sphinxhyphen{}order Sobol indices, and \(R^2\)  values from linear regression, all performed on the same factors, to derive insights about the effects on factors on different moments of the output distribution and about the linearity of their relationship.


\section{Software Toolkits}
\label{\detokenize{3_sensitivity_analysis_the_basics:software-toolkits}}\label{\detokenize{3_sensitivity_analysis_the_basics:id91}}
\sphinxAtStartPar
This section presents available open source sensitivity analysis software tools, based on the programming language they use and the methods they support \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-6}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-6}}}. Our review covers five widely used programming languages: R, MATLAB, Julia, Python, and C++, as well as one tool that provides a graphical user interface (GUI). Even though there are other commercial GUI\sphinxhyphen{}based SA tools available, only open source tools were considered within the scope of this overview. Each available SA tool was assessed on the number of SA methods and design of experiments methods it supports. For example, the \sphinxstyleemphasis{sensobol} package in R only supports the variance\sphinxhyphen{}based Sobol method. However, it is the only package we came across that calculates third\sphinxhyphen{}order interactions among parameters. On the other side of the spectrum, there are SA software packages that contain several popular SA methods. For example, \sphinxstyleemphasis{SALib} in Python {[}\hyperlink{cite.index:id139}{126}{]} supports seven different SA methods. The \sphinxstyleemphasis{DifferentialEquations} package is a comprehensive package developed for Julia, and \sphinxstyleemphasis{GlobalSensitivityAnalysis} is another Julia package that has mostly adapted SALib methods. \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-6}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-6}}} also identifies the SA packages that have been updated since 2018, indicating active support and development.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure3_6_softwaretoolkits}.png}
\caption{Sensitivity analysis packages available in different programming language platforms (R, Python, Julia, MATLAB, and C++), with the number of methods they support. Packages supporting more than five methods are indicated in pink. Packages updated since 2018 are indicated with asterisks.}\label{\detokenize{3_sensitivity_analysis_the_basics:id99}}\label{\detokenize{3_sensitivity_analysis_the_basics:figure-3-6}}\end{figure}


\chapter{Sensitivity Analysis: Diagnostic \& Exploratory Modeling}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis-diagnostic-exploratory-modeling}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling::doc}}

\section{Understanding Errors: What Is Controlling Model Performance?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:understanding-errors-what-is-controlling-model-performance}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:understanding-errors}}
\sphinxAtStartPar
Sensitivity analysis is a diagnostic tool when reconciling model outputs with observed data. It is helpful for clarifying how and under what conditions modeling choices (structure, parameterization, data inputs, etc.) propagate through model components and manifest in their effects on model outputs. This exploration is performed through carefully designed perturbation of multiple combinations of input parameters and subsequent evaluation of the model structures that are emerging as controlling factors. Model structure and parameterization are two of the most commonly explored aspects of models that have been a strong focus when evaluating their performance relative to available observations {[}\hyperlink{cite.index:id49}{18}{]}. Addressing these issues plays an important role in establishing credibility in model predictions, particularly in the positivist natural sciences literature. Traditional model evaluations compare the model with observed data, and then rely on expert judgements of its acceptability based on the closeness between simulation and observation with a single metric. However, this approach can be strongly myopic and misleading {[}\hyperlink{cite.index:id50}{127}{]}, as it is often impossible to use one metric to attribute a certain error and to link that with different parts of the model and its parameters. This means that, even when the error or fitting measure between the model estimates and observations is very small, it is not guaranteed that all the components in the model perfectly represent the conceptual reality that the model is abstracting: propagated errors in different parts of the model might cancel out each other, or there might be equifinality {[}\hyperlink{cite.index:id49}{18}{]}. This also implies that the opposite situation does not prove that the model is completely wrong either.

\sphinxAtStartPar
The inherent complexity of a system hinders accepting or rejecting a model based on one performance measure and different types of measures can potentially evaluate components of the model {[}\hyperlink{cite.index:id52}{26}, \hyperlink{cite.index:id51}{27}{]} as essentially a multiobjective problem. In addition, natural systems mimicked by the models contain various interacting components that might act differently across the spatial domain and time periods {[}\hyperlink{cite.index:id30}{51}, \hyperlink{cite.index:id53}{55}{]}. This heterogeneity is lost when a single performance measure is used, as a result of the inherent loss of process information occurring when transitioning from a highly dimensional and interactive system to a highly aggregated averaging of spatial or temporal output errors {[}\hyperlink{cite.index:id54}{16}{]}. One specific measure might be a good representative of a phenomena at a particular condition (e.g., time of year or location), but not at a different state of the system.

\sphinxAtStartPar
Therefore, diagnostic error analyses should consider multiple error signatures across different scales and states of concern when seeking to understand how our theoretical conceptions relate to observed data (\hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-1}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-1}}}). Diverse error signatures can be used to measure the consistency of underlying processes and behaviors of the model and to evaluate the dynamics of model controls under changing temporal and spatial conditions {[}\hyperlink{cite.index:id56}{128}{]}. Within this framework, even minimal information extracted from the data is beneficial as it helps us to understand the complexity of the model which potentially unearths structural inadequacies in the model, and even the underlying data itself. In this context, proper selection of measures of model performance and the number of measures could play consequential roles in our understanding of the model and its predictions {[}\hyperlink{cite.index:id55}{129}{]}.

\sphinxAtStartPar
As discussed earlier, instead of the traditional focus on using deterministic prediction that results in a single error measure, many plausible states and spaces could be searched for making different inferences and quantifying uncertainties. This process also requires estimates of prior probability distributions of all the important parameters and quantification of model behavior across input space. One strategy to reduce the search space is filtering of some model alternatives that are not consistent with observations and known system behaviors. Those implausible parts of the search space can be referred to as non\sphinxhyphen{}physical or non\sphinxhyphen{}behavioral alternatives {[}\hyperlink{cite.index:id29}{50}, \hyperlink{cite.index:id158}{104}{]}. This step is conducted before the Bayesian calibration exercise (see \hyperref[\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}]{Section \ref{\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}}}).

\sphinxAtStartPar
A comprehensive model diagnostic workflow typically entails the components demonstrated in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-1}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-1}}}. The workflow begins with the selection of model input parameters and their plausible ranges. After the parameter selection, we need to specify the design of experiment (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}}}) and the sensitivity analysis method (\hyperref[\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-methods}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-methods}}}) to be used. As previously discussed, these methods require different numbers of model simulations, and each method provides a different insights into the direct effects and interactions of the parameters. In addition, the simulation time of the model and the available computational resources are two of the primary considerations that influence these decisions. After identifying the appropriate methods, we generate a matrix of input parameters, where each set of input parameters will be used to conduct a model simulation. The model can include one or more output variables that fluctuate in time and space. The next step is to analyze model performance by comparing model outputs with observations. As discussed earlier, the positivist model evaluation paradigm focuses on a single model performance metric (error), leading to a loss of information about model parameters and the suitability of the model’s structure. However, a thorough investigation of the temporal and spatial signatures of model outputs using various performance metrics or time\sphinxhyphen{} and space\sphinxhyphen{}varying sensitivity analyses can shed more light on the fitness of each parameter set and the model’s internal structure. This analysis provides diagnostic feedback on the importance and range of model parameters and can guide further improvement of the model algorithm.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{figure4_1_diagnostic_workflow}.png}
\caption{Diagnostic evaluation of model fidelity using sensitivity analysis methods.}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:id50}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-1}}\end{figure}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Put this into practice! Click the following badge to try out an interactive tutorial on implementing a time\sphinxhyphen{}varying sensitivity analysis of HYMOD model parameters:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fhymod.ipynb}{HYMOD Jupyter Notebook}
\end{sphinxadmonition}


\section{Consequential Dynamics: What is Controlling Model Behaviors of Interest?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-dynamics-what-is-controlling-model-behaviors-of-interest}}
\sphinxAtStartPar
Consequential changes in dynamic systems can take many forms, but most dynamic behavior can be categorized in a few basic patterns. Feedback structures inherent in a system, be they positive or negative, generate these patterns which can be grouped into three groups for the simplest of systems: exponential growth, goal seeking, and oscillation (\hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-2}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-2}}}). A positive (or self\sphinxhyphen{}reinforcing) feedback gives rise to exponential growth, a negative (or self\sphinxhyphen{}correcting) feedback gives rise to a goal seeking mode, and negative feedbacks with time delays give rise to oscillatory behavior. Nonlinear interactions between the system’s feedback structures can give rise to more complex dynamic behavior modes, examples of which are also shown in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-2}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-2}}}, adapted from Sterman {[}\hyperlink{cite.index:id57}{130}{]}.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure4_2_behavior_modes}.png}
\caption{Common modes of behavior in dynamic systems, occurring based on the presence of positive and negative feedback relationships, and linear and non\sphinxhyphen{}linear interactions. Adapted from Sterman {[}\hyperlink{cite.index:id57}{130}{]}.}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:id51}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-2}}\end{figure}

\sphinxAtStartPar
The nature of feedback processes in a dynamic system shapes its fundamental behavior: positive feedbacks generate their own growth, negative feedbacks self\sphinxhyphen{}limit, seeking balance and equilibrium. In this manner, feedback processes give rise to different regimes, multiples of which could be present in each mode of behavior. Consider a population of mammals growing exponentially until it reaches the carrying capacity of its environment (referred to as S\sphinxhyphen{}shaped growth). When the population is exponentially growing, the regime is dominated by positive feedback relationships that reinforce its growth. As the population approaches its carrying capacity limit, negative feedback structures begin to dominate, counteracting the growth and establishing a stable equilibrium. Shifts between regimes can be thought of as tipping points, mathematically defined as unstable equilibria, where the presence of positive feedbacks amplifies disturbances and moves the system to a new equilibrium point. In the case of stable equilibria, the presence of negative feedbacks dampens any small disturbance and maintains the system at a stable state. As different feedback relationships govern each regime, different factors (those making up each feedback mechanism) are activated and shape the states the system is found in, as well as define the points of equilibria.

\sphinxAtStartPar
For simple stylized models with a small number of states, system dynamics analysis can analytically derive these equilibria, the conditions for their stability and the factors determining them. The ability for this to be performed is, however, significantly challenged when it comes to systems that attempt to more closely resemble real complex systems. We argue this is the case for several reasons. First, besides generally exhibiting complex nonlinear dynamics, real world systems are also made up from larger numbers of interacting elements, which often makes the analytic derivation of system characteristics intractable {[}\hyperlink{cite.index:id59}{131}, \hyperlink{cite.index:id58}{132}{]}. Second, human\sphinxhyphen{}natural systems temporally evolve and transform when human state\sphinxhyphen{}aware action is present. Consider, for instance, humans recreationally hunting the aforementioned population of mammals. Humans act based on the mammal population levels by enforcing hunting quotas or establishing protected territories or eliminating other predators. The mammal population reacts in a response, giving birth to ever changing state\sphinxhyphen{}action\sphinxhyphen{}consequence feedbacks, the path dependencies of which become difficult to diagnose and understand (e.g., {[}\hyperlink{cite.index:id60}{133}{]}). Trying to simulate the combination of these two challenges (large numbers of state\sphinxhyphen{}aware agents interacting with a natural resource and with each other) produces intractable models that require advanced heuristics to analyze their properties and establish useful inferences.

\sphinxAtStartPar
Sensitivity analysis paired with exploratory modeling methods offers a promising set of tools to address these challenges. We present a simple demonstrative application based on Quinn \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id61}{134}{]}. This stylistic example was first developed by Carpenter \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id163}{135}{]} and represents a town that must balance its agricultural and industrial productivity with the pollution it creates in a downstream lake. Increased productivity allows for increased profits, which the town aims to maximize, but it also produces more pollution for the lake. Too much phosphorus pollution can cause irreversible eutrophication, a process known as “tipping” the lake. The model of phosphorus in the lake \(X_t\) at time \(t\) is governed by:
\begin{equation*}
\begin{split}X_{t+1}= X_{t}+a_{t}+\frac{X_{t}^q} {1+X_{t}^q}-bX_t+\varepsilon\end{split}
\end{equation*}
\sphinxAtStartPar
where \(a_t \in [0,0.1]\) is the town’s pollution release at each timestep, \(b\) is the natural decay rate of phosphorus in the lake, \(q\) defines the lake’s recycling rate (primarily through sediments), and \(\varepsilon\) represents uncontrollable natural inflows of pollution modeled as a log\sphinxhyphen{}normal distribution with a given mean, \(\mu\), and standard deviation \(\sigma\).

\sphinxAtStartPar
Panels (a\sphinxhyphen{}c) in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}}} plot the fluxes of phosphorus into the lake versus the mass accumulation of phosphorus in the lake. The red line corresponds to the phosphorus sinks in the lake (natural decay), given by \(bX_t\). The grey shaded area represents the lake’s phosphorus recycling flux, given by \(\frac{X_{t}^q} {1+X_{t}^q}\). The points of intersection indicate the system’s equilibria, two of which are stable, and one is unstable (also known as the tipping point). The stable equilibrium in the bottom left of the figure reflects an oligotrophic lake, whereas the stable equilibrium in the top right represents a eutrophic lake. With increasing phosphorus values, the tipping point can be crossed, and the lake will experience irreversible eutrophication, as the recycling rate would exceed the removal rate even if the town’s pollution became zero. In the absence of anthropogenic and natural inflows of pollution in the lake (\(a_t\) and \(\varepsilon\) respectively), the area between the bottom\sphinxhyphen{}left black point and the white point in the middle can be considered as the safe operating space, before emission levels cross the tipping point.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure4_3_lake_problem_fluxes}.png}
\caption{Fluxes of phosphorus with regards to mass of phosphorus in the lake and sensitivity analysis results, assuming \(b=0.42\) and \(q=2\). (a) Fluxes of phosphorus assuming no emmisions policy and no natural inflows. (b\sphinxhyphen{}c) Fluxes phosphorus when applying two different emissions policies. The “Best economic policy” and the “Most realiable policy” have been identified by Quinn \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id61}{134}{]} and can be found at Quinn {[}\hyperlink{cite.index:id164}{136}{]}. (d) Results of a sensitivity analysis on the parameters of the model most consequential to the reliability of the “Most reliable policy”. The code to replicate the sensitivity analysis can be found at Hadka {[}\hyperlink{cite.index:id165}{137}{]}.}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:id52}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}}\end{figure}

\sphinxAtStartPar
The town has identified two potential policies that can be used to manage this lake, one that maximizes its economic profits (“best economic policy”) and one that maximizes the time below the tipping point (“most reliable policy”). Panels (b\sphinxhyphen{}c) in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}}} add the emissions from these policies to the recycling flux and show how the equilibria points shift as a result. In both cases the stable oligotrophic equilibrium increases and the tipping point decreases, narrowing the safe operating space {[}\hyperlink{cite.index:id59}{131}, \hyperlink{cite.index:id166}{138}{]}. The best economic policy results in a much narrower space of action, with the tipping point very close to the oligotrophic equilibrium. The performance of both policies depends significantly on the system parameters. For example, a higher value of \(b\), the natural decay rate, would shift the red line upward, moving the equilibria points and widening the safe operating space. Inversely, a higher value of \(q\), the lake’s recycling rate, would shift the recycling line upward, moving the tipping point lower and decreasing the safe operating space. The assumptions under which these policies were identified are therefore critical to their performance and any potential uncertainty in the parameter values could be detrimental to the system’s objectives being met.

\sphinxAtStartPar
Sensitivity analysis can be used to clarify the role these parameters play on policy performance. \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}}} (d) shows the results of a Sobol sensitivity analysis on the reliability of the “most reliable” policy in a radial convergence diagram. The significance of each parameter is indicated by the size of circles corresponding to it. The size of the interior dark circle indicates the parameter’s first\sphinxhyphen{}order effects and the size of the exterior circle indicates the parameter’s total\sphinxhyphen{}order effects. The thickness of the lines between two parameters indicated the extent of their interaction (second\sphinxhyphen{}order effects). In this case, parameters \(b\) and \(q\) appear to have the most significant importance on the system, followed by the mean, \(\mu\), of the natural inflows. All these parameters function in a manner that shifts the location of the three equilibria and therefore policies that are identified ignoring this parametric uncertainty might fail to meet their intended goals.

\sphinxAtStartPar
It is worth mentioning that current sensitivity analysis methods are somewhat challenged in addressing several system dynamics analysis questions. The fundamental reason is that sensitivity analysis methods and tools have been developed to gauge numerical sensitivity of model output to changes in factor values. This is natural, as most simulation studies (e.g., all aforementioned examples) have been traditionally concerned with this type of sensitivity. In system dynamics modeling, however, a more important and pertinent concern is changes between regimes or between behavior modes (also known as bifurcations) as a result of changes in model factors {[}\hyperlink{cite.index:id57}{130}, \hyperlink{cite.index:id63}{139}{]}. This poses two new challenges. First, identifying a change in regime depends on several characteristics besides a change in output value, like the rate and direction of change. Second, behavior mode changes are qualitative and discontinuous, as equilibria change in stability but also move in and out of existence.

\sphinxAtStartPar
Despite these challenges, recent advanced sensitivity analysis methods can help illuminate which factors in a system are most important in shaping boundary conditions (tipping points) between different regimes and determining changes in behavior modes. Reviewing such methods is outside the scope of this text, but the reader is directed to the examples of Eker \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id64}{23}{]} and Hadjimichael \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id60}{133}{]}, who apply parameterised perturbation on the functional relationships of a system to study the effects of model structural uncertainty on model outputs and bifurcations, and Hekimoğlu and Barlas {[}\hyperlink{cite.index:id63}{139}{]} and Steinmann \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id65}{140}{]} who, following wide sampling of uncertain inputs, cluster the resulting time series in modes of behavior and identify most important factors for each.


\section{Consequential Scenarios: What is Controlling Consequential Outcomes?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios-what-is-controlling-consequential-outcomes}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios}}
\sphinxAtStartPar
As overviewed in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}]{Section \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}}}, most models are abstractions of systems in the real world. When sufficient confidence has been established in a model, it can then act as a surrogate for the actual system, in that the consequences of potential stressors, proposed actions or other changes can be evaluated by computer model simulations {[}\hyperlink{cite.index:id15}{36}{]}. A model simulation then represents a computational experiment, which can be used to assess how the modeled system would behave should the various changes come to be. Steven Bankes coined the term exploratory modeling to describe the use of large sets of such computational experiments to investigate their implications on the system. \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}} presents a typical workflow of an exploratory modeling application. Exploratory modeling approaches typically use sampling designs to generate large ensembles of states that represent combinations of changes happening together, spanning the entire range of potential values a factor might take (indicated in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}} by numbers 2\sphinxhyphen{}5). This perspective on modeling is particularly relevant to studies making long term projections into the future.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure4_4_exploratory_workflow}.png}
\caption{A typical exploratory modeling workflow}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:id53}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}\end{figure}

\sphinxAtStartPar
In the long\sphinxhyphen{}term policy analysis literature, exploratory modeling has prominently placed itself as an alternative to traditional narrative scenario or assumptions\sphinxhyphen{}based planning approaches, in what can be summarized in the following two\sphinxhyphen{}pronged critique {[}\hyperlink{cite.index:id15}{36}, \hyperlink{cite.index:id67}{141}, \hyperlink{cite.index:id66}{142}{]}. The most prevalent criticism sees that the future and how it might evolve is both highly complex and deeply uncertain. Despite its benefits for interpretation and intuitive appeal, a small number of scenarios invariably misses many other potential futures that did not get selected as sufficiently representative of the future. This is especially the case for aggregate, narrative scenarios that describe simultaneous changes in multiple sectors together (e.g., “increased energy demand, combined with high agricultural land use and large economic growth”), such as the emission scenarios produced by the Intergovernmental Panel on Climate Change {[}\hyperlink{cite.index:id68}{143}{]}. The bias introduced by this reduced set of potential changes can skew inferences drawn from the model, particularly when the original narrative scenarios are focused on a single or narrow set of measures of system behavior.

\sphinxAtStartPar
The second main criticism of traditional narrative scenario\sphinxhyphen{}based planning methods is that they provide no systematic way to distinguish which of the constituent factors lead to the undesirable consequences produced by a scenario. Narrative scenarios (e.g., the scenario matrix framework of RCPs\sphinxhyphen{}SSPs\sphinxhyphen{}SPAs; {[}\hyperlink{cite.index:id69}{144}{]}) encompass multiple changes happening together selected to span the range of potential changes but are not typically generated in a systematic factorial manner that considers the multiple ways the factors can be combined. This has two critical limitations. It obfuscates the role each component factor plays in the system, both in isolation and in combination with others (e.g., “is it the increased energy demand or the high agricultural land use that cause unbearable water stress?”). It also renders the delineation of how much change in a factor is critical near impossible. Consider, for example, narrative scenario A with a 5\% increase in energy demand, and scenario B with a 30\% increase in energy demand, which would have dire consequences. At which point between 5\% and 30\% do the dire consequences actually begin to occur? Such questions cannot be answered without a wide exploration of the space of potential changes. It should be noted that for some levels of model complexity and computational demands (e.g., global\sphinxhyphen{}scale models) there is little feasible recourse beyond the use of narrative scenarios.

\sphinxAtStartPar
Exploratory modeling is typically paired with scenario discovery methods (indicated by number 9 in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}}) that identify which of the scenarios (also known as states of the world) generated indeed have consequences of interest for stakeholders and policy makers, in an approach referred to as ensemble\sphinxhyphen{}based scenario\sphinxhyphen{}discovery {[}\hyperlink{cite.index:id16}{45}, \hyperlink{cite.index:id71}{102}, \hyperlink{cite.index:id70}{103}{]}. This approach therefore flips the planning analysis from one that attempts to predict future system conditions to one that attempts to discover the (un)desirable future conditions. Ensemble\sphinxhyphen{}based scenario discovery can thus inform what modeling choices yield the most consequential behavioral changes or outcomes, especially when considering deeply uncertain, scenario\sphinxhyphen{}informed projections {[}\hyperlink{cite.index:id12}{8}, \hyperlink{cite.index:id72}{145}{]}. The relative likelihoods and relevance of the discovered scenarios can be subsequently evaluated by the practitioners a posteriori, within a richer context of knowing the wider set of potential consequences {[}\hyperlink{cite.index:id73}{146}{]}. This can include changing how an analysis is framed (number 10 in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}}). For instance, one could initially focus on ensemble modeling of vulnerability using a single uncertain factor that is assumed to be well characterized by historical observations (e.g., streamflow; this step is represented by numbers 2\sphinxhyphen{}3 in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}}). The analysis can then shift to include projections of more factors treated as deeply uncertain (e.g., urbanization, population demands, temperature, and snow\sphinxhyphen{}melt) to yield a far wider space of challenging projected futures. UC experiments contrasting these two framings can be highly valuable for tracing how vulnerability inferences change as the modeled space of futures expands from the historical baseline {[}\hyperlink{cite.index:id61}{134}{]}.

\sphinxAtStartPar
An important nuance to be clarified here is that the focus or purpose of a modeling exercise plays a major role in whether a given factor of interest is considered well\sphinxhyphen{}characterized or deeply uncertain. Take the example context of characterizing temperature or streamflow extremes, where for each state variable of interest for a given location of focus there is a century of historical observations. Clearly, the observation technologies will have evolved over time uniquely for temperature and streamflow measurements and they likely lack replicate experiments (data uncertainty). A century of record will be insufficient to capture very high impact and rare extreme events (i.e., increasingly poor structural/parametric inference for the distributions of specific extreme single or compound events). The mechanistic processes as well as their evolving variability will be interdependent but uniquely different for each of these state variables. A large body of statistical literature exists focusing on the topics of synthetic weather {[}\hyperlink{cite.index:id74}{81}, \hyperlink{cite.index:id75}{82}{]} or streamflow {[}\hyperlink{cite.index:id76}{83}, \hyperlink{cite.index:id77}{84}{]} generation that provides a rich suite of approaches for developing history\sphinxhyphen{}informed, well\sphinxhyphen{}characterized stochastic process models to better estimate rare individual or compound extremes. These history\sphinxhyphen{}focused approaches can be viewed as providing well\sphinxhyphen{}characterized quantifications of streamflow or temperature distributions; however, they do not capture how coupled natural\sphinxhyphen{}human processes can fundamentally change their dynamics when transitioning to projections of longer\sphinxhyphen{}term futures (e.g., streamflow and temperature in 2055). Consequently, changing the focus of the modeling to making long term projections of future streamflow or temperature now makes these processes deeply uncertain.

\sphinxAtStartPar
Scenario discovery methods (number 9 in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-4}}}) can be qualitative or quantitative and they generally attempt to distinguish futures in which a system or proposed policies to manage the system meet or miss their goals {[}\hyperlink{cite.index:id70}{103}{]}. The emphasis placed by exploratory modeling on model outputs that have decision relevant consequences represents a shift toward a broader class of metrics that are reflective of the stakeholders’ concerns, agency and preferences (also discussed in \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}]{Section \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}}}). As a result, sensitivity analysis and scenario discovery methods in this context are therefore applied to performance metrics that go beyond model error but are rather focused on broader metrics such as the resilience of a sector, the reliability of a process, or the vulnerability of a population in the face of uncertainty. In exploratory modeling literature, this metric is most typically—but not always—a measure of robustness (number 8 in Fig. 13). Robustness is a property of a system or a design choice capturing its insensitivity to uncertainty and can be measured via a variety of means, most recently reviewed by Herman \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id78}{147}{]} and McPhail \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id55}{129}{]}.

\sphinxAtStartPar
Scenario discovery is typically performed through the use of algorithms applied on large databases of model runs, generated through exploratory modeling, with each model run representing the performance of the system in one potential state of the world. The algorithms seek to identify the combinations of factor values (e.g., future conditions) that best distinguish the cases in which the system does or does not meet its objectives. The most widely known classification algorithms are the Patient Rule Induction Method (PRIM; {[}\hyperlink{cite.index:id79}{97}{]}) and Classification and Regression Trees (CART; {[}\hyperlink{cite.index:id80}{98}{]}). These factor mapping algorithms create orthogonal boundaries (multi\sphinxhyphen{}dimensional hypercubes) between states of the world that are successful or unsuccessful in meeting the system’s goals {[}\hyperlink{cite.index:id81}{65}{]}. The algorithms attempt to strike a balance between simplicity of classification (and as a result, interpretability) and accuracy {[}\hyperlink{cite.index:id16}{45}, \hyperlink{cite.index:id70}{103}, \hyperlink{cite.index:id82}{148}{]}.

\sphinxAtStartPar
Even though these approaches have been shown to yield interpretable and relevant scenarios {[}\hyperlink{cite.index:id83}{149}{]}, several authors have pointed out the limitations of these methods with regards to their division of space in orthogonal behavioral and non\sphinxhyphen{}behavioral regions {[}\hyperlink{cite.index:id84}{150}{]}. Due to their reliance on boundaries orthogonal to the uncertainty axes, PRIM and CART cannot capture interactions between the various uncertain factors considered, which can often be significant {[}\hyperlink{cite.index:id85}{151}{]}. More advanced methods have been proposed to address this drawback, with logistic regression being perhaps the most prominent {[}\hyperlink{cite.index:id85}{151}, \hyperlink{cite.index:id86}{152}, \hyperlink{cite.index:id87}{153}{]}. Logistic regression can produce boundaries that are not necessarily orthogonal to each uncertainty axis, nor necessarily linear, if interactive terms between two parameters are used to build the regression model. It also describes the probability that a state of the world belongs to the scenarios that lead to failure. This feature allows users to define regions of success based on a gradient of estimated probability of success in those worlds, unlike PRIM which only classifies states of the world in two regions {[}\hyperlink{cite.index:id85}{151}, \hyperlink{cite.index:id202}{154}{]}.

\sphinxAtStartPar
Another more advanced factor mapping method is boosted trees {[}\hyperlink{cite.index:id89}{99}, \hyperlink{cite.index:id88}{155}{]}. Boosted trees can avoid two limitations inherent to the application of logistic regression: i) to build a nonlinear classification model the interactive term between two uncertainties needs to be pre\sphinxhyphen{}specified and cannot be discovered (e.g., we need to know a\sphinxhyphen{}priori whether factor \(x_1\) interacts with \(x_2\) in a relationship that looks like \(x_1\)·\(x_2\) or \(x_1^{x_2}\)); and ii) the subspaces defined are always convex. The application of such a factor mapping algorithm is limited in the presence of threshold\sphinxhyphen{}based rules with discrete actions in a modeled system (e.g., “if network capacity is low, build new infrastructure”), which results in failure regions that are nonlinear and non\sphinxhyphen{}convex {[}\hyperlink{cite.index:id84}{150}{]}. Boosting works by creating an ensemble of classifiers and forcing some of them to focus on the hard\sphinxhyphen{}to\sphinxhyphen{}learn parts of the problem, and others to focus on the easy\sphinxhyphen{}to\sphinxhyphen{}learn parts. Boosting applied to CART trees can avoid the aforementioned challenges faced by other scenario discovery methods, while resisting overfitting {[}\hyperlink{cite.index:id90}{156}{]}, assuring the identified success and failure regions are still easy to interpret.

\sphinxAtStartPar
Below we provide an example application of two scenario discovery methods, PRIM and logistic regression, using the lake problem introduced in the previous section. From the sensitivity analysis results presented in \hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-3}}} (d), we can already infer that parameters \(b\) and \(q\) have important effects on model outputs (i.e., we have performed factor prioritization). Scenario discovery (i.e., factor mapping) complements this analysis by further identifying the specific values of b and q that can lead to consequential and undesirable outcomes. For the purposes of demonstration, we can assume the undesirable outcome in this case is defined as the management policy failing to achieve 90\% reliability in a state of the world.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure14lake_problem_SD}.png}
\caption{Scenario discovery for the lake problem, using (a) PRIM and (b) logistic regression.}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:id54}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-5}}\end{figure}

\sphinxAtStartPar
\hyperref[\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-5}]{Fig.\@ \ref{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:figure-4-5}}} shows the results of scenario discovery, performed through (a) PRIM and (b) logistic regression. Each point in the two panels indicates a potential state of the world, generated through Latin Hypercube Sampling. Each point is colored by whether the policy meets the above performance criterion, with blue indicating success and red indicating failure. PRIM identifies several orthogonal areas of interest, one of which is shown in panel (a). As discussed above, this necessary orthogonality limits how PRIM identifies areas of success (the area within the box). As factors  \(b\) and \(q\) interact in this system, the transition boundary between the regions of success and failure is not orthogonal to any of the axes. As a result, a large number of points in the bottom right and the top left of the figure are left outside of the identified region. Logistic regression can overcome this limitation by identifying a diagonal boundary between the two regions, seen in panel (b). This method also produces a gradient of estimated probability of success across these regions.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Put this into practice! Click the following link to try out an interactive tutorial on performing factor mapping using logistic regression:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fbasin\_users\_logistic\_regression.ipynb}{Logistic Regression Jupyter Notebook}
\end{sphinxadmonition}


\chapter{Conclusion}
\label{\detokenize{5_conclusion:conclusion}}\label{\detokenize{5_conclusion:id1}}\label{\detokenize{5_conclusion::doc}}
\sphinxAtStartPar
As noted in the Introduction (\hyperref[\detokenize{1_introduction:introduction}]{Section \ref{\detokenize{1_introduction:introduction}}}), the computational and conceptual challenges of the multi\sphinxhyphen{}model, transdisciplinary workflows that characterize ambitious projects such as IM3 have limited UC and UQ analyses. Moreover, the very nature and purpose of modeling and diagnostic model evaluation can have very diverse philosophical framings depending on the disciplines involved (see \hyperref[\detokenize{1_introduction:figure-1-1}]{Fig.\@ \ref{\detokenize{1_introduction:figure-1-1}}} and \hyperref[\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}]{Section \ref{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives}}}). The guidance provided in this text can be used to frame consistent and rigorous experimental designs for better understanding the consequences and insights from our modeling choices when seeking to capture complex human\sphinxhyphen{}natural systems. The progression of sections of this text provide a thorough introduction of the concepts and definitions of diagnostic model evaluation, sensitivity analysis and UC. In addition, we comprehensively discuss how specific modeling objectives and applications should guide the selection of appropriate techniques; broadly, these can include model diagnostics, in\sphinxhyphen{}depth analysis of the behavior of the abstracted system, and projections under conditions of deep uncertainty. This text also contains a detailed presentation of the main sensitivity analysis methods and a discussion of their features and main limitations. Readers are also provided with an overview of computer tools and platforms that have been developed and could be considered in addressing IM3 scientific questions. The appendices of this text include an overview of UQ methods, a terminology glossary of the key concepts as well as example test cases and scripts to showcase various UC related capabilities.

\sphinxAtStartPar
Although we distinguish the UC and UQ model diagnostics, the reader should note that we suggest an overall consistent approach to both in this text by emphasizing “exploratory modeling” (see review by Moallemi \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id12}{8}{]}). Although data support, model complexity, and computational limits strongly distinguish the feasibility and appropriateness of various UC diagnostic tools (e.g., see \hyperref[\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}]{Fig.\@ \ref{\detokenize{3_sensitivity_analysis_the_basics:figure-3-5}}}), we overall recommend that modelers view their work through the lens of cycles of learning. Iterative and deliberative exploration of model\sphinxhyphen{}based hypotheses and inferences for transdisciplinary teams is non\sphinxhyphen{}trivial and ultimately critical for mapping where innovations or insights are most consequential. Overall, we recommend approaching modeling with an openness to the diverse disciplinary perspectives such as those mirrored by the IM3 family of models in a progression from evaluating models relative to observed history to advanced formalized analyses to make inferences on multi\sphinxhyphen{}sector, multi\sphinxhyphen{}scale vulnerabilities and resilience. Exploratory modeling approaches can help fashion experiments with large numbers of alternative hypotheses on the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path\sphinxhyphen{}dependent changes in the form and function of coupled human\sphinxhyphen{}natural systems {[}\hyperlink{cite.index:id130}{37}{]}. This text guides the reader through the use of sensitivity analysis and uncertainty methods across the diverse perspectives that have shaped modern diagnostic and exploratory modeling.


\chapter{Glossary}
\label{\detokenize{6_glossary:glossary}}\label{\detokenize{6_glossary::doc}}

\chapter{Uncertainty Quantification}
\label{\detokenize{A1_Uncertainty_Quantification:uncertainty-quantification}}\label{\detokenize{A1_Uncertainty_Quantification:a1-uncertainty-quantification}}\label{\detokenize{A1_Uncertainty_Quantification::doc}}

\section{Introduction}
\label{\detokenize{A1_Uncertainty_Quantification:introduction}}
\sphinxAtStartPar
As defined in \hyperref[\detokenize{1_introduction:introduction}]{Section \ref{\detokenize{1_introduction:introduction}}}, uncertainty quantification (UQ) refers to the formal focus on the full specification of likelihoods as well as distributional forms necessary to infer the joint  probabilistic response across all modeled factors of interest {[}\hyperlink{cite.index:id11}{7}{]}. This is in contrast to UC (the primary focus of the main document of this book), which is instead aimed at identifying which modeling choices yield the most consequential changes or outcomes and exploring alternative hypotheses related to the form and function of modeled systems {[}\hyperlink{cite.index:id12}{8}, \hyperlink{cite.index:id13}{9}{]}.

\sphinxAtStartPar
UQ is important for quantifying the relative merits of hypotheses for at least three main reasons. First, identifying model parameters that are consistent with observations is an important part of model development. Due to several effects, including correlations between parameters, simplified or incomplete model structures (relative to the full real\sphinxhyphen{}world dynamics), and uncertainty in the observations, many different combinations of parameter values can be consistent with the model structure and the observations to varying extents. Accounting for this uncertainty is conceptually preferable to selecting a single “best fit” parameter vector, particularly as consistency with historical or present observations does not necessarily  guarantee skillful future projections.

\sphinxAtStartPar
The act of quantification requires specific assumptions about distributional forms and likelihoods, which may be more or less justified depending on prior information about the system or model behavior. As a result, UQ is well\sphinxhyphen{}suited for studies accounting for or addressing hypotheses related to systems with a relatively large amount of available data and models which are computationally inexpensive, particularly when the emphasis is on prediction. As shown in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-1}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-1}}}, there is a fundamental tradeoff between the available number of model evaluations (for a fixed computational budget) and the number of parameters treated as uncertain. Sensitivity analyses are therefore part of a typical UQ workflow to identify which factors can be fixed and which ought to be prioritized in the UQ.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figureA1_1_UQ_approaches}.png}
\caption{Overview of selected existing approaches for uncertainty quantification and their appropriateness given the number of uncertain model parameters and the number of available model simulations. Green shading denotes regions suitable for uncertainty quantification and red shading indicates regions more appropriate for uncertainty characterization.}\label{\detokenize{A1_Uncertainty_Quantification:id36}}\label{\detokenize{A1_Uncertainty_Quantification:figure-a1-1}}\end{figure}

\sphinxAtStartPar
The choice of a particular UQ method depends on both the desired level of quantification and the ability to navigate the tradeoff between computational expense and the number of uncertain parameters (\hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-1}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-1}}}). For example, Markov chain Monte Carlo with a full system model can provide an improved representation of uncertainty compared to the coarser pre\sphinxhyphen{}calibration approach {[}\hyperlink{cite.index:id37}{157}{]}, but requires many more model evaluations. The use of a surrogate model to approximate the full system model can reduce the number of needed model evaluations by several orders of magnitude, but the uncertainty quantification can only accommodate a limited number of parameters.

\sphinxAtStartPar
The remainder of this appendix will focus on introducing workflows for particular UQ methods, including a brief discussion of advantages and limitations.


\section{Parametric Bootstrap}
\label{\detokenize{A1_Uncertainty_Quantification:parametric-bootstrap}}
\sphinxAtStartPar
The parametric bootstrap {[}\hyperlink{cite.index:id14}{158}{]} refers to a process of model recalibration to alternate realizations of the data. The bootstrap was originally developed to estimate standard errors and confidence intervals without ascertaining key assumptions that might not hold given the available data. In a setting where observations can be viewed as independent realizations of an underlying stochastic process, a sufficiently rich dataset can be treated as a population representing the data distribution. New datasets are then generated by resampling from the data with replacement, and the model can be refit to each new dataset using maximum\sphinxhyphen{}likelihood estimation. The resulting distribution of estimates can then be viewed as a representation of parametric uncertainty.

\sphinxAtStartPar
A typical workflow for the parametric bootstrap is shown in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-2}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-2}}}. After identifying outputs of interest and preparing the data, the parametric model is fit by some procedure such as minimizing root\sphinxhyphen{}mean\sphinxhyphen{}square\sphinxhyphen{}error or maximizing the likelihood. Alternate datasets are constructed by resampling from the population or by generating new samples from the fitted data\sphinxhyphen{}generating process. It is important at this step that the resampled quantities are independent of one another. For example, in the context of temporally\sphinxhyphen{} or spatially\sphinxhyphen{}correlated data, such as time series, the raw observations cannot be treated as independent realizations. However, the residuals resulting from fitting the model to the data could be (depending on their structure). For example, if the residuals are treated as independent, they can then be resampled with replacement, and these residuals added to the original model fit to create new realizations. If the residuals are assumed to be the result of an autoregressive process, this process could be fit to the original residual series and new residuals be created using this model {[}\hyperlink{cite.index:id35}{159}{]}. The model is then refit to each new realization.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figureA1_2_bootstrap_workflow}.png}
\caption{Workflow for the parametric bootstrap.}\label{\detokenize{A1_Uncertainty_Quantification:id37}}\label{\detokenize{A1_Uncertainty_Quantification:figure-a1-2}}\end{figure}

\sphinxAtStartPar
The bootstrap is computationally convenient, particularly as the process of fitting the model to each realization can be easily parallelized. This approach also requires minimal prior assumptions. However, due to the assumption that the available data are representative of the underlying data distribution, the bootstrap can neglect key uncertainties which might influence the results. For example, when using an autoregressive process to generate new residuals, uncertainty in the autocorrelation parameter and innovation variance is neglected, which may bias estimates of, for example, low\sphinxhyphen{}probability but high\sphinxhyphen{}impact events {[}\hyperlink{cite.index:id36}{160}{]}.


\section{Pre\sphinxhyphen{}Calibration}
\label{\detokenize{A1_Uncertainty_Quantification:pre-calibration}}
\sphinxAtStartPar
Pre\sphinxhyphen{}calibration {[}\hyperlink{cite.index:id38}{19}, \hyperlink{cite.index:id39}{161}, \hyperlink{cite.index:id40}{162}{]} involves the identification of a plausible set of parameters using some prespecified screening criterion, such as the distance from the model results to the observations (based on an appropriate metric for the desired matching features, such as root\sphinxhyphen{}mean\sphinxhyphen{}squared error). A typical workflow is shown in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-3}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-3}}}. Parameter values are obtained by systematically sampling the input space (see \hyperref[\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}]{Section \ref{\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}}}). After the model is evaluated at the samples, only those passing the distance criterion are retained. This selects a subset of the parameter space as “plausible” based on the screening criterion, though there is no assignment of probabilities within this plausible region.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figureA1_3_precal_workflow}.png}
\caption{Workflow for pre\sphinxhyphen{}calibration.}\label{\detokenize{A1_Uncertainty_Quantification:id38}}\label{\detokenize{A1_Uncertainty_Quantification:figure-a1-3}}\end{figure}

\sphinxAtStartPar
Pre\sphinxhyphen{}calibration can be useful for models which are inexpensive enough that a reasonable
number of samples can be used to represent the parameter space, but which are too expensive to facilitate full uncertainty quantification. High\sphinxhyphen{}dimensional parameter spaces, which can be problematic for the uncertainty quantification methods below, may also be explored using pre\sphinxhyphen{}calibration. One key prerequisite to using this method is the ability to place a meaningful distance metric on the output space.

\sphinxAtStartPar
However, pre\sphinxhyphen{}calibration results in a very coarse characterization of uncertainty, especially when considering a large number of parameters, as more samples are needed to fully characterize the parameter space. Due to the inability to evaluate the relative probability of regions of the parameter space beyond the binary plausible\sphinxhyphen{}and\sphinxhyphen{}implausible characterization, pre\sphinxhyphen{}calibration can also result in degraded hindcast and projection skills and parameter estimates {[}\hyperlink{cite.index:id37}{157}, \hyperlink{cite.index:id41}{163}, \hyperlink{cite.index:id42}{164}{]}.

\sphinxAtStartPar
A related method, widely used in hydrological studies, is generalized likelihood uncertainty estimation, or GLUE {[}\hyperlink{cite.index:id38}{19}{]}. Unlike pre\sphinxhyphen{}calibration, the underlying argument for GLUE relies on the concept of equifinality {[}\hyperlink{cite.index:id43}{165}{]}, which posits that it is impossible to find a uniquely well\sphinxhyphen{}performing parameter vector for models of abstract environmental systems {[}\hyperlink{cite.index:id43}{165}, \hyperlink{cite.index:id44}{166}{]}. In other words, there exist multiple parameter vectors which perform equally or similarly well. As with pre\sphinxhyphen{}calibration, GLUE uses a goodness\sphinxhyphen{}of\sphinxhyphen{}fit measure (though this is called a “likelihood” in the GLUE literature, as opposed to a statistical likelihood function {[}\hyperlink{cite.index:id45}{167}{]}) to evaluate samples. After setting a threshold of acceptable performance with respect to that measure, samples are evaluated and classified into “behavioral” or “non\sphinxhyphen{}behavioral” according to the threshold.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Put this into practice! Click the following badge to try out an interactive tutorial on utilizing Pre\sphinxhyphen{}Calibration and GLUE for HYMOD model calibration:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fhymod.ipynb}{Pre\sphinxhyphen{}Calibration Jupyter Notebook}
\end{sphinxadmonition}


\section{Markov Chain Monte Carlo}
\label{\detokenize{A1_Uncertainty_Quantification:markov-chain-monte-carlo}}
\sphinxAtStartPar
Markov chain Monte Carlo (MCMC) is a “gold standard” approach to full uncertainty quantification. MCMC refers to a category of algorithms which systematically sample from a target distribution (in this case, the posterior distribution) by constructing a Markov chain. A Markov chain is a probabilistic structure consisting of a state space, an initial probability distribution over the states, and a transition distribution between states. If a Markov chain satisfies certain properties {[}\hyperlink{cite.index:id170}{168}, \hyperlink{cite.index:id169}{169}{]}, the probability of being in each state will eventually converge to a stable, or stationary, distribution, regardless of the initial probabilities.

\sphinxAtStartPar
MCMC algorithms construct a Markov chain of samples from a parameter space (the combination of model and statistical parameters). This Markov chain is constructed so that the stationary distribution is a target distribution, in this case the (Bayesian) posterior distribution. As a result, after the transient period, the resulting samples can be viewed as a set of dependent samples from the posterior (the dependence is due to the autocorrelation between samples resulting from the Markov chain transitions). Expected values can be computed from these samples (for example, using batch\sphinxhyphen{}means estimators {[}\hyperlink{cite.index:id171}{170}{]}), or the chain can be sub\sphinxhyphen{}sampled or thinned and the resulting samples used as independent Monte Carlo samples due to the reduced or eliminated autocorrelation.

\sphinxAtStartPar
A general workflow for MCMC is shown in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-4}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-4}}}. The first decision is whether to use the full model or a surrogate model (or emulator). Typical surrogates include Gaussian process emulation {[}\hyperlink{cite.index:id172}{171}, \hyperlink{cite.index:id173}{172}{]}, polynomial chaos expansions {[}\hyperlink{cite.index:id174}{173}, \hyperlink{cite.index:id175}{174}{]}, support vector machines :cite:p\textasciigrave{}ciccazzo\_svm\_2016, pruett\_creation\_2016\textasciigrave{}, and neural networks {[}\hyperlink{cite.index:id178}{175}, \hyperlink{cite.index:id179}{176}{]}. Surrogate modeling can be faster, but requires a sufficient number of model evaluations for the surrogate to accurately represent the model’s response surface, and this typically limits the number of parameters which can be included in the analysis.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figureA1_4_mcmc_workflow}.png}
\caption{Workflow for Markov chain Monte Carlo.}\label{\detokenize{A1_Uncertainty_Quantification:id39}}\label{\detokenize{A1_Uncertainty_Quantification:figure-a1-4}}\end{figure}

\sphinxAtStartPar
After selecting the variables which will be treated as uncertain, the next step is to specify the likelihood based on the selected surrogate model or the structure of the data\sphinxhyphen{}model residuals. For example, it may not always be appropriate to treat the residuals as independent and identically distributed (as is commonly done in linear regression). A mis\sphinxhyphen{}specification of the residual structure can result in biases and over\sphinxhyphen{} or under\sphinxhyphen{}confident inferences and projections {[}\hyperlink{cite.index:id180}{177}{]}.

\sphinxAtStartPar
After specifying the prior distributions (see \hyperref[\detokenize{A1_Uncertainty_Quantification:critical-first-step}]{Section \ref{\detokenize{A1_Uncertainty_Quantification:critical-first-step}}}), the selected MCMC algorithm should be used to draw samples from the posterior distribution. There are many MCMC algorithms, all of which have advantages and disadvantages for a particular problem. These include the Metropolis\sphinxhyphen{}Hastings algorithm {[}\hyperlink{cite.index:id169}{169}{]} and Hamiltonian Monte Carlo {[}\hyperlink{cite.index:id181}{178}, \hyperlink{cite.index:id182}{179}{]}. Software packages typically implement one MCMC method, sometimes designed for a particular problem setting or likelihood specification. For example, R’s \sphinxstyleemphasis{adaptMCMC} implements an adaptive Metropolis\sphinxhyphen{}Hastings algorithm {[}\hyperlink{cite.index:id183}{180}{]}, while \sphinxstyleemphasis{NIMBLE} {[}\hyperlink{cite.index:id184}{181}, \hyperlink{cite.index:id185}{182}{]} uses a user\sphinxhyphen{}customizable Metropolis\sphinxhyphen{}Hastings implementation, as well as functionality for Gibbs sampling (which is a special case of Metropolis\sphinxhyphen{}Hastings where the prior distribution has a convenient mathematical form). Some recent implementations, such as \sphinxstyleemphasis{Stan} {[}\hyperlink{cite.index:id186}{183}{]}, \sphinxstyleemphasis{pyMC3} {[}\hyperlink{cite.index:id187}{184}{]}, and \sphinxstyleemphasis{Turing} {[}\hyperlink{cite.index:id188}{185}{]} allow different algorithms to be used.

\sphinxAtStartPar
A main consideration when using MCMC algorithms is testing for convergence to the target distribution. As convergence is guaranteed only for a sufficiently large number of transitions, it is impossible to conclude for certain that a chain has converged for a fixed number of iterations. However, several heuristics have been developed {[}\hyperlink{cite.index:id171}{170}, \hyperlink{cite.index:id189}{186}{]} to increase evidence that convergence has occurred.


\section{Other Methods}
\label{\detokenize{A1_Uncertainty_Quantification:other-methods}}
\sphinxAtStartPar
Other common methods for UQ exist. These include sequential Monte Carlo, otherwise known as particle filtering {[}\hyperlink{cite.index:id190}{187}, \hyperlink{cite.index:id191}{188}, \hyperlink{cite.index:id192}{189}{]}, where a number of particles are used to evaluate samples. An advantage of sequential Monte Carlo is that the vast majority of the computation can be parallelized, unlike with standard MCMC. A major weakness is the potential for degeneracy {[}\hyperlink{cite.index:id191}{188}{]}, where many particles have extremely small weights, resulting in the effective use of only a few samples.

\sphinxAtStartPar
Another method is approximate Bayesian computation (ABC) {[}\hyperlink{cite.index:id193}{190}, \hyperlink{cite.index:id194}{191}, \hyperlink{cite.index:id195}{192}{]}. ABC is a likelihood\sphinxhyphen{}free approach that compares model output to a set of summary statistics. ABC is therefore well\sphinxhyphen{}suited for models and residual structures which do not lend themselves to a computationally\sphinxhyphen{}tractable likelihood, but the resulting inferences are known to be biased if the set of summary statistics is not sufficient, which can be difficult to know a\sphinxhyphen{}priori.


\section{The Critical First Step: How to Choose a Prior Distribution}
\label{\detokenize{A1_Uncertainty_Quantification:the-critical-first-step-how-to-choose-a-prior-distribution}}\label{\detokenize{A1_Uncertainty_Quantification:critical-first-step}}
\sphinxAtStartPar
Prior distributions play an important role in Bayesian uncertainty quantification, particularly when data is limited relative to the dimension of the model. Bayesian updating can be thought of as an information filter, where each additional datum is added to the information contained in the prior; eventually, the prior makes relatively little impact. In real world problems, it can be extremely difficult to assess how much data is required for the choice of prior to become less relevant. The choice of prior can also be influential when conducting SA prior to or without UQ. This is because a prior distribution for a sensitivity analysis for a given parameter which is much wider than the region where the model response surface is sensitive to the parameter value might cause the sensitivity calculation to underestimate the response in that potentially critical region. Similarly, a prior which is too narrow may miss regions where the model responds to the parameter altogether.

\sphinxAtStartPar
Ideally, prior distributions are constructed independently of any analysis of the new data considered. This is because using data to inform the prior as well as to compute the likelihood reuses information in a potentially inappropriate way, which can lead to overconfident inferences. Following Jaynes {[}\hyperlink{cite.index:id196}{193}{]}, Gelman \sphinxstyleemphasis{et al.} {[}\hyperlink{cite.index:id197}{194}{]} refers to the ideal prior as one which encodes all available information about the model. For practical reasons (difficulty of construction or computational inconvenience), most priors fail to achieve this ideal. These compromises mean that priors should be transparently articulated and justified, so that the impact of the choice of prior can be fully understood. When there is ambiguity about an appropriate prior, such as how fat the tails should be, an analyst should examine how sensitive the UQ results are to the choice of prior.

\sphinxAtStartPar
Priors can also be classified in terms of the information encoded by them, demonstrated in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-5}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-5}}}. Non\sphinxhyphen{}informative priors (illustrated in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-5}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-5}}} (a)) allegedly correspond to (and are frequently justified by) a position of ignorance. A classic example is the use of a uniform distribution. A uniform prior can, however, be problematic, as it can lead to improper inferences by giving extremely large values the same prior probability as values which may seem more likely {[}\hyperlink{cite.index:id197}{194}, \hyperlink{cite.index:id198}{195}{]}, and therefore does not really reflect a state of complete ignorance. In the extreme case of a uniform prior over the entire real line, every particular region has effectively a prior weight of zero, even though not all regions are a priori unlikely {[}\hyperlink{cite.index:id198}{195}{]}. Moreover, a uniform prior which excludes possible parameter values is not actually noninformative, as it assigns zero probability to those values while jumping to a nonzero probability as soon as the boundary is crossed. While a uniform prior can be problematic for the task of uncertainty quantification, it may be useful for an initial sensitivity analysis to identify the boundary of any regions where the model is sensitive to the parameter.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figureA1_5_priors_posteriors}.png}
\caption{Impact of priors on posterior inferences. These plots show the results of inference for a linear regression model with 15 data points. The true value of the parameter is equal to \sphinxhyphen{}3. All priors have mean 0. In panel (a), a non\sphinxhyphen{}informative prior allows the tails of the posterior to extend freely, which may result in unreasonably large parameter values. In panel (b), a weakly informative prior constrains the tails more, but allows them to extend without too much restriction. In panel (c), an informative prior strongly constrains the tails of the posterior and biases the inference closer towards the prior mean (the posterior mean is \sphinxhyphen{}0.89 in this case, and closer to \sphinxhyphen{}3 in the other two cases).}\label{\detokenize{A1_Uncertainty_Quantification:id40}}\label{\detokenize{A1_Uncertainty_Quantification:figure-a1-5}}\end{figure}

\sphinxAtStartPar
Informative priors strongly bound the range of probable values (illustrated in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-5}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-5}}} (c)). One example is a Gaussian distribution with a relatively small standard deviation, so that large values are assigned a close to null prior probability. Another example is the jump from zero to non\sphinxhyphen{}zero probability occurring at the truncation point of a truncated Gaussian, which could be justified based on information that the parameter cannot take on values beyond this point. Without this type of justification, however, priors may be too informative, failing to allow the information contained in the available data to update them.

\sphinxAtStartPar
Finally, weakly informative priors (illustrated in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-5}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-5}}} (b)) fall in between {[}\hyperlink{cite.index:id197}{194}{]}. They regularize better than non\sphinxhyphen{}informative priors, but allow for more inference flexibility than fully informative priors. An example might be a Gaussian distribution with a moderate standard deviation, which still assigns negligible probability for values far away from the mean, but is less constrained than a narrow Gaussian for a reasonably large area. A key note is that it is not necessarily better to be more informative if this cannot be justified by the available information.


\section{The Critical Final Step: Predictive Checks}
\label{\detokenize{A1_Uncertainty_Quantification:the-critical-final-step-predictive-checks}}
\sphinxAtStartPar
Every UQ workflow requires a number of choices, potentially including selecting prior distributions, the likelihood specification, and any used numerical models. Checking the appropriateness of these choices is an essential step for sound inferences, as misspecification can produce biased results {[}\hyperlink{cite.index:id180}{177}{]}. Model checking in this fashion is part of an iterative UQ process, as the results can reveal adjustments to the statistical model or the need to select a different numerical model {[}\hyperlink{cite.index:id200}{196}, \hyperlink{cite.index:id199}{197}, \hyperlink{cite.index:id201}{198}{]}.

\sphinxAtStartPar
A classic example is the need to check the structure of residuals for correlations. Many standard statistical models, such as linear regression, assume that the residuals are independent and identically distributed from the error distribution. The presence of correlations, including temporal autocorrelations and spatial correlations, indicates a structural mismatch between the likelihood and the data. In these cases, the likelihood adjusted to account for these correlations.

\sphinxAtStartPar
Checking residuals in this fashion is one example of a predictive check (or a posterior predictive check in the Bayesian setting). One way to view UQ is as a means to recover data\sphinxhyphen{}generating processes (associated with each parameter vector) consistent with the observations. Predictive checks compare the inferred data\sphinxhyphen{}generating process to the observations to determine whether the model is capable of appropriately capturing uncertainty. After conducting the UQ analysis, alternatively realized datasets are simulated from sampled parameters. These alternative datasets, or their summary statistics, can be tested against the observations to determine adequacy of the fit. Predictive checks are therefore a way of probing various model components to identify shortcomings that might result in biased inferences or poor projections, depending on the goal of the analysis.

\sphinxAtStartPar
One example of a graphical predictive check for time series models is hindcasting, where predictive intervals are constructed from the alternative datasets and plotted along with the data. Hindcasts demonstrate how well the model is capable of capturing the broader dynamics of the data, as well as whether the parameter distributions produce appropriate levels of output uncertainty. A related quantitative check is the surprise index, which calculates the percentage of data points located within a fixed predictive interval. For example, the 90\% predictive interval should contain approximately 90\% of the data. More uncertainty than this reflects underconfidence, while less uncertainty reflects overconfidence. This could be the result of priors that are not appropriately informative, or a likelihood that does not account for correlations between data points appropriately. It could also be the result of a numerical model that isn’t sufficiently sensitive to the parameters that are treated as uncertain.


\section{Key Take\sphinxhyphen{}Home Points}
\label{\detokenize{A1_Uncertainty_Quantification:key-take-home-points}}
\sphinxAtStartPar
When appropriate, UQ is an important component of the exploratory modeling workflow. While a number of parameter sets could be consistent with observations, they may result in divergent model outputs when exposed to different future conditions. This can result in identifying risks which are not visible when selecting a “best fit” parameterization. Quantifying uncertainties also allows us to quantify the support for hypotheses, which is an essential part of the scientific process.

\sphinxAtStartPar
Due to the scale and complexity of the experiments taking place in IM3, UQ has not been extensively used. The tradeoff between the available number of function evaluations and the number of uncertain parameters illustrated in \hyperref[\detokenize{A1_Uncertainty_Quantification:figure-a1-1}]{Fig.\@ \ref{\detokenize{A1_Uncertainty_Quantification:figure-a1-1}}} is particularly challenging due to the increasing complexity of state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art models and the movement towards coupled, multisector models. This tradeoff can be addressed somewhat through the use of emulators and parallelizable methods. In particular, when attempting to navigate this tradeoff by limiting the number of uncertain parameters, it is important to carefully iterate with sensitivity analyses to ensure that critical parameters are identified.

\sphinxAtStartPar
Specifying prior distributions and likelihoods is an ongoing challenge. Prior distributions, in particular, should be treated as deeply uncertain when appropriate. One key advantage of the methods described in this chapter is that they have the potential for increased transparency. When it is not possible to conduct a sensitivity analysis on a number of critical priors due to limited computational budgets, fully specifying and providing a justification for the utilized distributions allows other researchers to identify key assumptions and build on existing work. The same is true for the specification of likelihoods—while likelihood\sphinxhyphen{}free methods avoid the need to specify a likelihood function, they require other assumptions or choices, which should be described and justified as transparently as possible.

\sphinxAtStartPar
We conclude this appendix with some key recommendations:
1. UQ analysis does not require full confidence in priors and likelihoods. Rather, UQ should be treated as part of an exploratory modeling workflow, where hypotheses related to model structures, prior distributions, and likelihoods can be tested.
2. For complex multisectoral models, UQ will typically require the use of a reduced set of parameters, either through emulation or by fixing the others to their best\sphinxhyphen{}fit values. These parameters should be selected through a careful sensitivity analysis.
3. Avoid the use of supposedly “non\sphinxhyphen{}informative” priors, such as uniform priors, whenever possible. In the absence of strong information about parameter values, the use of weakly informative priors, such as diffuse normals, is preferable.
4. Be cognizant of the limitations of conclusions that can be drawn by using each method. The bootstrap, for example, may result in overconfidence if the dataset is limited and is not truly representative of the underlying stochastic process.
5. When using MCMC, Markov chains can not be shown to have converged to the target distribution, but rather evidence can be collected to demonstrate that it is likely that they have.
6. Conduct predictive checks based on the assumptions underlying the choices made in the analysis, and iteratively update those choices if the assumptions prove to be ill\sphinxhyphen{}suited for the problem at hand.


\chapter{Jupyter Notebook Tutorials}
\label{\detokenize{A2_Jupyter_Notebooks:jupyter-notebook-tutorials}}\label{\detokenize{A2_Jupyter_Notebooks:a2-jupyter-notebooks}}\label{\detokenize{A2_Jupyter_Notebooks::doc}}

\section{Fishery Dynamics Tutorial}
\label{\detokenize{A2_Jupyter_Notebooks:fishery-dynamics-tutorial}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Run the tutorial interactively:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Ffishery\_dynamics.ipynb}{Fishery Dynamics Notebook}
\end{sphinxadmonition}


\subsection{Tutorial: Sensitivity Analysis (SA) to discover factors shaping consequential dynamics}
\label{\detokenize{A2_Jupyter_Notebooks:tutorial-sensitivity-analysis-sa-to-discover-factors-shaping-consequential-dynamics}}
\sphinxAtStartPar
This notebook demonstrates the application of sensitivity analysis to
discover factors that shape the behavior modes of a socio\sphinxhyphen{}ecological
system with dynamic human action.

\sphinxAtStartPar
The model represents a system of prey and predator fish, with a human
actor harvesting the prey fish. The system is simple but very rich in
the dynamic behaviors it exhibits. You can read more about this system
at \sphinxhref{https://doi.org/10.1155/2020/4170453}{Hadjimichael et
al. (2020)}.

\noindent\sphinxincludegraphics[width=781\sphinxpxdimen,height=284\sphinxpxdimen]{{eqn2}.png}

\sphinxAtStartPar
This complexity is accompanied by the presence of several equilibria
that come in and out of existence with different parameter values. The
equilibria also change in their stability according to different
parameter values, giving rise to different behavior modes.

\noindent\sphinxincludegraphics[width=5104\sphinxpxdimen,height=2650\sphinxpxdimen]{{Figure_1}.png}

\sphinxAtStartPar
In the unharvested system (without the human actor) the stability of
several of these equilibria can be derived analytically. The task
becomes significantly more difficult when the adaptive human actor is
introduced, deciding to harvest the system at different rates according
to their objectives and preferences.

\sphinxAtStartPar
Sensitivity analysis methods can help us identify the factors that most
control these dynamics by exploring the space of parameter values and
seeing how system outputs change as a result.

\sphinxAtStartPar
Through previously conducted optimization, there already exists a set of
potential harvesting strategies that were identified in pursuit of five
objectives:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Maximize Harvesting Discounted Profits (Net Present Value)

\item {} 
\sphinxAtStartPar
Minimize Prey Population Deficit

\item {} 
\sphinxAtStartPar
Minimize Longest Duration of Consecutive Low Harvest

\item {} 
\sphinxAtStartPar
Maximize Worst Harvest Instance

\item {} 
\sphinxAtStartPar
Minimize Harvest Variance

\end{itemize}

\sphinxAtStartPar
The identified harvesting strategies also meet the necessary constraint
of not causing inadvertent predator collapse.

\sphinxAtStartPar
We will be examining the effects of parametric uncertainty on these
identified strategies, particularly focusing on two strategies: one
selected to maximize harvesting profits and one identified through
previous analysis to perform ‘well enough’ for all objectives across a
wide range of states of the world (referred to as the ‘robust’
harvesting policy).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
import msdbook

import numpy as np
import matplotlib.pyplot as plt

from SALib.sample import saltelli
from SALib.analyze import sobol
from matplotlib import patheffects as pe

\PYGZsh{} load example data
msdbook.install\PYGZus{}package\PYGZus{}data()

\PYGZpc{}matplotlib inline
\PYGZpc{}config InlineBackend.print\PYGZus{}figure\PYGZus{}kwargs = \PYGZob{}\PYGZsq{}bbox\PYGZus{}inches\PYGZsq{}:None\PYGZcb{}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Downloading} \PYG{n}{example} \PYG{n}{data} \PYG{k}{for} \PYG{n}{msdbook} \PYG{n}{version} \PYG{l+m+mf}{0.1}\PYG{l+m+mf}{.5}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{uncertain\PYGZus{}params\PYGZus{}bounds}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}metric\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{solutions}\PYG{o}{.}\PYG{n}{resultfile}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LHsamples\PYGZus{}original\PYGZus{}1000}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{param\PYGZus{}values}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{collapse\PYGZus{}days}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}params\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LeafCatch}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}simulations\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{Robustness}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\subsubsection{Step 1: Load identified solutions and explore performance}
\label{\detokenize{A2_Jupyter_Notebooks:step-1-load-identified-solutions-and-explore-performance}}
\sphinxAtStartPar
Identify and load the most robust and profit\sphinxhyphen{}maximizing solutions

\begin{sphinxVerbatim}[commandchars=\\\{\}]
robustness = msdbook.load\PYGZus{}robustness\PYGZus{}data()
results = msdbook.load\PYGZus{}profit\PYGZus{}maximization\PYGZus{}data()

robust\PYGZus{}solution = np.argmax(robustness[:,\PYGZhy{}1]) \PYGZsh{}pick robust solution
profit\PYGZus{}solution = np.argmin(results[:,6]) \PYGZsh{}pick profitable solution
objective\PYGZus{}performance = \PYGZhy{}results[:,6:]

\PYGZsh{} Get decision variables for each of the policies
highprofitpolicy = results[profit\PYGZus{}solution,0:6]
mostrobustpolicy = results[robust\PYGZus{}solution,0:6]
\end{sphinxVerbatim}

\sphinxAtStartPar
Plot the identified solutions with regards to their objective
performance in a parallel axis plot

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-objective-performance}{plot\_objective\_performance}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ax, ax1 = msdbook.plot\PYGZus{}objective\PYGZus{}performance(objective\PYGZus{}performance, profit\PYGZus{}solution, robust\PYGZus{}solution)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=1296\sphinxpxdimen,height=648\sphinxpxdimen]{{fishery_output_6_0}.png}

\sphinxAtStartPar
The results of the optimization are presented in a parallel axis plot
where each of the five objectives (and one constraint) are represented
as an axis. Each solution on the Pareto front is represented as a line
where the color of the line indicates the value of the NPV objective.
The preference for objective values is in the upward direction.
Therefore, the ideal solution would be a line straight across the top of
the plot that satisfies every objective. However, no such line exists
because there are tradeoffs when sets of objectives are prioritized over
the others. When lines cross in between axes, this indicates a tradeoff
between objectives (as seen in the first two axes).The solution that is
most robust in the NPV objective has the highest value on the first axis
and is outlined in dark gold. The solution that is most robust across
all objectives is outlined in a brighter yellow.


\subsubsection{Step 2: Use SALib to generate a sample for a Sobol sensitivity analysis}
\label{\detokenize{A2_Jupyter_Notebooks:step-2-use-salib-to-generate-a-sample-for-a-sobol-sensitivity-analysis}}
\sphinxAtStartPar
To do so, we first need to define the problem dictionary that allows us
to generate alternative states of the world.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Set up SALib problem
problem = \PYGZob{}
  \PYGZsq{}num\PYGZus{}vars\PYGZsq{}: 9,
  \PYGZsq{}names\PYGZsq{}: [\PYGZsq{}a\PYGZsq{}, \PYGZsq{}b\PYGZsq{}, \PYGZsq{}c\PYGZsq{}, \PYGZsq{}d\PYGZsq{}, \PYGZsq{}h\PYGZsq{}, \PYGZsq{}K\PYGZsq{}, \PYGZsq{}m\PYGZsq{}, \PYGZsq{}sigmaX\PYGZsq{}, \PYGZsq{}sigmaY\PYGZsq{}],
  \PYGZsq{}bounds\PYGZsq{}: [[0.002, 2], [0.005, 1], [0.2, 1], [0.05, 0.2], [0.001, 1],
             [100, 5000], [0.1, 1.5], [0.001, 0.01], [0.001, 0.01]]
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
You can use the following to generate a Saltelli sample using the
following:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{param\PYGZus{}values} \PYG{o}{=} \PYG{n}{saltelli}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{problem}\PYG{p}{,} \PYG{l+m+mi}{1024}\PYG{p}{,} \PYG{n}{calc\PYGZus{}second\PYGZus{}order}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\end{sphinxVerbatim}

\sphinxAtStartPar
Generally, it is a good idea to save the result of the sample since it
is often reused and regenerating it produces a different sample set. For
this reason, we will load one from file that was previously generated.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load previously generated Saltelli sample from our msdbook package data
param\PYGZus{}values = msdbook.load\PYGZus{}saltelli\PYGZus{}param\PYGZus{}values()
\end{sphinxVerbatim}


\subsubsection{Step 3: Evaluate the system over all generated states of the world}
\label{\detokenize{A2_Jupyter_Notebooks:step-3-evaluate-the-system-over-all-generated-states-of-the-world}}
\sphinxAtStartPar
We need to identify the states where the predator population collapses,
as an inadvertent consequence of applying the harvesting strategy under
a state of the world different from the one originally assumed. Due to
how long this step takes to execute within the tutorial, we will read in
the solutions from an external file. However, the block of code below
shows how evaluation can be implemented.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} create array to store collapse values under both policies}
\PYG{n}{collapse\PYGZus{}days} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{[}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{param\PYGZus{}values}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} evaluate performance under every state}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{param\PYGZus{}values}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}

    \PYG{n}{additional\PYGZus{}inputs} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Previous\PYGZus{}Prey}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                                  \PYG{p}{[}\PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{7}\PYG{p}{]}\PYG{p}{,}
                                   \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{8}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{collapse\PYGZus{}days}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{=}\PYG{n}{fish\PYGZus{}game}\PYG{p}{(}\PYG{n}{highprofitpolicy}\PYG{p}{,} \PYG{n}{additional\PYGZus{}inputs}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
    \PYG{n}{collapse\PYGZus{}days}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{=}\PYG{n}{fish\PYGZus{}game}\PYG{p}{(}\PYG{n}{mostrobustpolicy}\PYG{p}{,} \PYG{n}{additional\PYGZus{}inputs}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load the simulation data from our msdbook package data
collapse\PYGZus{}days = msdbook.load\PYGZus{}collapse\PYGZus{}data()
\end{sphinxVerbatim}


\subsubsection{Step 4: Calculate sensitivity indices}
\label{\detokenize{A2_Jupyter_Notebooks:step-4-calculate-sensitivity-indices}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Si\PYGZus{}profit = sobol.analyze(problem, collapse\PYGZus{}days[:, 0],
                          calc\PYGZus{}second\PYGZus{}order=False,
                          conf\PYGZus{}level=0.95,
                          print\PYGZus{}to\PYGZus{}console=True)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
              \PYG{n}{ST}   \PYG{n}{ST\PYGZus{}conf}
\PYG{n}{a}       \PYG{l+m+mf}{0.278724}  \PYG{l+m+mf}{0.051918}
\PYG{n}{b}       \PYG{l+m+mf}{0.188124}  \PYG{l+m+mf}{0.027986}
\PYG{n}{c}       \PYG{l+m+mf}{0.015588}  \PYG{l+m+mf}{0.012159}
\PYG{n}{d}       \PYG{l+m+mf}{0.077655}  \PYG{l+m+mf}{0.016051}
\PYG{n}{h}       \PYG{l+m+mf}{0.025096}  \PYG{l+m+mf}{0.014796}
\PYG{n}{K}       \PYG{l+m+mf}{0.033239}  \PYG{l+m+mf}{0.014006}
\PYG{n}{m}       \PYG{l+m+mf}{0.845465}  \PYG{l+m+mf}{0.071372}
\PYG{n}{sigmaX}  \PYG{l+m+mf}{0.000708}  \PYG{l+m+mf}{0.000851}
\PYG{n}{sigmaY}  \PYG{l+m+mf}{0.000849}  \PYG{l+m+mf}{0.000470}
              \PYG{n}{S1}   \PYG{n}{S1\PYGZus{}conf}
\PYG{n}{a}       \PYG{l+m+mf}{0.126405}  \PYG{l+m+mf}{0.042938}
\PYG{n}{b}       \PYG{l+m+mf}{0.060739}  \PYG{l+m+mf}{0.034380}
\PYG{n}{c}       \PYG{l+m+mf}{0.003333}  \PYG{l+m+mf}{0.008758}
\PYG{n}{d}       \PYG{l+m+mf}{0.011388}  \PYG{l+m+mf}{0.025792}
\PYG{n}{h}       \PYG{l+m+mf}{0.010233}  \PYG{l+m+mf}{0.013034}
\PYG{n}{K}       \PYG{l+m+mf}{0.016699}  \PYG{l+m+mf}{0.015731}
\PYG{n}{m}       \PYG{l+m+mf}{0.609991}  \PYG{l+m+mf}{0.072196}
\PYG{n}{sigmaX}  \PYG{l+m+mf}{0.000531}  \PYG{l+m+mf}{0.001607}
\PYG{n}{sigmaY}  \PYG{l+m+mf}{0.000337}  \PYG{l+m+mf}{0.002014}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Si\PYGZus{}robustness = sobol.analyze(problem,
                              collapse\PYGZus{}days[:, 1],
                              calc\PYGZus{}second\PYGZus{}order=False,
                              conf\PYGZus{}level=0.95,
                              print\PYGZus{}to\PYGZus{}console=True)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
              \PYG{n}{ST}   \PYG{n}{ST\PYGZus{}conf}
\PYG{n}{a}       \PYG{l+m+mf}{0.226402}  \PYG{l+m+mf}{0.038177}
\PYG{n}{b}       \PYG{l+m+mf}{0.066819}  \PYG{l+m+mf}{0.017905}
\PYG{n}{c}       \PYG{l+m+mf}{0.004395}  \PYG{l+m+mf}{0.004478}
\PYG{n}{d}       \PYG{l+m+mf}{0.024509}  \PYG{l+m+mf}{0.006695}
\PYG{n}{h}       \PYG{l+m+mf}{0.009765}  \PYG{l+m+mf}{0.006605}
\PYG{n}{K}       \PYG{l+m+mf}{0.020625}  \PYG{l+m+mf}{0.010860}
\PYG{n}{m}       \PYG{l+m+mf}{0.897971}  \PYG{l+m+mf}{0.070086}
\PYG{n}{sigmaX}  \PYG{l+m+mf}{0.000136}  \PYG{l+m+mf}{0.000152}
\PYG{n}{sigmaY}  \PYG{l+m+mf}{0.000739}  \PYG{l+m+mf}{0.001088}
              \PYG{n}{S1}   \PYG{n}{S1\PYGZus{}conf}
\PYG{n}{a}       \PYG{l+m+mf}{0.087936}  \PYG{l+m+mf}{0.045617}
\PYG{n}{b}       \PYG{l+m+mf}{0.000554}  \PYG{l+m+mf}{0.019070}
\PYG{n}{c}      \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.002970}  \PYG{l+m+mf}{0.004227}
\PYG{n}{d}       \PYG{l+m+mf}{0.001206}  \PYG{l+m+mf}{0.015897}
\PYG{n}{h}       \PYG{l+m+mf}{0.004554}  \PYG{l+m+mf}{0.008202}
\PYG{n}{K}       \PYG{l+m+mf}{0.003843}  \PYG{l+m+mf}{0.012294}
\PYG{n}{m}       \PYG{l+m+mf}{0.751301}  \PYG{l+m+mf}{0.063511}
\PYG{n}{sigmaX} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.000325}  \PYG{l+m+mf}{0.001155}
\PYG{n}{sigmaY} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.001887}  \PYG{l+m+mf}{0.003287}
\end{sphinxVerbatim}

\sphinxAtStartPar
Looking at the total\sphinxhyphen{}order indices, (ST) factors \(m\), \(a\),
\(b\), \(d\) and \(K\) appear to affect the stability of
this system. Looking at the first\sphinxhyphen{}order indices (S1), we also see that
besides factors \(m\) and \(a\), all other factors are important
in this system through their interactions, which make up the difference
between their S1 and ST indices. This shows the danger of limiting
sensitivity analyses to first order effects, as factor importance might
be significantly misjudged.

\sphinxAtStartPar
These findings are supported by the analytical condition of equilibrium
stability in this system:

\noindent\sphinxincludegraphics[width=406\sphinxpxdimen,height=96\sphinxpxdimen]{{eqn4}.png}

\sphinxAtStartPar
In an unharvested system, this condition is both necessary and
sufficient for the equilibrium of the two species coexisting to be
stable.

\sphinxAtStartPar
When adaptive human action is introduced however, this condition is
still necessary, but no longer sufficient, as harvesting reduces the
numbers of prey fish and as a result reduces the resources for the
predator fish. Since this harvesting value is not constant, but can
dynamically adapt according to the harvester’s objectives, it cannot be
introduced into this simple equation.


\subsubsection{Step 5: Explore relationship between uncertain factors and performance}
\label{\detokenize{A2_Jupyter_Notebooks:step-5-explore-relationship-between-uncertain-factors-and-performance}}
\sphinxAtStartPar
In the following steps, we will use the results of our sensitivity
analysis to investigate the relationships between parametric
uncertainty, equilibrium stability and the performance of the two
policies.

\sphinxAtStartPar
We can use the top three factors identified (\(m\), \(a\), and
\(b\)) to visualize the performance of our policies in this
three\sphinxhyphen{}dimensional parametric space.

\sphinxAtStartPar
We first define the stability condition, as a function of \(b\) and
\(m\), and calculate the corresponding values of \(a\).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
def inequality(b, m, h, K):
    return ((b**m)/(h*K)**(1\PYGZhy{}m))

\PYGZsh{} boundary interval that separates successful and failed states of the world
b = np.linspace(start=0.005, stop=1, num=1000)
m = np.linspace(start=0.1, stop=1.5, num=1000)
h = np.linspace(start=0.001, stop=1, num=1000)
K = np.linspace(start=100, stop=2000, num=1000)
b, m = np.meshgrid(b, m)
a = inequality(b, m, h, K)
a = a.clip(0,2)
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-factor-performance}{plot\_factor\_performance}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} generate plot
ax1, ax2 = msdbook.plot\PYGZus{}factor\PYGZus{}performance(param\PYGZus{}values, collapse\PYGZus{}days, b, m, a)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=4800\sphinxpxdimen,height=2400\sphinxpxdimen]{{fishery_output_22_0}.png}

\sphinxAtStartPar
These figures show the combinations of factors that lead to success or
failure in different states of the world when the NPV\sphinxhyphen{}maximizing and
Robust policies are utilized. Each point is a state of the world,
characterized by specific values of the parameters, and ideally, we
would like the color of the point to be blue, to represent that there
are a low number of days with a predator collapse in that world. The
gray curve denotes the highly non\sphinxhyphen{}linear nature of the boundary that
separates successful and failed states of the world. The figures
demonstrate the following key points:

\sphinxAtStartPar
First, as asserted above, the policies interact with the system in
different and complex ways. In the presence of human action, the
stability condition is not sufficient in determining whether the policy
will succeed, even though it clearly shapes the system in a fundamental
manner.

\sphinxAtStartPar
Secondly, the robust policy manages to avoid collapse in many more of
the sampled states of the world, indicated by the number of blue points.
This presents a clear tradeoff between profit\sphinxhyphen{}maximizing performance and
robustness against uncertainty.


\section{Sobol SA Tutorial}
\label{\detokenize{A2_Jupyter_Notebooks:sobol-sa-tutorial}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Run the tutorial interactively:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fsa\_saltelli\_sobol\_ishigami.ipynb}{Sobol SA Tutorial}
\end{sphinxadmonition}


\subsection{Tutorial: Sensitivity Analysis (SA) using the Saltelli sampling scheme with Sobol SA}
\label{\detokenize{A2_Jupyter_Notebooks:tutorial-sensitivity-analysis-sa-using-the-saltelli-sampling-scheme-with-sobol-sa}}
\sphinxAtStartPar
In this tutorial, we will use the popular Python Sensitivity Analysis
Library (\sphinxhref{https://salib.readthedocs.io/en/latest/index.html}{SALib})
to: 1. Generate a problem set as a dictionary for our Ishigami function
that has three inputs 2. Generate 8000 samples for our problem set using
the Saltelli1,2 sampling scheme 3. Execute the Ishigami function for
each of our samples and gather the outputs 4. Compute the sensitivity
analysis to generate first\sphinxhyphen{}order and total\sphinxhyphen{}order sensitivity indices
using the Sobol3 method 5. Interpret the meaning of our results


\subsubsection{Let’s get started!}
\label{\detokenize{A2_Jupyter_Notebooks:lets-get-started}}
\sphinxAtStartPar
\sphinxstylestrong{NOTE}: Content from this tutorial is taken directly from the SALib
\sphinxhref{https://salib.readthedocs.io/en/latest/basics.html}{“Basics”}
walkthrough.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
import numpy as np

from SALib.sample import saltelli
from SALib.analyze import sobol
from SALib.test\PYGZus{}functions import Ishigami
\end{sphinxVerbatim}


\subsubsection{Step 1: Generate the problem dictionary}
\label{\detokenize{A2_Jupyter_Notebooks:step-1-generate-the-problem-dictionary}}
\sphinxAtStartPar
The Ishigami function is of the form:
\begin{equation*}
\begin{split}f(x) = sin(x_1)+asin^2(x_2)+bx_3^4sin(x_1)\end{split}
\end{equation*}
\sphinxAtStartPar
and has three inputs, 𝑥1, 𝑥2, 𝑥3 where 𝑥𝑖 ∈ {[}−𝜋, 𝜋{]}.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
problem = \PYGZob{}
    \PYGZsq{}num\PYGZus{}vars\PYGZsq{}: 3,
    \PYGZsq{}names\PYGZsq{}: [\PYGZsq{}x1\PYGZsq{}, \PYGZsq{}x2\PYGZsq{}, \PYGZsq{}x3\PYGZsq{}],
    \PYGZsq{}bounds\PYGZsq{}: [[\PYGZhy{}3.14159265359, 3.14159265359],
               [\PYGZhy{}3.14159265359, 3.14159265359],
               [\PYGZhy{}3.14159265359, 3.14159265359]]
\PYGZcb{}
\end{sphinxVerbatim}


\subsubsection{Step 2: Generate samples using the Saltelli sampling scheme}
\label{\detokenize{A2_Jupyter_Notebooks:step-2-generate-samples-using-the-saltelli-sampling-scheme}}
\sphinxAtStartPar
Sobol SA requires the use of the Saltelli sampling scheme. The output of
the \sphinxcode{\sphinxupquote{saltelli.sample}} function is a NumPy array that is of shape 2048
by 3. The sampler generates 𝑁∗(2𝐷+2) samples, where in this example N is
256 (the argument we supplied) and D is 3 (the number of model inputs),
yielding 2048 samples. The keyword argument \sphinxcode{\sphinxupquote{calc\_second\_order=False}}
will exclude second\sphinxhyphen{}order indices, resulting in a smaller sample matrix
with 𝑁∗(𝐷+2) rows instead.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
param\PYGZus{}values = saltelli.sample(problem, 256)

print(f\PYGZdq{}`param\PYGZus{}values` shape:  \PYGZob{}param\PYGZus{}values.shape\PYGZcb{}\PYGZdq{})
\end{sphinxVerbatim}
\begin{sphinxalltt}
\sphinxtitleref{param\_values} shape:  (2048, 3)
\end{sphinxalltt}


\subsubsection{Step 3: Execute the Ishigami function over our sample set}
\label{\detokenize{A2_Jupyter_Notebooks:step-3-execute-the-ishigami-function-over-our-sample-set}}
\sphinxAtStartPar
SALib provides a nice wrapper to the Ishigami function that allows the
user to directly pass the \sphinxcode{\sphinxupquote{param\_values}} array we just generated into
the function directly.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Y = Ishigami.evaluate(param\PYGZus{}values)
\end{sphinxVerbatim}


\subsubsection{Step 4: Compute first\sphinxhyphen{}, second\sphinxhyphen{}, and total\sphinxhyphen{}order sensitivity indices using the Sobol method}
\label{\detokenize{A2_Jupyter_Notebooks:step-4-compute-first-second-and-total-order-sensitivity-indices-using-the-sobol-method}}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{sobol.analyze}} function will use our problem dictionary and the
result of the Ishigami runs (\sphinxcode{\sphinxupquote{Y}}) to compute first\sphinxhyphen{}, second\sphinxhyphen{}, and
total\sphinxhyphen{}order indicies.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Si = sobol.analyze(problem, Y)
\end{sphinxVerbatim}

\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Si}} is a Python dict with the keys “S1”, “S2”, “ST”, “S1\_conf”,
“S2\_conf”, and “ST\_conf”. The \sphinxcode{\sphinxupquote{\_conf}} keys store the corresponding
confidence intervals, typically with a confidence level of 95\%. Use the
keyword argument \sphinxcode{\sphinxupquote{print\_to\_console=True}} to print all indices. Or, we
can print the individual values from \sphinxcode{\sphinxupquote{Si}} as shown in the next step.


\subsubsection{Step 5: Interpret our results}
\label{\detokenize{A2_Jupyter_Notebooks:step-5-interpret-our-results}}
\sphinxAtStartPar
When we execute the following code to take a look at our first\sphinxhyphen{}order
indices (\sphinxcode{\sphinxupquote{S1}}) for each of our three parameters, we see that 𝑥1 and 𝑥2
exibit first\sphinxhyphen{}order sensitivities. This means that there is contribution
to the output variance by those parameters independently, whereas 𝑥3
does not contribute to the output variance.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
first\PYGZus{}order = Si[\PYGZsq{}S1\PYGZsq{}]

print(\PYGZsq{}First\PYGZhy{}order:\PYGZsq{})
print(f\PYGZdq{}x1: \PYGZob{}first\PYGZus{}order[0]\PYGZcb{}, x2: \PYGZob{}first\PYGZus{}order[1]\PYGZcb{}, x3: \PYGZob{}first\PYGZus{}order[2]\PYGZcb{}\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{First}\PYG{o}{\PYGZhy{}}\PYG{n}{order}\PYG{p}{:}
\PYG{n}{x1}\PYG{p}{:} \PYG{l+m+mf}{0.3260389719592443}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{:} \PYG{l+m+mf}{0.4820072841939227}\PYG{p}{,} \PYG{n}{x3}\PYG{p}{:} \PYG{l+m+mf}{0.011125510338583004}
\end{sphinxVerbatim}

\sphinxAtStartPar
Next, we evaluate the total\sphinxhyphen{}order indices and find that they are
substantially larger than the first\sphinxhyphen{}order indices, which reveals that
higher\sphinxhyphen{}order interactions are occurring. Our total\sphinxhyphen{}order indices measure
the contribution to the output variance caused by a model input,
including both its first\sphinxhyphen{}order effects (the input varying alone) and all
higher\sphinxhyphen{}order interactions. Now we see that 𝑥3 has non\sphinxhyphen{}negligible total
order indices.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
total\PYGZus{}order = Si[\PYGZsq{}ST\PYGZsq{}]

print(\PYGZsq{}Total\PYGZhy{}order:\PYGZsq{})
print(f\PYGZdq{}x1: \PYGZob{}total\PYGZus{}order[0]\PYGZcb{}, x2: \PYGZob{}total\PYGZus{}order[1]\PYGZcb{}, x3: \PYGZob{}total\PYGZus{}order[2]\PYGZcb{}\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Total}\PYG{o}{\PYGZhy{}}\PYG{n}{order}\PYG{p}{:}
\PYG{n}{x1}\PYG{p}{:} \PYG{l+m+mf}{0.5646024820275896}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{:} \PYG{l+m+mf}{0.4570071429804512}\PYG{p}{,} \PYG{n}{x3}\PYG{p}{:} \PYG{l+m+mf}{0.2506488435438359}
\end{sphinxVerbatim}

\sphinxAtStartPar
Finally, we can investigate these higher order interactions by viewing
the second\sphinxhyphen{}order outputs. The second\sphinxhyphen{}order indicies measure the
contribution to the output variance caused by the interaction between
any two model inputs.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
second\PYGZus{}order = Si[\PYGZsq{}S2\PYGZsq{}]

print(\PYGZdq{}Second\PYGZhy{}order:\PYGZdq{})
print(f\PYGZdq{}x1\PYGZhy{}x2:  \PYGZob{}second\PYGZus{}order[0,1]\PYGZcb{}\PYGZdq{})
print(f\PYGZdq{}x1\PYGZhy{}x3:  \PYGZob{}second\PYGZus{}order[0,2]\PYGZcb{}\PYGZdq{})
print(f\PYGZdq{}x2\PYGZhy{}x3:  \PYGZob{}second\PYGZus{}order[1,2]\PYGZcb{}\PYGZdq{})
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Second}\PYG{o}{\PYGZhy{}}\PYG{n}{order}\PYG{p}{:}
\PYG{n}{x1}\PYG{o}{\PYGZhy{}}\PYG{n}{x2}\PYG{p}{:}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.018110907981879032}
\PYG{n}{x1}\PYG{o}{\PYGZhy{}}\PYG{n}{x3}\PYG{p}{:}  \PYG{l+m+mf}{0.2648898732606599}
\PYG{n}{x2}\PYG{o}{\PYGZhy{}}\PYG{n}{x3}\PYG{p}{:}  \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.005645845624612848}
\end{sphinxVerbatim}

\sphinxAtStartPar
We can see that there are strong interactions between 𝑥1 and 𝑥3. Note in
the Ishigami function, these two variables are multiplied in the last
term, which creates these interactive effects. If we were considering
first order indices alone, we would erroneously assume that 𝑥3 has no
effect on our output, but the second\sphinxhyphen{}order and total order indices
reveal that this is not the case. It’s easy to understand where we might
see iteractive effects in the case of the simple Ishigami function.
However, it’s important to remember that in more complex systems, there
may be many higher\sphinxhyphen{}order interactions that are not apparent, but could
be extremely consequential in contributing to the variance of the
output. Additionally, some computing error will appear in the
sensitivity indices. For example, we observe a negative value for the
𝑥2\sphinxhyphen{}𝑥3 index. Typically, these computing errors shrink as the number of
samples increases.


\subsubsection{References}
\label{\detokenize{A2_Jupyter_Notebooks:references}}
\sphinxAtStartPar
{[}1{]} Saltelli, A. (2002). “Making best use of model evaluations to
compute sensitivity indices.” Computer Physics Communications,
145(2):280\sphinxhyphen{}297, doi:10.1016/S0010\sphinxhyphen{}4655(02)00280\sphinxhyphen{}1.

\sphinxAtStartPar
{[}2{]} Saltelli, A., P. Annoni, I. Azzini, F. Campolongo, M. Ratto, and S.
Tarantola (2010). “Variance based sensitivity analysis of model output.
Design and estimator for the total sensitivity index.” Computer Physics
Communications, 181(2):259\sphinxhyphen{}270, doi:10.1016/j.cpc.2009.09.018.

\sphinxAtStartPar
{[}3{]} Sobol, I. M. (2001). “Global sensitivity indices for nonlinear
mathematical models and their Monte Carlo estimates.” Mathematics and
Computers in Simulation, 55(1\sphinxhyphen{}3):271\sphinxhyphen{}280,
doi:10.1016/S0378\sphinxhyphen{}4754(00)00270\sphinxhyphen{}6.


\section{Logistic Regression Tutorial}
\label{\detokenize{A2_Jupyter_Notebooks:logistic-regression-tutorial}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Run the tutorial interactively:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fbasin\_users\_logistic\_regression.ipynb}{Logistic Regression Tutorial}
\end{sphinxadmonition}


\subsection{Tutorial: Logistic Regression for Factor Mapping}
\label{\detokenize{A2_Jupyter_Notebooks:tutorial-logistic-regression-for-factor-mapping}}
\sphinxAtStartPar
This tutorial replicates a scenario discovery analysis performed in
\sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020EF001503}{Hadjimichael et
al. (2020)}.


\subsubsection{Background}
\label{\detokenize{A2_Jupyter_Notebooks:background}}
\sphinxAtStartPar
Planners in the the Upper Colorado River Basin (UCRB, shown in the
figure below) are seeking to understand the vulnerability of water users
to uncertainties stemming from climate change, population growth and
water policy changes. The UCRB spans 25,682 km2 in western Colorado and
is home to approximately 300,000 residents and 1,012 km2 of irrigated
land. Several thousand irrigation ditches divert water from the main
river and its tributaties for irrigation (shown as small black dots in
the figure). Transmountain diversions of approximately 567,4000,000 m3
per year are exported for irrigation, industrial and municipal uses in
northern and eastern Colorado, serving the major population centers of
Denver and Colorado Springs. These diversions are carried through
tunnels, shown as large black dots in the figure.

\noindent\sphinxincludegraphics[width=1278\sphinxpxdimen,height=803\sphinxpxdimen]{{basin_map}.png}

\sphinxAtStartPar
An important planning consideration is the water rights of each user,
defined by seniority across all water uses (irrigation diversions,
transboundary diversions, power plants etc.) in the basin. To assess the
vulnerability of users with varying degrees of water rights seniority,
planners simulate the system across an ensemble of scenarios using the
state of Colorado’s StateMod platform. The model simulates streamflow,
diversions, instream demands, and reservoir operations.

\sphinxAtStartPar
Focusing on decision\sphinxhyphen{}relevant metrics, the scenario discovery is applied
to the water shortages experienced by each individual user (i.e., not on
a single basin\sphinxhyphen{}wide or sector\sphinxhyphen{}wide metric). For this training example,
we’ll be performing scenario discovery for three different water users,
two irrigation users and one municipal user.

\sphinxAtStartPar
Before we start our analysis, we’ll load the relevant Python libraries
and create lists storing the names of the users of interest.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
import msdbook

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt

\PYGZsh{} load example data
msdbook.install\PYGZus{}package\PYGZus{}data()

all\PYGZus{}IDs = [\PYGZsq{}7000550\PYGZsq{},\PYGZsq{}7200799\PYGZsq{},\PYGZsq{}3704614\PYGZsq{}] \PYGZsh{} IDs for three that we will perform the analysis for
usernames = [\PYGZsq{}Medium seniority irrigation\PYGZsq{},
             \PYGZsq{}Low seniority irrigation\PYGZsq{},
             \PYGZsq{}Transbasin municipal diversion\PYGZsq{}]
nStructures = len(all\PYGZus{}IDs)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Downloading} \PYG{n}{example} \PYG{n}{data} \PYG{k}{for} \PYG{n}{msdbook} \PYG{n}{version} \PYG{l+m+mf}{0.1}\PYG{l+m+mf}{.5}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{uncertain\PYGZus{}params\PYGZus{}bounds}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}metric\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{solutions}\PYG{o}{.}\PYG{n}{resultfile}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LHsamples\PYGZus{}original\PYGZus{}1000}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{param\PYGZus{}values}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{collapse\PYGZus{}days}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}params\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LeafCatch}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}simulations\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{Robustness}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}


\subsubsection{Step 1: Load Latin Hypercube Sample and set up problem}
\label{\detokenize{A2_Jupyter_Notebooks:step-1-load-latin-hypercube-sample-and-set-up-problem}}
\sphinxAtStartPar
To examine regional vulnerability, we generate an ensemble of plausible
future states of the worlds (SOWs) using Latin Hypercube Sampling. For
this tutorial, we’ll load a file containing 1,000 parameter samples.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
LHsamples = msdbook.load\PYGZus{}lhs\PYGZus{}basin\PYGZus{}sample()
param\PYGZus{}bounds = msdbook.load\PYGZus{}basin\PYGZus{}param\PYGZus{}bounds()

param\PYGZus{}names=[\PYGZsq{}Irrigation demand multiplier\PYGZsq{},\PYGZsq{}Reservoir loss\PYGZsq{},\PYGZsq{}Transbasin demand multiplier\PYGZsq{},
             \PYGZsq{}Municipal \PYGZam{} industrial multiplier\PYGZsq{}, \PYGZsq{}Shoshone\PYGZsq{},\PYGZsq{}Environmental flows\PYGZsq{},
             \PYGZsq{}Evaporation change\PYGZsq{},\PYGZsq{}Mean dry flow\PYGZsq{},\PYGZsq{}Dry flow variance\PYGZsq{},
             \PYGZsq{}Mean wet flow\PYGZsq{},\PYGZsq{}Wet flow variance\PYGZsq{},\PYGZsq{}Dry\PYGZhy{}dry probability\PYGZsq{},
             \PYGZsq{}Wet\PYGZhy{}wet probability\PYGZsq{}, \PYGZsq{}Snowmelt shift\PYGZsq{}]
\end{sphinxVerbatim}


\subsubsection{Step 2: Define decision\sphinxhyphen{}relevant metrics for illustration}
\label{\detokenize{A2_Jupyter_Notebooks:step-2-define-decision-relevant-metrics-for-illustration}}
\sphinxAtStartPar
Scenario discovery attempts to identify parametric regions that lead to
‘success’ and ‘failure’. For this demonstration we’ll be defining
‘success’ as states of the world where a shortage level doesn’t exceed
its historical frequency.


\subsubsection{Step 3: Run the logistic regression}
\label{\detokenize{A2_Jupyter_Notebooks:step-3-run-the-logistic-regression}}
\sphinxAtStartPar
Logistic regression estimates the probability that a future SOW will be
classified as a success or failure given a set of performance criteria.
A logistic regression model is defined by:
\begin{equation*}
\begin{split}ln \bigg (\frac{p_i}{1-p_i} \bigg ) = X^T_i \beta\end{split}
\end{equation*}
\sphinxAtStartPar
where \(p_i\) is the probability the performance in the
\(i^{th}\) SOW will be classified as a success, \(X_i\) is the
vector of covariates describing the \(i^{th}\) SOW, and
\(\beta\) is the vector of coefficients describing the relationship
between the covariates and the response, which here will be estimated
using maximum likelihood estimation.

\sphinxAtStartPar
A logistic regression model was fit to the ensemble of SOWs using the
performance criteria defined in step 2. Logistic regression modeling was
conducted using the \sphinxhref{https://www.statsmodels.org/stable/index.html}{Statsmodel
Python} package. The
data required for the full analysis is too large to include in this
tutorial, but results can be found in the data file loaded below.

\sphinxAtStartPar
The results files contain the occurence of different frequency and
magnitude combinations under the experiment, in increments of 10,
between 0 and 100. These combinations (100 for each user) are
alternative decision\sphinxhyphen{}relevant metrics that can be used for scenario
discovery.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} Set arrays for shortage frequencies and magnitudes
frequencies = np.arange(10, 110, 10)
magnitudes = np.arange(10, 110, 10)
realizations = 10


\PYGZsh{} Load performance and pseudo r scores for each of the users
results = [msdbook.load\PYGZus{}user\PYGZus{}heatmap\PYGZus{}array(all\PYGZus{}IDs[i]) / 100 for i in range(len(all\PYGZus{}IDs))]
\end{sphinxVerbatim}


\subsubsection{Step 4: Factor ranking}
\label{\detokenize{A2_Jupyter_Notebooks:step-4-factor-ranking}}
\sphinxAtStartPar
To rank the importance of each uncertain factor, we utilize McFadden’s
psuedo\sphinxhyphen{}R2, a measure that quantifies the improvement of the model when
utilizing each given predictor as compared to prediction using the mean
of the data set:
\begin{equation*}
\begin{split}R^2_{McFadden}=1-\frac{ln \hat{L}(M_{full})}{ln \hat{L}(M_{intercept})}\end{split}
\end{equation*}
\sphinxAtStartPar
Where \(ln \hat{L}(M_{full})\) is the log likelihood of the full
model (including the predictor) and \(ln \hat{L}(M_{intercept})\) is
the log likelihood of the intercept model (which predicts the mean
probability of success across all SOWs).

\sphinxAtStartPar
Higher values of McFadden’s psuedo\sphinxhyphen{}R2 indicate higher factor importance
(when the likelihood of the full model approaches one, the ratio of the
likelihood of the full model compared to the intercept model will get
very small).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
scores = [msdbook.load\PYGZus{}user\PYGZus{}pseudo\PYGZus{}scores(all\PYGZus{}IDs[i]) for i in range(len(all\PYGZus{}IDs))]

freq = [1,0,0]
mag = [7,3,7]
\end{sphinxVerbatim}


\subsubsection{Step 5: Draw factor maps}
\label{\detokenize{A2_Jupyter_Notebooks:step-5-draw-factor-maps}}
\sphinxAtStartPar
The McFadden’s psuedo\sphinxhyphen{}R2 scores files contain preliminary logistic
regression results on parameter importance for each of these
combinations. Using these psuedo\sphinxhyphen{}R2 scores we will identify the two most
important factors for each metric which we’ll use to generate the final
scenario discovery maps (note: there may be more than two important
metrics for each user, but here we will demonstrate by mapping two).

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} setup figure
fig, axes = plt.subplots(3,1, figsize=(6,18), tight\PYGZus{}layout=True)
fig.patch.set\PYGZus{}facecolor(\PYGZsq{}white\PYGZsq{})

for i in range(len(axes.flat)):

    ax = axes.flat[i]

    allSOWsperformance = results[i]
    all\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores = scores[i]

    \PYGZsh{} construct dataframe
    dta = pd.DataFrame(data=np.repeat(LHsamples, realizations, axis = 0), columns=param\PYGZus{}names)
    dta[\PYGZsq{}Success\PYGZsq{}] = allSOWsperformance[freq[i],mag[i],:]

    pseudo\PYGZus{}r\PYGZus{}scores = all\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores[str(frequencies[freq[i]])+\PYGZsq{}yrs\PYGZus{}\PYGZsq{}+str(magnitudes[mag[i]])+\PYGZsq{}prc\PYGZsq{}].values
    top\PYGZus{}predictors = np.argsort(pseudo\PYGZus{}r\PYGZus{}scores)[::\PYGZhy{}1][:2] \PYGZsh{}Sort scores and pick top 2 predictors

    \PYGZsh{} define color map for dots representing SOWs in which the policy
    \PYGZsh{} succeeds (light blue) and fails (dark red)
    dot\PYGZus{}cmap = mpl.colors.ListedColormap(np.array([[227,26,28],[166,206,227]])/255.0)

    \PYGZsh{} define color map for probability contours
    contour\PYGZus{}cmap = mpl.cm.get\PYGZus{}cmap(\PYGZsq{}RdBu\PYGZsq{})

    \PYGZsh{} define probability contours
    contour\PYGZus{}levels = np.arange(0.0, 1.05,0.1)

    \PYGZsh{} define base values of the predictors
    SOW\PYGZus{}values = np.array([1,1,1,1,0,0,1,1,1,1,1,0,0,0]) \PYGZsh{} default parameter values for base SOW
    base = SOW\PYGZus{}values[top\PYGZus{}predictors]
    ranges = param\PYGZus{}bounds[top\PYGZus{}predictors]

    \PYGZsh{} define grid of x (1st predictor), and y (2nd predictor) dimensions
    \PYGZsh{} to plot contour map over
    xgrid = np.arange(param\PYGZus{}bounds[top\PYGZus{}predictors[0]][0],
                      param\PYGZus{}bounds[top\PYGZus{}predictors[0]][1], np.around((ranges[0][1]\PYGZhy{}ranges[0][0])/500,decimals=4))
    ygrid = np.arange(param\PYGZus{}bounds[top\PYGZus{}predictors[1]][0],
                      param\PYGZus{}bounds[top\PYGZus{}predictors[1]][1], np.around((ranges[1][1]\PYGZhy{}ranges[1][0])/500,decimals=4))
    all\PYGZus{}predictors = [ dta.columns.tolist()[i] for i in top\PYGZus{}predictors]
    dta[\PYGZsq{}Interaction\PYGZsq{}] = dta[all\PYGZus{}predictors[0]]*dta[all\PYGZus{}predictors[1]]

    \PYGZsh{} logistic regression here
    predictor\PYGZus{}list = [all\PYGZus{}predictors[i] for i in [0,1]]
    result = msdbook.fit\PYGZus{}logit(dta, predictor\PYGZus{}list)

    \PYGZsh{} plot contour map
    contourset = msdbook.plot\PYGZus{}contour\PYGZus{}map(ax, result, dta, contour\PYGZus{}cmap,
                                          dot\PYGZus{}cmap, contour\PYGZus{}levels, xgrid,
                                          ygrid, all\PYGZus{}predictors[0], all\PYGZus{}predictors[1], base)

    ax.set\PYGZus{}title(usernames[i])

\PYGZsh{} set up colorbar
cbar\PYGZus{}ax = fig.add\PYGZus{}axes([0.98, 0.15, 0.05, 0.7])
cbar = fig.colorbar(contourset, cax=cbar\PYGZus{}ax)
cbar\PYGZus{}ax.set\PYGZus{}ylabel(\PYGZsq{}Probability of Success\PYGZsq{}, fontsize=16)
cbar\PYGZus{}ax.tick\PYGZus{}params(axis=\PYGZsq{}y\PYGZsq{}, which=\PYGZsq{}major\PYGZsq{}, labelsize=12)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Optimization} \PYG{n}{terminated} \PYG{n}{successfully}\PYG{o}{.}
         \PYG{n}{Current} \PYG{n}{function} \PYG{n}{value}\PYG{p}{:} \PYG{l+m+mf}{0.378619}
         \PYG{n}{Iterations} \PYG{l+m+mi}{8}
\PYG{n}{Optimization} \PYG{n}{terminated} \PYG{n}{successfully}\PYG{o}{.}
         \PYG{n}{Current} \PYG{n}{function} \PYG{n}{value}\PYG{p}{:} \PYG{l+m+mf}{0.397285}
         \PYG{n}{Iterations} \PYG{l+m+mi}{8}
\PYG{n}{Optimization} \PYG{n}{terminated} \PYG{n}{successfully}\PYG{o}{.}
         \PYG{n}{Current} \PYG{n}{function} \PYG{n}{value}\PYG{p}{:} \PYG{l+m+mf}{0.377323}
         \PYG{n}{Iterations} \PYG{l+m+mi}{8}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=493\sphinxpxdimen,height=1288\sphinxpxdimen]{{notebook_logistic_output_11_1}.png}

\sphinxAtStartPar
The figure above demonstrates how different combinations of the
uncertain factors lead to success or failure in different states of the
world, which are denoted by the blue and red dots. The probability of
success and failure are further denoted by the contours in the figure.
Several insights can be drawn from this figure.

\sphinxAtStartPar
First, using metrics chosen to be decision\sphinxhyphen{}relevant (specific to each
user) causes different factors to be identified as most important by
this scenario\sphinxhyphen{}discovery exercise (the x\sphinxhyphen{} and y\sphinxhyphen{}axes for each of the
subplots). In other words, depending on what the decision makers of this
system want to prioritize they might choose to monitor different
uncertain factors to track performance.

\sphinxAtStartPar
Second, in the top panel, the two identified factors appear to also have
an interactive effect on the metric used (shortages of a certain level
and frequency in this example). In terms of scenario discovery, the
Patient Rule Induction Method (PRIM) or Classification And Regression
Trees (CART) would not be able to delineate this non\sphinxhyphen{}linear space and
would therefore misclassify parameter combinations as ‘desirable’ when
they were in fact undesirable, and vice versa.

\sphinxAtStartPar
Lastly, logistic regression also produces contours of probability of
success, i.e. different factor\sphinxhyphen{}value combinations are assigned different
probabilities that a shortage level will be exceeded. This allows the
decision makers to evaluate these insights while considering their risk
aversion.


\section{HYMOD Dynamics Tutorial}
\label{\detokenize{A2_Jupyter_Notebooks:hymod-dynamics-tutorial}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
Run the tutorial interactively:  \sphinxhref{https://mybinder.org/v2/gh/IMMM-SFA/msd\_uncertainty\_ebook/6baaa2d214ca3d8a53f01f5bfb7340bf1e097ac2?filepath=notebooks\%2Fhymod.ipynb}{HYMOD Notebook}
\end{sphinxadmonition}


\subsection{Tutorial: Sensitivity Analysis of the HYMOD Model}
\label{\detokenize{A2_Jupyter_Notebooks:tutorial-sensitivity-analysis-of-the-hymod-model}}
\sphinxAtStartPar
This tutorial has been developed to showcase some of the sensitivity
analysis and uncertainty quantification concepts and tools established
in the main text in the context of HYMOD, a rainfall\sphinxhyphen{}runoff model. The
tutorial includes the following sections:


\subsubsection{Introduction to HYMOD and Sensitivity Analysis}
\label{\detokenize{A2_Jupyter_Notebooks:introduction-to-hymod-and-sensitivity-analysis}}
\sphinxAtStartPar
{\hyperref[\detokenize{A2_Jupyter_Notebooks:hymod}]{\emph{1\sphinxhyphen{} Introduction to a simple hydrologic model (HYMOD)}}} {\hyperref[\detokenize{A2_Jupyter_Notebooks:sensitivity}]{\emph{2\sphinxhyphen{} An
overview of sensitivity analysis using SALib}}} \sphinxhref{sa\_metrics}{3\sphinxhyphen{}
Calculation of sensitivity analysis metrics}


\subsubsection{Time\sphinxhyphen{}Varying Sensitivity Analysis}
\label{\detokenize{A2_Jupyter_Notebooks:time-varying-sensitivity-analysis}}
\sphinxAtStartPar
{\hyperref[\detokenize{A2_Jupyter_Notebooks:TVSA}]{\emph{4\sphinxhyphen{} Time\sphinxhyphen{}varying sensitivity analysis}}}


\subsubsection{Ensemble\sphinxhyphen{}based Parametric Uncertainty}
\label{\detokenize{A2_Jupyter_Notebooks:ensemble-based-parametric-uncertainty}}
\sphinxAtStartPar
{\hyperref[\detokenize{A2_Jupyter_Notebooks:GLUE}]{\emph{5\sphinxhyphen{} Generalized Likelihood Uncertainty Estimation (GLUE)}}} {\hyperref[\detokenize{A2_Jupyter_Notebooks:precalibration}]{\emph{6\sphinxhyphen{}
Pre\sphinxhyphen{}Calibration}}}

\sphinxAtStartPar
It is important to note that, although in this tutorial we focus on
HYMOD, which is a hydrologic model, it can also be thought of as an
example of a model that abstracts complex non\sphinxhyphen{}linear systems. Therefore,
many of the methods that we use in the tutorial can be applied to
numerical models that simulate other complex non\sphinxhyphen{}linear systems.


\subsection{Introduction to HYMOD and Sensitivity Analysis}
\label{\detokenize{A2_Jupyter_Notebooks:id3}}

\subsection{1\sphinxhyphen{} HYMOD}
\label{\detokenize{A2_Jupyter_Notebooks:hymod}}
\sphinxAtStartPar
HYMOD is a simple hydrologic model (rainfall\sphinxhyphen{}runoff model) that
simulates key hydrologic fluxes such as infiltration, streamflow and
evapotranspiration. The model has been originally developed and used for
river flow forecasting. However, in the last two decades the model has
been widely used to explore different sensitivity analysis (e.g.,
\sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20124}{Herman et al.,
2013}),
uncertainty quantification (e.g., \sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2006WR005205}{Smith et al.,
2008}),
and optimization (e.g., \sphinxhref{https://www.sciencedirect.com/science/article/pii/S0022169414006362?casa\_token=IRqE19Hkfa8AAAAA:\_fXOqfwpzxMpchvu8\_0njCe0Ok9H29Gyw2F46l9PzG9UVODDTUg6wIOSiyp6uybGevNVnZ7N}{Ye et al.,
2014})
concepts.

\sphinxAtStartPar
There are two main assumptions in the model: 1) Rainfall is generated
through infiltration excess overland flow. 2) Runoff generation can be
formulated by the probability\sphinxhyphen{}distributed principle (Moore, 1985).
Therefore the cumulative rate storage capacity \((F(C))\) can be
calculated using the following relationship:
\begin{equation*}
\begin{split}F(C) = 1 - (1 - \frac{C}{C_{MAX}})-{B_{exp}}\end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{:math:\textasciigrave{}C\textasciigrave{}} is the water storage capacity; \sphinxstylestrong{:math:\textasciigrave{}C\_\{MAX\}\textasciigrave{}} is
the parameter describing basin maximum water storage capacity (mm); and
\sphinxstylestrong{:math:\textasciigrave{}B\_\{exp\}\textasciigrave{}} is the parameter describing the degree of spatial
variability within the basin.

\sphinxAtStartPar
The portion of precipitation that exceeds the water storage capacity is
treated as runoff. The evapotranspiration is equal to potential
evapotranspiration (\(PET\)) if enough water is available.
Otherwise, it equals the available water storage.

\sphinxAtStartPar
Then, based on a parameter \(Alpha\), the runoff is divided into
quick flow and slow flow, which are routed through three identical quick
flow tanks \(Q1, Q2, Q3\) and a parallel slow flow tank,
respectively.

\sphinxAtStartPar
The flow rates in the routing system are described by the resident time
in the quick tanks \(K_q\) (day) and the slow tank \(K_s\)
(day), respectively.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=850\sphinxpxdimen,height=270\sphinxpxdimen]{{hymod}.png}
\caption{alt text}\label{\detokenize{A2_Jupyter_Notebooks:id9}}\end{figure}

\sphinxAtStartPar
Vrugt et al., (2008)


\subsubsection{1\sphinxhyphen{}1 Model Parameters}
\label{\detokenize{A2_Jupyter_Notebooks:model-parameters}}
\sphinxAtStartPar
\(C_{MAX}\): parameters describing basin maximum water storage
capacity (mm)

\sphinxAtStartPar
\(B_{exp}\): parameters describing the degree of spatial variability
within the basin between 0 and Huz

\sphinxAtStartPar
\(Alp\): Fraction of runoff contributing to quick flow

\sphinxAtStartPar
\(K_q\): Quick flow residence time of linear infinite reservoir (the
Kq values of all three linear reservoirs are the same)

\sphinxAtStartPar
\(K_s\): Slow flow residence time of linear infinite reservoir


\subsubsection{1\sphinxhyphen{}2 Input data}
\label{\detokenize{A2_Jupyter_Notebooks:input-data}}
\sphinxAtStartPar
The HYMOD model only requires precipitation and potential
evapotranspiration as inputs. The Leaf River example that we use here is
also a widely used test case of HYMOD. The dataset also includes
observed runoff that we later use to evaluate the performace of each
sensitvity analysis sample set.

\sphinxAtStartPar
We can use the following to read the input file:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
import msdbook

import numpy as np
import pandas as pd
import seaborn as sns

from sklearn import metrics
from matplotlib import pyplot as plt

\PYGZsh{} load example data
msdbook.install\PYGZus{}package\PYGZus{}data()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Downloading} \PYG{n}{example} \PYG{n}{data} \PYG{k}{for} \PYG{n}{msdbook} \PYG{n}{version} \PYG{l+m+mf}{0.1}\PYG{l+m+mf}{.5}\PYG{o}{.}\PYG{o}{.}\PYG{o}{.}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{uncertain\PYGZus{}params\PYGZus{}bounds}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}metric\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{solutions}\PYG{o}{.}\PYG{n}{resultfile}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LHsamples\PYGZus{}original\PYGZus{}1000}\PYG{o}{.}\PYG{n}{txt}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{3704614}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{param\PYGZus{}values}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{collapse\PYGZus{}days}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}params\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7000550}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}heatmap}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{sa\PYGZus{}by\PYGZus{}yr\PYGZus{}delta}\PYG{o}{.}\PYG{n}{npy}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{l+m+mi}{7200799}\PYG{n}{\PYGZus{}pseudo\PYGZus{}r\PYGZus{}scores}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{LeafCatch}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{hymod\PYGZus{}simulations\PYGZus{}256samples}\PYG{o}{.}\PYG{n}{csv}
\PYG{n}{Unzipped}\PYG{p}{:} \PYG{o}{/}\PYG{n}{srv}\PYG{o}{/}\PYG{n}{conda}\PYG{o}{/}\PYG{n}{envs}\PYG{o}{/}\PYG{n}{notebook}\PYG{o}{/}\PYG{n}{lib}\PYG{o}{/}\PYG{n}{python3}\PYG{l+m+mf}{.7}\PYG{o}{/}\PYG{n}{site}\PYG{o}{\PYGZhy{}}\PYG{n}{packages}\PYG{o}{/}\PYG{n}{msdbook}\PYG{o}{/}\PYG{n}{data}\PYG{o}{/}\PYG{n}{Robustness}\PYG{o}{.}\PYG{n}{txt}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load the HYMOD input file
leaf\PYGZus{}data = msdbook.load\PYGZus{}hymod\PYGZus{}input\PYGZus{}file()

\PYGZsh{} extract the first eleven years of data
leaf\PYGZus{}data = leaf\PYGZus{}data.iloc[0:4015].copy()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} There are only three columns in the file including precipitation, potential evapotranspiration and  streamflow
leaf\PYGZus{}data.head()
\end{sphinxVerbatim}




\subsubsection{1\sphinxhyphen{}3 Baseline Model Simulation}
\label{\detokenize{A2_Jupyter_Notebooks:baseline-model-simulation}}
\sphinxAtStartPar
We can start our sensitivity analysis experiment with running HYMOD
using its default parameters.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} assign input parameters to generate a baseline simulated streamflow
Nq = 3  \PYGZsh{} Number of quickflow routing tanks
Kq = 0.5 \PYGZsh{} Quickflow routing tanks\PYGZsq{} rate parameter
Ks =  0.001 \PYGZsh{} Slowflow routing tank\PYGZsq{}s rate parameter
Alp = 0.5 \PYGZsh{} Quick/slow split parameter
Huz = 100 \PYGZsh{} Maximum height of soil moisture accounting tank
B = 1.0 \PYGZsh{} Scaled distribution function shape parameter

\PYGZsh{} Note that the number of years is 11 years. One year of model warm\PYGZhy{}up and ten years are used for actual simulation
model = msdbook.hymod(Nq, Kq, Ks, Alp, Huz, B, leaf\PYGZus{}data, ndays=4015)
\end{sphinxVerbatim}


\subsubsection{1\sphinxhyphen{}4 Model Outputs}
\label{\detokenize{A2_Jupyter_Notebooks:model-outputs}}
\sphinxAtStartPar
Model outputs include actual evapotranspiration, quick and fast
streamflow, and combined runoff. In this tutorial we focus on the total
daily runoff (\(m-3/s\)). We can use the following script to plot
simulated streamflow against observed streamflow.


\subsubsection{Variables}
\label{\detokenize{A2_Jupyter_Notebooks:variables}}
\sphinxAtStartPar
\sphinxstylestrong{PP}: Precipitation

\sphinxAtStartPar
\sphinxstylestrong{ET}: Evapotranspiration

\sphinxAtStartPar
\sphinxstylestrong{OV}: Runoff

\sphinxAtStartPar
\sphinxstylestrong{Qq}: Quick Flow

\sphinxAtStartPar
\sphinxstylestrong{Qs}: Slow Flow

\sphinxAtStartPar
\sphinxstylestrong{QQ}: Streamflow (Quick Flow + Slow Flow)

\sphinxAtStartPar
\sphinxstylestrong{XHuz} and \sphinxstylestrong{XCuz}: Current moisture state of soil moisture
accounting component (as depth XH or volume XC)


\subsubsection{Plot the observed versus simulated streamflow.}
\label{\detokenize{A2_Jupyter_Notebooks:plot-the-observed-versus-simulated-streamflow}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-observed-vs-simulated-streamflow}{plot\_observed\_vs\_simulated\_streamflow}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{msdbook}\PYG{o}{.}\PYG{n}{plot\PYGZus{}observed\PYGZus{}vs\PYGZus{}simulated\PYGZus{}streamflow}\PYG{p}{(}\PYG{n}{df}\PYG{o}{=}\PYG{n}{leaf\PYGZus{}data}\PYG{p}{,} \PYG{n}{hymod\PYGZus{}dict}\PYG{o}{=}\PYG{n}{model}\PYG{p}{)}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=718\sphinxpxdimen,height=384\sphinxpxdimen]{{output_16_0}.png}


\subsection{2\sphinxhyphen{} Sensitivity Analysis}
\label{\detokenize{A2_Jupyter_Notebooks:sensitivity-analysis}}
\sphinxAtStartPar
Here we use the SALib Python library to explore how different HYMOD
input parameters affect model streamflow simulations. For this exercise,
we only use Sobol variance\sphinxhyphen{}based method. The following commands can be
used to import SALib

\begin{sphinxVerbatim}[commandchars=\\\{\}]
from SALib.sample import saltelli
from SALib.analyze import sobol
from SALib.analyze import delta
\end{sphinxVerbatim}


\subsubsection{2\sphinxhyphen{}2 Model simulations for sensitivity analysis}
\label{\detokenize{A2_Jupyter_Notebooks:model-simulations-for-sensitivity-analysis}}
\sphinxAtStartPar
We first define the model input and their ranges.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
problem\PYGZus{}hymod = \PYGZob{}
    \PYGZsq{}num\PYGZus{}vars\PYGZsq{}: 5,
    \PYGZsq{}names\PYGZsq{}: [\PYGZsq{}Kq\PYGZsq{}, \PYGZsq{}Ks\PYGZsq{}, \PYGZsq{}Alp\PYGZsq{}, \PYGZsq{}Huz\PYGZsq{}, \PYGZsq{}B\PYGZsq{}],
    \PYGZsq{}bounds\PYGZsq{}: [[0.1, 1],  \PYGZsh{} Kq
               [0, 0.1],  \PYGZsh{} Ks
               [0, 1],    \PYGZsh{} Alp
               [0.1, 500],  \PYGZsh{} Huz
               [0, 1.9]]  \PYGZsh{} B
\PYGZcb{}
\end{sphinxVerbatim}

\sphinxAtStartPar
Now we need to sample and then run the model for each of the sample
sets. We will load a sample that has already been created
\sphinxcode{\sphinxupquote{param\_values\_hymod}} for demonstration purposes. The actual model
simulation takes an extended period, so we also load the simulation data
from a previous run. The following demonstrates how to conduct this
analysis:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} generate 256 samples. This is an arbitrary number.}
\PYG{n}{param\PYGZus{}values\PYGZus{}hymod} \PYG{o}{=} \PYG{n}{saltelli}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{n}{problem\PYGZus{}hymod}\PYG{p}{,} \PYG{l+m+mi}{256}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} dictionary to store outputs in}
\PYG{n}{d\PYGZus{}outputs} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} run simulation for each parameter sample}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} run model for each sensitivity analysis parameter sets}
    \PYG{n}{hymod\PYGZus{}output} \PYG{o}{=} \PYG{n}{msdbook}\PYG{o}{.}\PYG{n}{hymod}\PYG{p}{(}\PYG{n}{Nq}\PYG{p}{,}
                                 \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,}
                                 \PYG{n}{leaf\PYGZus{}data}\PYG{p}{,}
                                 \PYG{n}{ndays}\PYG{o}{=}\PYG{l+m+mi}{4015}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} store the simulated total flow discharge}
    \PYG{n}{d\PYGZus{}outputs}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Q}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{hymod\PYGZus{}output}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Q}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}


\PYG{n}{Q\PYGZus{}df\PYGZus{}bw} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{d\PYGZus{}outputs}\PYG{p}{)}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load previously generated parameter values
param\PYGZus{}values\PYGZus{}hymod = msdbook.load\PYGZus{}hymod\PYGZus{}params()

\PYGZsh{} number of samples
n\PYGZus{}samples = len(param\PYGZus{}values\PYGZus{}hymod)

\PYGZsh{} load previously generated hymod simulated outputs
Q\PYGZus{}df\PYGZus{}bw = msdbook.load\PYGZus{}hymod\PYGZus{}simulation()

\PYGZsh{} column names of each sample simulation number
sample\PYGZus{}column\PYGZus{}names = [i for i in Q\PYGZus{}df\PYGZus{}bw.columns if i[0] == \PYGZsq{}Q\PYGZsq{}]
\end{sphinxVerbatim}


\paragraph{Model Warm\sphinxhyphen{}up}
\label{\detokenize{A2_Jupyter_Notebooks:model-warm-up}}
\sphinxAtStartPar
A hydrological model such as HYMOD usually includes ordinary
differential equations that are sensitive to their initial condition.
They also have components in their underlying formulation that have long
memory such that prior time steps can affect their current simulations.
For example, soil moisture or groundwater can hold water for a long time
and therefore they are often considered to exhibit a long memory. This
can affect the partitioning of water to runoff and infiltration, while
also controlling the generation of base flow. Therefore, it is important
to have a reasonable initial value for them. To achieve this,
hydrologists usually extend their simulation period and after the
simulations, they remove that extended time period that has unreasonable
groundwater or surface water values. This time period is called the
warm\sphinxhyphen{}up time period.

\sphinxAtStartPar
Here we extended our simulation for one year (from 10 years to 11 years)
and we removed the first year of simulation, therefore our warm\sphinxhyphen{}up
period is one year.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} exclude the first year of simulation from the simulations and reset the index
Q\PYGZus{}df = Q\PYGZus{}df\PYGZus{}bw.iloc[365:4015].copy().reset\PYGZus{}index(drop=True)

\PYGZsh{} exclude the first year of the input data and reset the index
leaf\PYGZus{}data = leaf\PYGZus{}data.iloc[365:4015].copy().reset\PYGZus{}index(drop=True)
\end{sphinxVerbatim}


\subsubsection{2\sphinxhyphen{}3 Visual inspection of the model outputs}
\label{\detokenize{A2_Jupyter_Notebooks:visual-inspection-of-the-model-outputs}}
\sphinxAtStartPar
Here we create a figure that shows HYMOD streamflow outputs under
different sample sets, and compare them with the observed streamflow.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} add date columns to our simulation data frame; for this data our start date is 1/1/2000
date\PYGZus{}ts = pd.date\PYGZus{}range(start=\PYGZsq{}1/1/2000\PYGZsq{}, periods=3650, freq=\PYGZsq{}D\PYGZsq{})
Q\PYGZus{}df[\PYGZsq{}date\PYGZsq{}] = date\PYGZus{}ts
Q\PYGZus{}df[\PYGZsq{}year\PYGZsq{}] = date\PYGZus{}ts.year
Q\PYGZus{}df[\PYGZsq{}month\PYGZsq{}] = date\PYGZus{}ts.month
Q\PYGZus{}df[\PYGZsq{}day\PYGZsq{}] = date\PYGZus{}ts.day

\PYGZsh{} aggregate the simulated observed streamflow to monthly mean
df\PYGZus{}sim\PYGZus{}mth\PYGZus{}mean = Q\PYGZus{}df.groupby([\PYGZsq{}year\PYGZsq{}, \PYGZsq{}month\PYGZsq{}])[sample\PYGZus{}column\PYGZus{}names].mean()

\PYGZsh{} do the same for the observed data
date\PYGZus{}ts = pd.date\PYGZus{}range(start=\PYGZsq{}1/1/2000\PYGZsq{}, periods=len(leaf\PYGZus{}data), freq=\PYGZsq{}D\PYGZsq{})
leaf\PYGZus{}data[\PYGZsq{}date\PYGZsq{}] = date\PYGZus{}ts
leaf\PYGZus{}data[\PYGZsq{}year\PYGZsq{}] = date\PYGZus{}ts.year
leaf\PYGZus{}data[\PYGZsq{}month\PYGZsq{}] = date\PYGZus{}ts.month
leaf\PYGZus{}data[\PYGZsq{}day\PYGZsq{}] = date\PYGZus{}ts.day

\PYGZsh{} aggregate the daily observed streamflow to monthly mean
df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean = leaf\PYGZus{}data.groupby([\PYGZsq{}year\PYGZsq{}, \PYGZsq{}month\PYGZsq{}]).mean()
\end{sphinxVerbatim}

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-observed-vs-sensitivity-streamflow}{plot\_observed\_vs\_sensitivity\_streamflow}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
ax = msdbook.plot\PYGZus{}observed\PYGZus{}vs\PYGZus{}sensitivity\PYGZus{}streamflow(df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean,
                                                     df\PYGZus{}sim=df\PYGZus{}sim\PYGZus{}mth\PYGZus{}mean)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=602\sphinxpxdimen,height=275\sphinxpxdimen]{{output_28_0}.png}


\subsection{3\sphinxhyphen{} Calculation of Sensitivity Analysis Indices}
\label{\detokenize{A2_Jupyter_Notebooks:calculation-of-sensitivity-analysis-indices}}
\sphinxAtStartPar
There are different options to calculate sensitivity indices. The
following section aggregates model streamflow outputs and calculates the
sensitivity indices.


\subsubsection{3\sphinxhyphen{}1 Aggregated sensitivity analysis indices}
\label{\detokenize{A2_Jupyter_Notebooks:aggregated-sensitivity-analysis-indices}}
\sphinxAtStartPar
This is the simplest way of calculating sensitivity analysis metrics,
however, averaging all model response can lead to loss of information
that we further explore in the following sections.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} overall aggregated indices
Y = Q\PYGZus{}df[sample\PYGZus{}column\PYGZus{}names].mean().to\PYGZus{}numpy()

\PYGZsh{} Perform analysis
Si = delta.analyze(problem\PYGZus{}hymod, param\PYGZus{}values\PYGZus{}hymod, Y, print\PYGZus{}to\PYGZus{}console=False)
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
print(\PYGZsq{}First order indices = \PYGZsq{}, Si[\PYGZsq{}S1\PYGZsq{}])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{First} \PYG{n}{order} \PYG{n}{indices} \PYG{o}{=}  \PYG{p}{[}\PYG{l+m+mf}{0.00810372} \PYG{l+m+mf}{0.0049972}  \PYG{l+m+mf}{0.00508833} \PYG{l+m+mf}{0.60039872} \PYG{l+m+mf}{0.28942293}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Si[\PYGZsq{}S1\PYGZsq{}].sum()
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{l+m+mf}{0.9080109105125653}
\end{sphinxVerbatim}


\subsubsection{3\sphinxhyphen{}2 How do different performance metrics affect the results of our sensitivity analysis?}
\label{\detokenize{A2_Jupyter_Notebooks:how-do-different-performance-metrics-affect-the-results-of-our-sensitivity-analysis}}
\sphinxAtStartPar
Streamflow has many different properties. In this section, we discuss
how the selection of metrics can lead to fundamentally different
sensitivity analysis results. For example, one can only focus on
aggregated streamflow metrics such as mean (what has been presented so
far), or only on extreme events such as drought or floods.

\sphinxAtStartPar
Here we compare three different metrics: 1\sphinxhyphen{} Mean error (ME) 2\sphinxhyphen{} Root Mean
Square Error (RMSE) 3\sphinxhyphen{} Log\sphinxhyphen{}Root Mean Square Error (Log(RMSE))

\sphinxAtStartPar
Each of these metrics focuses on a specific attribute of streamflow. For
example, RMSE highlights the impacts of extreme flood events, while
LOG(RMSE) focuses on model performance during low\sphinxhyphen{}flow events.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} calculate error metrics
me = Q\PYGZus{}df[sample\PYGZus{}column\PYGZus{}names].apply(lambda x: (x\PYGZhy{}leaf\PYGZus{}data[\PYGZdq{}Strmflw\PYGZdq{}]), axis=0)
mse = Q\PYGZus{}df[sample\PYGZus{}column\PYGZus{}names].apply(lambda x: metrics.mean\PYGZus{}squared\PYGZus{}error(x, leaf\PYGZus{}data[\PYGZdq{}Strmflw\PYGZdq{}]), axis=0)
rmse = mse**(1/2)

\PYGZsh{} add error metrics to a dictionary
d\PYGZus{}metrics = \PYGZob{}\PYGZsq{}ME\PYGZsq{}: me.mean().values,
             \PYGZsq{}RMSE\PYGZsq{}: rmse.values,
             \PYGZsq{}LOG[RMSE]\PYGZsq{}: np.log10(rmse.values)\PYGZcb{}

\PYGZsh{} convert to a dataframe
df\PYGZus{}metrics\PYGZus{}SA = pd.DataFrame(d\PYGZus{}metrics)
\end{sphinxVerbatim}

\sphinxAtStartPar
We can use the following to calculate the SA indices for each metric and
visualize it. Results are pre\sphinxhyphen{}loaded for efficiency.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} performance analysis}
\PYG{n}{df\PYGZus{}metric\PYGZus{}sa\PYGZus{}result} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} conduct sensitivity analysis for each metric}
\PYG{k}{for} \PYG{n}{index}\PYG{p}{,} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{d\PYGZus{}metrics}\PYG{o}{.}\PYG{n}{keys}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} get the data as a numpy array for the target metric}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{d\PYGZus{}metrics}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} use the metric to conduct SA}
    \PYG{n}{Si} \PYG{o}{=} \PYG{n}{delta}\PYG{o}{.}\PYG{n}{analyze}\PYG{p}{(}\PYG{n}{problem\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{print\PYGZus{}to\PYGZus{}console}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add the sensitivity indices to the output data frame}
    \PYG{n}{df\PYGZus{}metric\PYGZus{}sa\PYGZus{}result}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{index}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load previously ran simulation
df\PYGZus{}metric\PYGZus{}sa\PYGZus{}result = msdbook.load\PYGZus{}hymod\PYGZus{}metric\PYGZus{}simulation()

\PYGZsh{} view results
df\PYGZus{}metric\PYGZus{}sa\PYGZus{}result
\end{sphinxVerbatim}



\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} create seaborn heatmap with required labels
plt.subplots(figsize=(10, 5))

\PYGZsh{} labels for y\PYGZhy{}axis
y\PYGZus{}axis\PYGZus{}labels = [\PYGZsq{}Mean Error\PYGZsq{}, \PYGZsq{}RSME\PYGZsq{}, \PYGZsq{}Log(RMSE)\PYGZsq{}]

\PYGZsh{} plot heatmap
ax = sns.heatmap(df\PYGZus{}metric\PYGZus{}sa\PYGZus{}result, yticklabels=y\PYGZus{}axis\PYGZus{}labels,  cmap=\PYGZsq{}rocket\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=535\sphinxpxdimen,height=303\sphinxpxdimen]{{output_40_0}.png}

\sphinxAtStartPar
The results indicate that different goodness\sphinxhyphen{}of\sphinxhyphen{}fit metrics can produce
different sensitivity indices. This is because streamflow time series
have several dimensions and regimes (e.g., extreme high flow and low
flow) and focusing on only one metric will neglect the sensitivity of
other dimensions.

\sphinxAtStartPar
Therefore, we can argue that a single goodness\sphinxhyphen{}of\sphinxhyphen{}fit measure will never
be able to capture the entire response of model to different parametric
combinations. For more discussion about this topic readers can refer to
\sphinxhref{https://www.sciencedirect.com/science/article/pii/S1574954110000580?via\%3Dihub}{Liu and Sun
(2010)}
and \sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR007255}{Foglia et al.,
(2009)}.


\subsection{4\sphinxhyphen{} Time\sphinxhyphen{}Varying Sensitivity Analysis}
\label{\detokenize{A2_Jupyter_Notebooks:id4}}
\sphinxAtStartPar
Hydrological processes are often state\sphinxhyphen{}dependent, meaning that their
responses are affected by the time\sphinxhyphen{}varying condition that they are in.
For example, rainfall\sphinxhyphen{}runoff processes are different in winter and
summer. These processes are also different during wet years and dry
years.

\sphinxAtStartPar
Hydrological processes are also path\sphinxhyphen{}dependent, meaning that previous
time\sphinxhyphen{}steps on the model affect the present and future simulation of
different hydrologic components. To take these properties into account,
we can zoom into different time periods to explore how the sensitivity
of model parameters evolve in different time steps. This is referred to
as time\sphinxhyphen{}varying sensitivity analysis.

\sphinxAtStartPar
For more information about time\sphinxhyphen{}varying sensitivity analysis, readers
can refer to \sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20124}{Herman et
al. (2013)}
and \sphinxhref{https://link.springer.com/article/10.1007/s12206-018-0223-8}{Xu et
al. (2018)}.


\subsubsection{4\sphinxhyphen{}1 Sensitivity analysis indices for each month}
\label{\detokenize{A2_Jupyter_Notebooks:sensitivity-analysis-indices-for-each-month}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} aggregate simulated streamflow data to monthly time series
df\PYGZus{}sim\PYGZus{}by\PYGZus{}mth\PYGZus{}mean = Q\PYGZus{}df.groupby(\PYGZsq{}month\PYGZsq{})[sample\PYGZus{}column\PYGZus{}names].mean()

\PYGZsh{} aggregate observed streamflow data to monthly time series
df\PYGZus{}obs\PYGZus{}by\PYGZus{}mth\PYGZus{}mean = leaf\PYGZus{}data.groupby(\PYGZsq{}month\PYGZsq{}).mean()
\end{sphinxVerbatim}

\sphinxAtStartPar
We can use the following to calculate the SA indices for each month and
visualize it. Results are pre\sphinxhyphen{}loaded for efficiency.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} set up dataframes to store outputs}
\PYG{n}{df\PYGZus{}mth\PYGZus{}s1} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}mth\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} iterate through each month}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} generate the simulation data}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{df\PYGZus{}sim\PYGZus{}by\PYGZus{}mth\PYGZus{}mean}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} run SA}
    \PYG{n}{Si} \PYG{o}{=} \PYG{n}{delta}\PYG{o}{.}\PYG{n}{analyze}\PYG{p}{(}\PYG{n}{problem\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{print\PYGZus{}to\PYGZus{}console}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add to output dataframes}
    \PYG{n}{df\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{df\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{delta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} convert to arrays}
\PYG{n}{arr\PYGZus{}mth\PYGZus{}s1} \PYG{o}{=} \PYG{n}{df\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{arr\PYGZus{}mth\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{values}
\end{sphinxVerbatim}


\paragraph{First\sphinxhyphen{}order Indices}
\label{\detokenize{A2_Jupyter_Notebooks:first-order-indices}}
\sphinxAtStartPar
The following can be used to visualize the time\sphinxhyphen{}varying first\sphinxhyphen{}order
indices. The first order represents the direct impacts of a specific
parameter on model outputs.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-monthly-heatmap}{plot\_monthly\_heatmap}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load previously ran data
arr\PYGZus{}mth\PYGZus{}delta, arr\PYGZus{}mth\PYGZus{}s1 = msdbook.load\PYGZus{}hymod\PYGZus{}monthly\PYGZus{}simulations()

\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}monthly\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}mth\PYGZus{}s1.T,
                                       df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}by\PYGZus{}mth\PYGZus{}mean,
                                       title=\PYGZsq{}First Order \PYGZhy{} Mean Monthly SA\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=727\sphinxpxdimen,height=370\sphinxpxdimen]{{output_49_0}.png}

\sphinxAtStartPar
This figure demonstrates the first order sensitivity indices when the
streamflow data are aggregated by month. The purple line represents the
observed monthly discharge. The figure indicates that the first order
indices are highest for B and Huz across all months and lowest for Alp,
Ks, and Kq.


\paragraph{Total\sphinxhyphen{}order indices}
\label{\detokenize{A2_Jupyter_Notebooks:total-order-indices}}
\sphinxAtStartPar
We can also focus on the total order sensitivity index that includes
first\sphinxhyphen{}order SA indices and interactions between parameters

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}monthly\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}mth\PYGZus{}delta.T,
                                       df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}by\PYGZus{}mth\PYGZus{}mean,
                                       title=\PYGZsq{}Total Order \PYGZhy{} Mean monthly SA\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=733\sphinxpxdimen,height=370\sphinxpxdimen]{{output_52_0}.png}

\sphinxAtStartPar
Notably, the total order sensitivity results are different than the
first order sensitivity results, which indicates that interactions
between the parameters (particularly in regards to \(Kq\),
\(Ks\), and \(Alp\)) contribute to variance in the HYMOD output.


\subsubsection{4\sphinxhyphen{}2 Annual sensitivity analysis indices}
\label{\detokenize{A2_Jupyter_Notebooks:annual-sensitivity-analysis-indices}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} group by year and get mean
df\PYGZus{}sim\PYGZus{}by\PYGZus{}yr\PYGZus{}mean = Q\PYGZus{}df.groupby([\PYGZsq{}year\PYGZsq{}])[sample\PYGZus{}column\PYGZus{}names].mean()

\PYGZsh{} group input data and get mean
df\PYGZus{}obs\PYGZus{}by\PYGZus{}yr\PYGZus{}mean = leaf\PYGZus{}data.groupby([\PYGZsq{}year\PYGZsq{}]).mean()
\end{sphinxVerbatim}

\sphinxAtStartPar
We can also calculate the sensitivity analysis indices for each
individual year. This will allow us to understand if model control
changes during different years. The following code first aggregates the
outputs to annual time steps, and then calculates the SA indices.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} set up dataframes to store outputs}
\PYG{n}{df\PYGZus{}yr\PYGZus{}s1} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df\PYGZus{}yr\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}yr\PYGZus{}s1}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} iterate through each year}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} generate the simulation data}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{df\PYGZus{}sim\PYGZus{}by\PYGZus{}yr\PYGZus{}mean}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} run SA}
    \PYG{n}{Si} \PYG{o}{=} \PYG{n}{delta}\PYG{o}{.}\PYG{n}{analyze}\PYG{p}{(}\PYG{n}{problem\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{print\PYGZus{}to\PYGZus{}console}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add to output dataframes}
    \PYG{n}{df\PYGZus{}yr\PYGZus{}s1}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{df\PYGZus{}yr\PYGZus{}delta}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{delta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} convert to arrays}
\PYG{n}{arr\PYGZus{}yr\PYGZus{}s1} \PYG{o}{=} \PYG{n}{df\PYGZus{}mth\PYGZus{}s1}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{arr\PYGZus{}yr\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}mth\PYGZus{}delta}\PYG{o}{.}\PYG{n}{values}
\end{sphinxVerbatim}


\paragraph{First\sphinxhyphen{}order indices}
\label{\detokenize{A2_Jupyter_Notebooks:id5}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-annual-heatmap}{plot\_annual\_heatmap}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load previously ran data
arr\PYGZus{}yr\PYGZus{}delta, arr\PYGZus{}yr\PYGZus{}s1 = msdbook.load\PYGZus{}hymod\PYGZus{}annual\PYGZus{}simulations()

\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}annual\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}yr\PYGZus{}s1.T,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}by\PYGZus{}yr\PYGZus{}mean,
                                      title=\PYGZsq{}First Order \PYGZhy{} Mean Annual SA\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=724\sphinxpxdimen,height=315\sphinxpxdimen]{{output_59_0}.png}

\sphinxAtStartPar
The first order sensitivities at the annual scale are not unlike the
first order monthly sensitivities. Once again, sensitivities vary across
year and Huz and B are the most consequential parameters.


\paragraph{Total\sphinxhyphen{}order indices}
\label{\detokenize{A2_Jupyter_Notebooks:id6}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}annual\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}yr\PYGZus{}delta.T,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}by\PYGZus{}yr\PYGZus{}mean,
                                      title=\PYGZsq{}Total Order \PYGZhy{} Mean Annual SA and Observed flow\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=731\sphinxpxdimen,height=315\sphinxpxdimen]{{output_62_0}.png}

\sphinxAtStartPar
Our results indicate that sensitivity analysis indices vary in different
years and now that interactions are included, the Kq, Ks, and Alp
variables impact the sensitivity of the streamflow output.


\subsubsection{4\sphinxhyphen{}3 Monthly time\sphinxhyphen{}varying sensitivity analysis}
\label{\detokenize{A2_Jupyter_Notebooks:monthly-time-varying-sensitivity-analysis}}
\sphinxAtStartPar
Although time\sphinxhyphen{}varying sensitivity analysis at average monthly and
average annual temporal resolutions is informative, TVSA is susceptible
to the aggregation issue that we discussed earlier in section 3\sphinxhyphen{}2. To
avoid that we can further discretize our time domain to zoom into
individual months. This will provide us with even more information about
model behavior and the sensitivity of different parameters in different
states of the system. The block of code demonstrates how to implement
the monthly TVSA.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} set up dataframes to store outputs}
\PYG{n}{df\PYGZus{}vary\PYGZus{}s1} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,}
                          \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

\PYG{n}{df\PYGZus{}vary\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} iterate through each month}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}

    \PYG{c+c1}{\PYGZsh{} generate the simulation data}
    \PYG{n}{Y} \PYG{o}{=} \PYG{n}{df\PYGZus{}sim\PYGZus{}mth\PYGZus{}mean}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} run SA}
    \PYG{n}{Si} \PYG{o}{=} \PYG{n}{delta}\PYG{o}{.}\PYG{n}{analyze}\PYG{p}{(}\PYG{n}{problem\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{param\PYGZus{}values\PYGZus{}hymod}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{print\PYGZus{}to\PYGZus{}console}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add to output dataframes}
    \PYG{n}{df\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{df\PYGZus{}vary\PYGZus{}delta}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{Si}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{delta}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} convert to arrays}
\PYG{n}{arr\PYGZus{}vary\PYGZus{}s1} \PYG{o}{=} \PYG{n}{df\PYGZus{}vary\PYGZus{}s1}\PYG{o}{.}\PYG{n}{values}
\PYG{n}{arr\PYGZus{}vary\PYGZus{}delta} \PYG{o}{=} \PYG{n}{df\PYGZus{}vary\PYGZus{}delta}\PYG{o}{.}\PYG{n}{values}
\end{sphinxVerbatim}


\paragraph{First\sphinxhyphen{}order indices}
\label{\detokenize{A2_Jupyter_Notebooks:id7}}
\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-varying-heatmap}{plot\_varying\_heatmap}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} load in previously ran data
arr\PYGZus{}vary\PYGZus{}delta, arr\PYGZus{}vary\PYGZus{}s1 = msdbook.load\PYGZus{}hymod\PYGZus{}varying\PYGZus{}simulations()

\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}varying\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}vary\PYGZus{}s1.T,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean,
                                      title=\PYGZsq{}First Order \PYGZhy{} Time\PYGZhy{}Varying SA\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=724\sphinxpxdimen,height=338\sphinxpxdimen]{{output_68_0}.png}

\sphinxAtStartPar
Compared to the TVSA when streamflow was aggregated, this figure
suggests that Kq is indeed a relevant parameter for influencing
streamflow output when individual months are considered.


\paragraph{Total order \sphinxhyphen{} time varying sensitivity analysis}
\label{\detokenize{A2_Jupyter_Notebooks:total-order-time-varying-sensitivity-analysis}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} plot figure
ax, ax2 = msdbook.plot\PYGZus{}varying\PYGZus{}heatmap(arr\PYGZus{}sim=arr\PYGZus{}vary\PYGZus{}delta.T,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean,
                                      title=\PYGZsq{}Total Order \PYGZhy{} Time\PYGZhy{}Varying SA\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=731\sphinxpxdimen,height=338\sphinxpxdimen]{{output_71_0}.png}

\sphinxAtStartPar
As above, the total order sensitivities further indicate the importance
of Kq that is not apparent if aggregation is utilized.


\subsection{Ensemble\sphinxhyphen{}based Parametric Uncertainty}
\label{\detokenize{A2_Jupyter_Notebooks:id8}}

\subsection{5\sphinxhyphen{} Generalized Likelihood Uncertainty Estimation (GLUE)}
\label{\detokenize{A2_Jupyter_Notebooks:generalized-likelihood-uncertainty-estimation-glue}}
\sphinxAtStartPar
The Generalized Likelihood Uncertainty Estimation (GLUE) is an
uncertainty analysis algorithm that has been widely used in hydrologic
studies. The main argument behind GLUE is rooted in model calibration
and the concept of equifinality. Calibration of complex simulation tools
such as hydrological models often produces more than one optimal or
near\sphinxhyphen{}optimal solutions and these solutions have equivalent chances to be
chosen \sphinxhref{https://www.sciencedirect.com/science/article/abs/pii/S0022169401004218}{(Beven and Freer,
2001)}.
This situation is called equifinality. GLUE provides a methodological
framework to handle this problem and consider more than one optimal
calibration set.

\sphinxAtStartPar
GLUE usually includes the following steps \sphinxhref{https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.3360060305?casa\_token=o2ooj-6wmC4AAAAA:WpVg1ysAtD59QbSpdHKX6IOjfjeHsOfqxCC6RvoXgiW6bDBRGNfdkOv-AH6h3WhT7-2mD4xmwzMi}{(Beven and Bineley,
1992)}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{)}%
\item {} 
\sphinxAtStartPar
Definition of a likelihood function

\item {} 
\sphinxAtStartPar
Definition of ranges of parameters

\item {} 
\sphinxAtStartPar
Sensitivity analysis

\item {} 
\sphinxAtStartPar
Calculating likelihood (goodness\sphinxhyphen{}of\sphinxhyphen{}fit) values for each model
simulation

\item {} 
\sphinxAtStartPar
Define a threshold and find sample sets that have higher likelihoods
than the threshold

\item {} 
\sphinxAtStartPar
Visualize the sample sets

\end{enumerate}


\subsubsection{5\sphinxhyphen{}1 Calculation of GLUE metrics}
\label{\detokenize{A2_Jupyter_Notebooks:calculation-of-glue-metrics}}

\paragraph{Likelihood calculation (inverse error variance)}
\label{\detokenize{A2_Jupyter_Notebooks:likelihood-calculation-inverse-error-variance}}
\sphinxAtStartPar
There are various likelihood metrics that have been used in previous
studies that use GLUE. A widely used example is inverse error variance
(IEV; \sphinxhref{https://link.springer.com/article/10.1007/s00477-008-0274-y}{Vrugt et
al. 2009}
and \sphinxhref{https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.3360060305?casa\_token=o2ooj-6wmC4AAAAA:WpVg1ysAtD59QbSpdHKX6IOjfjeHsOfqxCC6RvoXgiW6bDBRGNfdkOv-AH6h3WhT7-2mD4xmwzMi}{Beven and Bineley,
1992}):
\begin{equation*}
\begin{split}IEV = {({\sigma_{e}}-{2})}-{-T} = ({{\frac{SSR}{n-2}}})-{-T}\end{split}
\end{equation*}
\sphinxAtStartPar
The other metric that can be used as an estimation of likelihood is
normalized inverse error variance:
\begin{equation*}
\begin{split}NIEV = \frac{IEV}{\sum_{i=1}-{n} IEV(i)}\end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{SSR} is the sum of squared residuals; \(n\) is the number of
samples; and \(T\) is an arbitrary coefficient. Low \(T\) values
lead to equal weights placed on each sample set while higher \(T\)
values concentrate on the best parameter sets.

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} From Vrugt et al (2008) : inverse error variance

\PYGZsh{} T=0 means that we only select the best simulated values that are closer to the observed values
\PYGZsh{}  T=infinity means that all sample sets have the same probability
T= 0.70

\PYGZsh{} calculate metrics from Beven and Binley, 1992
df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}SSR\PYGZdq{}] = df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}RMSE\PYGZdq{}]**2 * 3650
df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}InverseErrorVariance\PYGZdq{}] = (df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}SSR\PYGZdq{}] / df\PYGZus{}metrics\PYGZus{}SA.shape[0])**(\PYGZhy{}T)
df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}Normalized\PYGZus{}IEV\PYGZdq{}] = df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}InverseErrorVariance\PYGZdq{}] / df\PYGZus{}metrics\PYGZus{}SA[\PYGZdq{}InverseErrorVariance\PYGZdq{}].sum()

\PYGZsh{} convert array to dataframe
param\PYGZus{}values\PYGZus{}hymod\PYGZus{}df = pd.DataFrame(param\PYGZus{}values\PYGZus{}hymod, columns=[\PYGZsq{}Kq\PYGZsq{}, \PYGZsq{}Ks\PYGZsq{}, \PYGZsq{}Alp\PYGZsq{}, \PYGZsq{}Huz\PYGZsq{}, \PYGZsq{}B\PYGZsq{}])

\PYGZsh{} combine the metrics and param values dataframes and calculate combined metrics
concat\PYGZus{}df = pd.concat([df\PYGZus{}metrics\PYGZus{}SA, param\PYGZus{}values\PYGZus{}hymod\PYGZus{}df], axis=1)
concat\PYGZus{}df[\PYGZdq{}Ks\PYGZus{}rescale\PYGZdq{}] = concat\PYGZus{}df[\PYGZdq{}Ks\PYGZdq{}] / 0.1
concat\PYGZus{}df[\PYGZdq{}Huz\PYGZus{}rescale\PYGZdq{}] = concat\PYGZus{}df[\PYGZdq{}Huz\PYGZdq{}] / 500
concat\PYGZus{}df[\PYGZdq{}B\PYGZus{}rescale\PYGZdq{}] = concat\PYGZus{}df[\PYGZdq{}B\PYGZdq{}] / 2

\PYGZsh{} display the first 5 rows of the dataframe
df\PYGZus{}metrics\PYGZus{}SA.head()
\end{sphinxVerbatim}




\paragraph{Distribution of likelihoods (inverse error variance) values}
\label{\detokenize{A2_Jupyter_Notebooks:distribution-of-likelihoods-inverse-error-variance-values}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} density plot and histogram of the inverse error variance
h = sns.histplot(data=df\PYGZus{}metrics\PYGZus{}SA,
                 x=\PYGZdq{}InverseErrorVariance\PYGZdq{},
                 kde=True,
                 bins=int(180/5),
                 color = \PYGZsq{}gold\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=385\sphinxpxdimen,height=260\sphinxpxdimen]{{output_80_0}.png}


\paragraph{Distribution of normalized inverse error variance values}
\label{\detokenize{A2_Jupyter_Notebooks:distribution-of-normalized-inverse-error-variance-values}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} density plot and histogram of the normalized inverse error variance
h = sns.histplot(data=df\PYGZus{}metrics\PYGZus{}SA,
                 x=\PYGZdq{}Normalized\PYGZus{}IEV\PYGZdq{},
                 kde=True,
                 bins=int(180/5),
                 color=\PYGZsq{}darkred\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=385\sphinxpxdimen,height=261\sphinxpxdimen]{{output_82_0}.png}


\paragraph{Selection of important sample sets and setting a threshold for physical/non\sphinxhyphen{}physical sample sets}
\label{\detokenize{A2_Jupyter_Notebooks:selection-of-important-sample-sets-and-setting-a-threshold-for-physical-non-physical-sample-sets}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} selection of important sample sets
percentile = 95

threshold = np.percentile(concat\PYGZus{}df[\PYGZdq{}InverseErrorVariance\PYGZdq{}], percentile)
print(f\PYGZdq{}Threshold using the \PYGZob{}percentile\PYGZcb{} percentile:  \PYGZob{}threshold\PYGZcb{}\PYGZdq{})

\PYGZsh{} select values greater than the threshold
selected\PYGZus{}values\PYGZus{}glue = concat\PYGZus{}df[concat\PYGZus{}df[\PYGZdq{}InverseErrorVariance\PYGZdq{}] \PYGZgt{} threshold]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Threshold} \PYG{n}{using} \PYG{n}{the} \PYG{l+m+mi}{95} \PYG{n}{percentile}\PYG{p}{:}  \PYG{l+m+mf}{0.4981408917485908}
\end{sphinxVerbatim}


\subsubsection{5\sphinxhyphen{}2 Visual inspection of GLUE results}
\label{\detokenize{A2_Jupyter_Notebooks:visual-inspection-of-glue-results}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} format the data frame so that it may be used for plotting
to\PYGZus{}plot = pd.melt(selected\PYGZus{}values\PYGZus{}glue,
                  id\PYGZus{}vars=[\PYGZsq{}ME\PYGZsq{}],
                  value\PYGZus{}vars=[\PYGZsq{}Kq\PYGZsq{}, \PYGZsq{}Ks\PYGZus{}rescale\PYGZsq{}, \PYGZsq{}Alp\PYGZsq{}, \PYGZsq{}Huz\PYGZus{}rescale\PYGZsq{}, \PYGZsq{}B\PYGZus{}rescale\PYGZsq{}])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} build a plot with multiple panels of scatter plots where ME is the target metric
g = sns.FacetGrid(to\PYGZus{}plot, col=\PYGZdq{}variable\PYGZdq{})

\PYGZsh{} map the scatter plots to the facet grid panels
gf = g.map(sns.scatterplot, \PYGZdq{}value\PYGZdq{}, \PYGZdq{}ME\PYGZdq{},  alpha=0.7)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=1069\sphinxpxdimen,height=208\sphinxpxdimen]{{output_87_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} build a plot with multiple panels of histogram plots where ME is the target metric
g = sns.FacetGrid(to\PYGZus{}plot, col=\PYGZdq{}variable\PYGZdq{})

\PYGZsh{} map the plots to the facet grid panels
gf = g.map(sns.histplot, \PYGZdq{}value\PYGZdq{}, kde=True)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=1069\sphinxpxdimen,height=208\sphinxpxdimen]{{output_88_0}.png}


\subsubsection{5\sphinxhyphen{}3 Comment on the GLUE results}
\label{\detokenize{A2_Jupyter_Notebooks:comment-on-the-glue-results}}
\sphinxAtStartPar
Our results suggest that it is challenging to find an clear and
interpretable relationship between different selected near\sphinxhyphen{}optimal
sample sets at least by visual inspection. The main reason for this is
that HYMOD includes a complex non\sphinxhyphen{}linear system of equations that is
also affected by initial conditions and complexity of its input time
series. Therefore, it does not have a clear control.

\sphinxAtStartPar
Glue has been widely used in hydrology, the original paper has more than
5000 citations. However, the likelihood measure that GLUE uses is not
actually a statistically sound likelihood metric and is in fact a
goodness\sphinxhyphen{}of\sphinxhyphen{}fit measure. Therefore, it might not produce valid insights
when dealing with situations of non\sphinxhyphen{}normality, heteroscedasticity, and
serial correlation. For more on these issues reader can refer to
\sphinxhref{https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR006822\%4010.1002/\%28ISSN\%291944-7973.ASSESS1}{Stedinger et al.,
(2008)},
\sphinxhref{https://www.sciencedirect.com/science/article/pii/S0022169406002162?casa\_token=Ml8dhBrO5PkAAAAA:Ake1YuQo0OxK6BaaG-8wIdHa\_kd4cuUpm7WiHBFur-G\_DlRze6Z0\_GkwWH3qHDLKwbJDO9mN}{Mantovan and Todini,
(2006)},
and \sphinxhref{https://onlinelibrary.wiley.com/doi/full/10.1002/hyp.10082}{Beven And Binley,
(2014)}.


\subsection{6\sphinxhyphen{} Pre\sphinxhyphen{}Calibration}
\label{\detokenize{A2_Jupyter_Notebooks:pre-calibration}}
\sphinxAtStartPar
Pre\sphinxhyphen{}calibration \sphinxhref{https://link.springer.com/article/10.1007/s00382-010-0921-0}{(Edwards et al,
2010)}
is a simplified method to deal with uncertainty in complex environmental
models. Pre\sphinxhyphen{}calibration can also be thought of as another method that
tackles the shortcomings and conceptual challenges involved in
calibration of complex environmental models. In pre\sphinxhyphen{}calibration instead
of finding the best solutions, we focus on finding the sample sets that
create outputs that are against the common understanding of the system.
These parameter sets are called non\sphinxhyphen{}physical parameter sets. In other
words, the probability that these parameters are among the best sample
sets is zero or near zero and can be neglected in practice.

\sphinxAtStartPar
Pre\sphinxhyphen{}calibration can include the following steps: 1) Sensitivity analysis
2) Definition of non\sphinxhyphen{}physical boundaries 3) Delineating regions in the
output space which are non\sphinxhyphen{}physical (Implausible) 4) Map non\sphinxhyphen{}physical
sets back to input space 5) Interpret the non\sphinxhyphen{}physical sample sets


\subsubsection{6\sphinxhyphen{}1 Pre\sphinxhyphen{}calibration calculations}
\label{\detokenize{A2_Jupyter_Notebooks:pre-calibration-calculations}}

\paragraph{Distribution of mean error, RMSE, and Log{[}RMSE{]} under different sample sets}
\label{\detokenize{A2_Jupyter_Notebooks:distribution-of-mean-error-rmse-and-log-rmse-under-different-sample-sets}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} set up figure and axis objects
fig, axs = plt.subplots(nrows=3, figsize=(14,16))

\PYGZsh{} axis 1 (first row in figure)
a = sns.histplot(data=concat\PYGZus{}df,
                 x=\PYGZdq{}RMSE\PYGZdq{},
                 ax=axs[0],
                 kde=True,
                 bins=int(180/5),
                 color=\PYGZsq{}peachpuff\PYGZsq{}).set\PYGZus{}title(\PYGZsq{}Distribution of RMSE in Different Sample Sets\PYGZsq{})

\PYGZsh{} axis 2 (second row in figure)
b = sns.histplot(data=concat\PYGZus{}df,
                 x=\PYGZdq{}ME\PYGZdq{},
                 ax=axs[1],
                 kde=True,
                 bins=int(180/5),
                 color=\PYGZsq{}lightgreen\PYGZsq{}).set\PYGZus{}title(\PYGZsq{}Distribution of ME in Different Sample Sets\PYGZsq{})

\PYGZsh{} axis 3 (third row in figure)
c = sns.histplot(data=concat\PYGZus{}df,
                 x=\PYGZdq{}LOG[RMSE]\PYGZdq{},
                 ax=axs[2],
                 kde=True,
                 bins=int(180/5),
                 color = \PYGZsq{}lightseagreen\PYGZsq{}).set\PYGZus{}title(\PYGZsq{}Distribution of LOG[RMSE] in Different Sample Sets\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=832\sphinxpxdimen,height=927\sphinxpxdimen]{{output_94_0}.png}


\paragraph{Setting a threshold for physical/non\sphinxhyphen{}physical sample sets}
\label{\detokenize{A2_Jupyter_Notebooks:setting-a-threshold-for-physical-non-physical-sample-sets}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} selection of physical/non\PYGZhy{}physical sample sets
percentile = 95

threshold\PYGZus{}precal = np.percentile(concat\PYGZus{}df[\PYGZdq{}RMSE\PYGZdq{}], percentile)
print(f\PYGZdq{}Threshold using the \PYGZob{}percentile\PYGZcb{} percentile:  \PYGZob{}threshold\PYGZcb{}\PYGZdq{})

\PYGZsh{} select values greater than the threshold
selected\PYGZus{}values\PYGZus{}precal = concat\PYGZus{}df[concat\PYGZus{}df[\PYGZdq{}RMSE\PYGZdq{}] \PYGZgt{} threshold\PYGZus{}precal]
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{Threshold} \PYG{n}{using} \PYG{n}{the} \PYG{l+m+mi}{95} \PYG{n}{percentile}\PYG{p}{:}  \PYG{l+m+mf}{0.4981408917485908}
\end{sphinxVerbatim}


\paragraph{Visual inspection of non\sphinxhyphen{}physical sample sets}
\label{\detokenize{A2_Jupyter_Notebooks:visual-inspection-of-non-physical-sample-sets}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} format the data frame so that it may be used for plotting
to\PYGZus{}plot\PYGZus{}precal = pd.melt(selected\PYGZus{}values\PYGZus{}precal,
                         id\PYGZus{}vars=[\PYGZsq{}ME\PYGZsq{}],
                         value\PYGZus{}vars=[\PYGZsq{}Kq\PYGZsq{}, \PYGZsq{}Ks\PYGZus{}rescale\PYGZsq{}, \PYGZsq{}Alp\PYGZsq{}, \PYGZsq{}Huz\PYGZus{}rescale\PYGZsq{}, \PYGZsq{}B\PYGZus{}rescale\PYGZsq{}])
\end{sphinxVerbatim}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} build a plot with multiple panels of scatter plots where ME is the target metric
g = sns.FacetGrid(to\PYGZus{}plot\PYGZus{}precal, col=\PYGZdq{}variable\PYGZdq{})

\PYGZsh{} map the scatter plots to the facet grid panels
gh = g.map(sns.scatterplot, \PYGZdq{}value\PYGZdq{}, \PYGZdq{}ME\PYGZdq{},  alpha=0.7, color=\PYGZsq{}plum\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=1069\sphinxpxdimen,height=208\sphinxpxdimen]{{output_99_0}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} build a plot with multiple panels of histogram plots where ME is the target metric
g = sns.FacetGrid(to\PYGZus{}plot\PYGZus{}precal, col=\PYGZdq{}variable\PYGZdq{})

\PYGZsh{} map the plots to the facet grid panels
gh = g.map(sns.histplot, \PYGZdq{}value\PYGZdq{}, kde=True, color=\PYGZsq{}plum\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=1069\sphinxpxdimen,height=208\sphinxpxdimen]{{output_100_0}.png}

\sphinxAtStartPar
The following figure shows the flow discharge provided by the ensemble
of parameters sets from Pre\sphinxhyphen{}Calibration versus the observed flow data.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-precalibration-flow}{plot\_precalibration\_flow}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZsh{} mean monthly indices
Q\PYGZus{}df\PYGZus{}precal = pd.concat([Q\PYGZus{}df.iloc[:, selected\PYGZus{}values\PYGZus{}precal.index],
                         Q\PYGZus{}df[[\PYGZsq{}month\PYGZsq{} , \PYGZsq{}year\PYGZsq{}]]],
                        axis=1)

\PYGZsh{} calculate year, month mean
df\PYGZus{}precal\PYGZus{}mth\PYGZus{}mean = Q\PYGZus{}df\PYGZus{}precal.groupby([\PYGZsq{}year\PYGZsq{}, \PYGZsq{}month\PYGZsq{}]).mean()

\PYGZsh{} plot observed versus pre\PYGZhy{}calibration outputs
ax = msdbook.plot\PYGZus{}precalibration\PYGZus{}flow(df\PYGZus{}sim=df\PYGZus{}precal\PYGZus{}mth\PYGZus{}mean,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=602\sphinxpxdimen,height=275\sphinxpxdimen]{{output_103_0}.png}

\sphinxAtStartPar
The following figure shows the flow discharge provided by the ensemble
of parameters sets from both pre\sphinxhyphen{}calibration and GLUE and how these
flows compare to the observed flow data.

\begin{sphinxadmonition}{note}{Note:}
\sphinxAtStartPar
\sphinxstylestrong{Tip:} View the source code used to create this plot here:  \sphinxhref{https://immm-sfa.github.io/msd\_uncertainty\_ebook/A3\_plotting\_code.html\#plot-precalibration-glue}{plot\_precalibration\_glue}
\end{sphinxadmonition}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
Q\PYGZus{}df\PYGZus{}glue = pd.concat([Q\PYGZus{}df.iloc[:, selected\PYGZus{}values\PYGZus{}glue.index],
                       Q\PYGZus{}df.loc[:,[\PYGZsq{}month\PYGZsq{} , \PYGZsq{}year\PYGZsq{}]]],
                      axis=1)

\PYGZsh{} calculate year, month mean
df\PYGZus{}glue\PYGZus{}mth\PYGZus{}mean = Q\PYGZus{}df\PYGZus{}glue.groupby([\PYGZsq{}year\PYGZsq{}, \PYGZsq{}month\PYGZsq{}]).mean()


\PYGZsh{} plot observed versus pre\PYGZhy{}calibration and GLUE outputs
ax = msdbook.plot\PYGZus{}precalibration\PYGZus{}glue(df\PYGZus{}precal=df\PYGZus{}precal\PYGZus{}mth\PYGZus{}mean,
                                      df\PYGZus{}glue=df\PYGZus{}glue\PYGZus{}mth\PYGZus{}mean,
                                      df\PYGZus{}obs=df\PYGZus{}obs\PYGZus{}mth\PYGZus{}mean)
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics[width=602\sphinxpxdimen,height=275\sphinxpxdimen]{{output_106_0}.png}


\subsubsection{6\sphinxhyphen{}2 Comments on Pre\sphinxhyphen{}Calibration}
\label{\detokenize{A2_Jupyter_Notebooks:comments-on-pre-calibration}}
\sphinxAtStartPar
Although Pre\sphinxhyphen{}calibrations provides a helpful and relatively simple
alternative for statistical inferences, it has some disadvantages. For
example, it is often subjective and challenging to provide a threshold
for non\sphinxhyphen{}physical model outputs. Moreover, as discussed earlier different
goodness\sphinxhyphen{}of\sphinxhyphen{}fit metrics can produce distinct physical and non\sphinxhyphen{}physical
sample sets.

\sphinxAtStartPar
More information about pre\sphinxhyphen{}calibration can be found in \sphinxhref{https://link.springer.com/article/10.1007/s00382-010-0921-0}{(Edwards et al.,
2010)},
and \sphinxhref{https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0170052}{Ruckert et al.,
(2017)}.


\chapter{Plotting Code Samples}
\label{\detokenize{A3_plotting_code:plotting-code-samples}}\label{\detokenize{A3_plotting_code::doc}}

\section{hymod.ipynb}
\label{\detokenize{A3_plotting_code:hymod-ipynb}}
\sphinxAtStartPar
The following are the plotting functions as described in the \sphinxcode{\sphinxupquote{hymod.ipynb}} Jupyter notebook tutorial.

\sphinxAtStartPar
The following are the necessary package imports to run these functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k+kn}{from} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{lines} \PYG{k+kn}{import} \PYG{n}{Line2D}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_observed\_vs\_simulated\_streamflow()}}
\label{\detokenize{A3_plotting_code:plot-observed-vs-simulated-streamflow}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}observed\PYGZus{}vs\PYGZus{}simulated\PYGZus{}streamflow}\PYG{p}{(}\PYG{n}{df}\PYG{p}{,} \PYG{n}{hymod\PYGZus{}dict}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot observed versus simulated streamflow.}

\PYG{l+s+sd}{    :param df:              Dataframe of hymod input data including columns for precip, potential evapotranspiration,}
\PYG{l+s+sd}{                            and streamflow}

\PYG{l+s+sd}{    :param hymod\PYGZus{}dict:      A dictionary of hymod outputs}
\PYG{l+s+sd}{    :type hymod\PYGZus{}dict:       dict}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set plot style}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{style}\PYG{o}{.}\PYG{n}{use}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{seaborn\PYGZhy{}white}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot observed streamflow}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pink}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot simulated streamflow}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{hymod\PYGZus{}dict}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Q}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set axis labels}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Streamflow(\PYGZdl{}m\PYGZca{}3/s\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Days}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set plot title}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed vs. Simulated Streamflow}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_observed\_vs\_sensitivity\_streamflow()}}
\label{\detokenize{A3_plotting_code:plot-observed-vs-sensitivity-streamflow}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}observed\PYGZus{}vs\PYGZus{}sensitivity\PYGZus{}streamflow}\PYG{p}{(}\PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{df\PYGZus{}sim}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot observed streamflow versus simulations generated from sensitivity analysis.}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly hymod input data including columns for precip,}
\PYG{l+s+sd}{                            potential evapotranspiration, and streamflow}

\PYG{l+s+sd}{    :param df\PYGZus{}sim:          Dataframe of mean monthly simulation data from sensitivity analysis}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{n}{month\PYGZus{}list} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}sim}\PYG{p}{)}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set labels}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Days}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge (m\PYGZca{}3/s)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plots all simulated streamflow cases under different sample sets}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n}{df\PYGZus{}sim}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{month\PYGZus{}list}\PYG{p}{,} \PYG{n}{df\PYGZus{}sim}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{pink}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot observed streamflow}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{month\PYGZus{}list}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed vs. Sensitivity Analysis Outputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_monthly\_heatmap()}}
\label{\detokenize{A3_plotting_code:plot-monthly-heatmap}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}monthly\PYGZus{}heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot a sensitivity metric overlain by observed flow.}

\PYG{l+s+sd}{    :param arr\PYGZus{}sim:         Numpy array of simulated metrics}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly observed data from sensitivity analysis}

\PYG{l+s+sd}{    :param title:           Title of plot}
\PYG{l+s+sd}{    :type title:            str}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot heatmap}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,}
                \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,}
                \PYG{n}{yticklabels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{sns}\PYG{o}{.}\PYG{n}{color\PYGZus{}palette}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ch:s=\PYGZhy{}.2,r=.6}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} setup overlay axis}
    \PYG{n}{ax2} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{twinx}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{12.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot points on line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{12.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set axis limits and labels}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{jan}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{feb}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mar}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{apr}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{may}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{jun}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{jul}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{aug}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sep}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{oct}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{nov}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dec}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge(\PYGZdl{}m\PYGZca{}3/s\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}\PYG{p}{,} \PYG{n}{ax2}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_annual\_heatmap()}}
\label{\detokenize{A3_plotting_code:plot-annual-heatmap}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}annual\PYGZus{}heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{14}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot a sensitivity metric overlain by observed flow..}

\PYG{l+s+sd}{    :param arr\PYGZus{}sim:         Numpy array of simulated metrics}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly observed data from sensitivity analysis}

\PYG{l+s+sd}{    :param title:           Title of plot}
\PYG{l+s+sd}{    :type title:            str}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot heatmap}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{sns}\PYG{o}{.}\PYG{n}{color\PYGZus{}palette}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{YlOrBr}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} setup overlay axis}
    \PYG{n}{ax2} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{twinx}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{10.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot points on line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{10.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up axis lables and limits}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{2000}\PYG{p}{,} \PYG{l+m+mi}{2010}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge(\PYGZdl{}m\PYGZca{}3/s\PYGZdl{})}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}\PYG{p}{,} \PYG{n}{ax2}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_varying\_heatmap()}}
\label{\detokenize{A3_plotting_code:plot-varying-heatmap}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}varying\PYGZus{}heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{title}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{14}\PYG{p}{,}\PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot a sensitivity metric overlain by observed flow..}

\PYG{l+s+sd}{    :param arr\PYGZus{}sim:         Numpy array of simulated metrics}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly observed data from sensitivity analysis}

\PYG{l+s+sd}{    :param title:           Title of plot}
\PYG{l+s+sd}{    :type title:            str}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot heatmap}
    \PYG{n}{sns}\PYG{o}{.}\PYG{n}{heatmap}\PYG{p}{(}\PYG{n}{arr\PYGZus{}sim}\PYG{p}{,}
                \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{,}
                \PYG{n}{yticklabels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Kq}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ks}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Alp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Huz}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{B}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{sns}\PYG{o}{.}\PYG{n}{light\PYGZus{}palette}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{seagreen}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{as\PYGZus{}cmap}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)}

    \PYG{n}{n\PYGZus{}years} \PYG{o}{=} \PYG{n}{df\PYGZus{}obs}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} setup overlay axis}
    \PYG{n}{ax2} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{twinx}\PYG{p}{(}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n\PYGZus{}years}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot points on line}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n\PYGZus{}years}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{slateblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up axis lables and limits}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{119.5}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of Months}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{title}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}\PYG{p}{,} \PYG{n}{ax2}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_precalibration\_flow()}}
\label{\detokenize{A3_plotting_code:plot-precalibration-flow}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}precalibration\PYGZus{}flow}\PYG{p}{(}\PYG{n}{df\PYGZus{}sim}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot flow discharge provided by the ensemble of parameters sets from Pre\PYGZhy{}Calibration versus the observed}
\PYG{l+s+sd}{    flow data.}

\PYG{l+s+sd}{    :param df\PYGZus{}sim:          Dataframe of simulated metrics}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly observed data from sensitivity analysis}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set axis labels}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Days}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot pre\PYGZhy{}calibration results}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{df\PYGZus{}sim}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}sim}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}sim}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightgreen}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot observed}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}sim}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed vs. Pre\PYGZhy{}Calibration Outputs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} customize legend}
    \PYG{n}{custom\PYGZus{}lines} \PYG{o}{=} \PYG{p}{[}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightgreen}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{custom\PYGZus{}lines}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pre\PYGZhy{}Calibration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_precalibration\_glue()}}
\label{\detokenize{A3_plotting_code:plot-precalibration-glue}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}precalibration\PYGZus{}glue}\PYG{p}{(}\PYG{n}{df\PYGZus{}precal}\PYG{p}{,} \PYG{n}{df\PYGZus{}glue}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot flow discharge provided by the ensemble of parameters sets from Pre\PYGZhy{}Calibration versus the observed}
\PYG{l+s+sd}{    flow data.}

\PYG{l+s+sd}{    :param df\PYGZus{}sim:          Dataframe of simulated metrics}

\PYG{l+s+sd}{    :param df\PYGZus{}obs:          Dataframe of mean monthly observed data from sensitivity analysis}

\PYG{l+s+sd}{    :param figsize:         Matplotlib figure size}
\PYG{l+s+sd}{    :type figsize:          list}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set up figure}
    \PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set axis labels}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Days}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Flow Discharge}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot pre\PYGZhy{}calibration results}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{df\PYGZus{}precal}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}precal}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}precal}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightgreen}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot glue}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{df\PYGZus{}glue}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}glue}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}glue}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightblue}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot observed}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{df\PYGZus{}precal}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{df\PYGZus{}obs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Strmflw}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed vs. Sensitivity Analysis Outputs across GLUE/Pre\PYGZhy{}Calibration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} customize legend}
    \PYG{n}{custom\PYGZus{}lines} \PYG{o}{=} \PYG{p}{[}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}  \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightgreen}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{lightblue}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{,}
                    \PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{black}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{]}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{custom\PYGZus{}lines}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Pre\PYGZhy{}Calibration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{GLUE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Observed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}
\end{sphinxVerbatim}


\section{fishery\_dynamics.ipynb}
\label{\detokenize{A3_plotting_code:fishery-dynamics-ipynb}}
\sphinxAtStartPar
The following are the plotting functions as described in the \sphinxcode{\sphinxupquote{fishery\_dynamics.ipynb}} Jupyter notebook tutorial.

\sphinxAtStartPar
The following are the necessary package imports to run these functions:

\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{k+kn}{from} \PYG{n+nn}{matplotlib} \PYG{k+kn}{import} \PYG{n}{patheffects} \PYG{k}{as} \PYG{n}{pe}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_objective\_performance()}}
\label{\detokenize{A3_plotting_code:plot-objective-performance}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}objective\PYGZus{}performance}\PYG{p}{(}\PYG{n}{objective\PYGZus{}performance}\PYG{p}{,} \PYG{n}{profit\PYGZus{}solution}\PYG{p}{,} \PYG{n}{robust\PYGZus{}solution}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Plot the identified solutions with regards to their objective performance}
\PYG{l+s+sd}{    in a parallel axis plot}

\PYG{l+s+sd}{    :param objective\PYGZus{}performance:               Objective performance array}
\PYG{l+s+sd}{    :param profit\PYGZus{}solution:                     Profitable solutions array}
\PYG{l+s+sd}{    :param robust\PYGZus{}solution:                     Robust solutions array}
\PYG{l+s+sd}{    :param figsize:                             Figure size}
\PYG{l+s+sd}{    :type figsize:                              tuple}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} create the figure object}
    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{figsize}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up subplot axis object}
    \PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} labels where constraint is always 0}
    \PYG{n}{objs\PYGZus{}labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Net present}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{value (NPV)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Prey population deficit}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Longest duration}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{of low harvest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Worst harvest instance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Variance of harvest}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                   \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Duration of predator}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{population collapse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} normalization across objectives}
    \PYG{n}{mins} \PYG{o}{=} \PYG{n}{objective\PYGZus{}performance}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{maxs} \PYG{o}{=} \PYG{n}{objective\PYGZus{}performance}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{norm\PYGZus{}reference} \PYG{o}{=} \PYG{n}{objective\PYGZus{}performance}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{mm} \PYG{o}{=} \PYG{n}{objective\PYGZus{}performance}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{mx} \PYG{o}{=} \PYG{n}{objective\PYGZus{}performance}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{mm} \PYG{o}{!=} \PYG{n}{mx}\PYG{p}{:}
            \PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{p}{(}\PYG{n}{objective\PYGZus{}performance}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{mm}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{mx} \PYG{o}{\PYGZhy{}} \PYG{n}{mm}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{l+m+mi}{1}

    \PYG{c+c1}{\PYGZsh{} colormap from matplotlib}
    \PYG{n}{cmap} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{get\PYGZus{}cmap}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Blues}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} plot all solutions}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{ys} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{)}
        \PYG{n}{xs} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{,} \PYG{n}{ys}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} to highlight robust solutions}
    \PYG{n}{ys} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{n}{profit\PYGZus{}solution}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Most profitable}
    \PYG{n}{xs} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{l1} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{ys}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{c}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
                 \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Most robust in NPV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                 \PYG{n}{path\PYGZus{}effects}\PYG{o}{=}\PYG{p}{[}\PYG{n}{pe}\PYG{o}{.}\PYG{n}{Stroke}\PYG{p}{(}\PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{foreground}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{darkgoldenrod}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pe}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

    \PYG{n}{ys} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{norm\PYGZus{}reference}\PYG{p}{[}\PYG{n}{robust\PYGZus{}solution}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Most robust in all criteria}
    \PYG{n}{xs} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{l2} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{xs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{ys}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{:}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                 \PYG{n}{c}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{(}\PYG{n}{ys}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,}
                 \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Most robust across criteria}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
                 \PYG{n}{path\PYGZus{}effects}\PYG{o}{=}\PYG{p}{[}\PYG{n}{pe}\PYG{o}{.}\PYG{n}{Stroke}\PYG{p}{(}\PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{foreground}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gold}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{n}{pe}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} build colorbar}
    \PYG{n}{sm} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{ScalarMappable}\PYG{p}{(}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{)}
    \PYG{n}{sm}\PYG{o}{.}\PYG{n}{set\PYGZus{}array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{objective\PYGZus{}performance}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{objective\PYGZus{}performance}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{cbar} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{sm}\PYG{p}{)}
    \PYG{n}{cbar}\PYG{o}{.}\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{Net present value (NPV)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} tick values}
    \PYG{n}{minvalues} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.3f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{mins}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.3f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{mins}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n+nb}{str}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{mins}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.3f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{mins}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{mins}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n+nb}{str}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

    \PYG{n}{maxvalues} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{maxs}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.3f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{maxs}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n+nb}{str}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{maxs}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{n}{maxs}\PYG{p}{[}\PYG{l+m+mi}{3}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+si}{\PYGZob{}0:.2f\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{o}{.}\PYG{n}{format}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{n}{maxs}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
                 \PYG{n+nb}{str}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{]}

    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Preference \PYGZhy{}\PYGZgt{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{n}{minvalues}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n}{objs\PYGZus{}labels}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{objs\PYGZus{}labels}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} make a twin axis for toplabels}
    \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{ax}\PYG{o}{.}\PYG{n}{twiny}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}yticks}\PYG{p}{(}\PYG{p}{[}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{p}{[}\PYG{n}{maxvalues}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{maxs}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax}\PYG{p}{,} \PYG{n}{ax1}
\end{sphinxVerbatim}


\subsection{\sphinxstylestrong{plot\_factor\_performance()}}
\label{\detokenize{A3_plotting_code:plot-factor-performance}}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}factor\PYGZus{}performance}\PYG{p}{(}\PYG{n}{param\PYGZus{}values}\PYG{p}{,} \PYG{n}{collapse\PYGZus{}days}\PYG{p}{,} \PYG{n}{b}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{a}\PYG{p}{)}\PYG{p}{:}
    \PYG{l+s+sd}{\PYGZdq{}\PYGZdq{}\PYGZdq{}Visualize the performance of our policies in three\PYGZhy{}dimensional}
\PYG{l+s+sd}{    parametric space.}

\PYG{l+s+sd}{    :param param\PYGZus{}values:                Saltelli sample array}
\PYG{l+s+sd}{    :param collapse\PYGZus{}days:               Simulation array}
\PYG{l+s+sd}{    :param b:                           b parameter boundary interval}
\PYG{l+s+sd}{    :param m:                           m parameter boundary interval}
\PYG{l+s+sd}{    :param a:                           a parameter boundary interval}

\PYG{l+s+sd}{    \PYGZdq{}\PYGZdq{}\PYGZdq{}}

    \PYG{c+c1}{\PYGZsh{} set colormap}
    \PYG{n}{cmap} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{get\PYGZus{}cmap}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{RdBu\PYGZus{}r}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} build figure object}
    \PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figaspect}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{600}\PYG{p}{,} \PYG{n}{constrained\PYGZus{}layout}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up scalable colormap}
    \PYG{n}{sm} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{cm}\PYG{o}{.}\PYG{n}{ScalarMappable}\PYG{p}{(}\PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up subplot for profit maximizing policy}
    \PYG{n}{ax1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add point data for profit plot}
    \PYG{n}{sows} \PYG{o}{=} \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{c}\PYG{o}{=}\PYG{n}{collapse\PYGZus{}days}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,}
                       \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add surface data for boundary separating successful and failed states of the world}
    \PYG{n}{pts\PYGZus{}ineq} \PYG{o}{=} \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add reference point to plot}
    \PYG{n}{pt\PYGZus{}ref} \PYG{o}{=} \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.005}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up plot aesthetics and labels}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{b}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{a}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{2.0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}view\PYGZus{}interval}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,}  \PYG{l+m+mf}{0.5}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}facecolor}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{white}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{view\PYGZus{}init}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{17}\PYG{p}{)}
    \PYG{n}{ax1}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Profit maximizing policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up subplot for robust policy}
    \PYG{n}{ax2} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add point data for robust plot}
    \PYG{n}{sows} \PYG{o}{=} \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{6}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{param\PYGZus{}values}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{c}\PYG{o}{=}\PYG{n}{collapse\PYGZus{}days}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
                       \PYG{n}{cmap}\PYG{o}{=}\PYG{n}{cmap}\PYG{p}{,}
                       \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add surface data for boundary separating successful and failed states of the world}
    \PYG{n}{pts\PYGZus{}ineq} \PYG{o}{=} \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{b}\PYG{p}{,} \PYG{n}{m}\PYG{p}{,} \PYG{n}{a}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.25}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} add reference point to plot}
    \PYG{n}{pt\PYGZus{}ref} \PYG{o}{=} \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.005}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{zorder}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up plot aesthetics and labels}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{b}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{m}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{a}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{2.0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylim}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{xaxis}\PYG{o}{.}\PYG{n}{set\PYGZus{}view\PYGZus{}interval}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}facecolor}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{white}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{view\PYGZus{}init}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{17}\PYG{p}{)}
    \PYG{n}{ax2}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Robust policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} set up colorbar}
    \PYG{n}{sm}\PYG{o}{.}\PYG{n}{set\PYGZus{}array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{collapse\PYGZus{}days}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{collapse\PYGZus{}days}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{cbar} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{sm}\PYG{p}{)}
    \PYG{n}{cbar}\PYG{o}{.}\PYG{n}{set\PYGZus{}label}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Days with predator collapse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{ax1}\PYG{p}{,} \PYG{n}{ax2}
\end{sphinxVerbatim}

\sphinxAtStartPar


\begin{sphinxthebibliography}{100}
\bibitem[1]{index:id2}
\sphinxAtStartPar
National Research Council and others. \sphinxstyleemphasis{Convergence: Facilitating transdisciplinary integration of life sciences, physical sciences, engineering, and beyond}. National Academies Press, 2014.
\bibitem[2]{index:id3}
\sphinxAtStartPar
Sondoss Elsawah, Tatiana Filatova, Anthony J Jakeman, Albert J Kettner, Moira L Zellner, Ioannis N Athanasiadis, Serena H Hamilton, Robert L Axtell, Daniel G Brown, Jonathan M Gilligan, and others. Eight grand challenges in socio\sphinxhyphen{}environmental systems modeling. \sphinxstyleemphasis{Socio\sphinxhyphen{}Environmental Systems Modelling}, 2:16226\textendash{}16226, 2020.
\bibitem[3]{index:id4}
\sphinxAtStartPar
Yacov Y Haimes. Risk modeling of interdependent complex systems of systems: theory and practice. \sphinxstyleemphasis{Risk analysis}, 38(1):84\textendash{}98, 2018.
\bibitem[4]{index:id5}
\sphinxAtStartPar
Dirk Helbing. Globally networked risks and how to respond. \sphinxstyleemphasis{Nature}, 497(7447):51\textendash{}59, 2013.
\bibitem[5]{index:id6}
\sphinxAtStartPar
Andrea Saltelli, Ksenia Aleksankina, William Becker, Pamela Fennell, Federico Ferretti, Niels Holst, Sushan Li, and Qiongli Wu. Why so many published sensitivity analyses are false: a systematic review of sensitivity analysis practices. \sphinxstyleemphasis{Environmental modelling \& software}, 114:29\textendash{}39, 2019.
\bibitem[6]{index:id7}
\sphinxAtStartPar
Daniel Wirtz and Wolfgang Nowak. The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control. \sphinxstyleemphasis{Environmental Modelling \& Software}, 93:180\textendash{}192, 2017.
\bibitem[7]{index:id11}
\sphinxAtStartPar
Roger Cooke and others. \sphinxstyleemphasis{Experts in uncertainty: opinion and subjective probability in science}. Oxford University Press on Demand, 1991.
\bibitem[8]{index:id12}
\sphinxAtStartPar
Enayat A Moallemi, Jan Kwakkel, Fjalar J de Haan, and Brett A Bryan. Exploratory modeling for analyzing coupled human\sphinxhyphen{}natural systems under uncertainty. \sphinxstyleemphasis{Global Environmental Change}, 65:102186, 2020.
\bibitem[9]{index:id13}
\sphinxAtStartPar
Warren E Walker, Poul Harremoës, Jan Rotmans, Jeroen P Van Der Sluijs, Marjolein BA Van Asselt, Peter Janssen, and Martin P Krayer von Krauss. Defining uncertainty: a conceptual basis for uncertainty management in model\sphinxhyphen{}based decision support. \sphinxstyleemphasis{Integrated assessment}, 4(1):5\textendash{}17, 2003.
\bibitem[10]{index:id8}
\sphinxAtStartPar
Jan H Kwakkel, Warren E Walker, and Marjolijn Haasnoot. Coping with the wickedness of public policy problems: approaches for decision making under deep uncertainty. 2016.
\bibitem[11]{index:id9}
\sphinxAtStartPar
Saul I Gass and Carl M Harris. Encyclopedia of operations research and management science. \sphinxstyleemphasis{Journal of the Operational Research Society}, 48(7):759\textendash{}760, 1997.
\bibitem[12]{index:id10}
\sphinxAtStartPar
Andrea Saltelli, Philip B Stark, William Becker, and Pawel Stano. Climate models as economic guides scientific challenge or quixotic quest? \sphinxstyleemphasis{Issues in Science and Technology}, 31(3):79\textendash{}84, 2015.
\bibitem[13]{index:id46}
\sphinxAtStartPar
Hoshin V. Gupta, Thorsten Wagener, and Yuqiong Liu. Reconciling theory with observations: elements of a diagnostic approach to model evaluation. \sphinxstyleemphasis{Hydrological Processes: An International Journal}, 22(18):3802\textendash{}3813, 2008. Publisher: Wiley Online Library.
\bibitem[14]{index:id47}
\sphinxAtStartPar
Antonia Hadjimichael, Julianne Quinn, and Patrick Reed. Advancing Diagnostic Model Evaluation to Better Understand Water Shortage Mechanisms in Institutionally Complex River Basins. \sphinxstyleemphasis{Water Resources Research}, 56(10):e2020WR028079, 2020. URL: \sphinxurl{http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020WR028079} (visited on 2020\sphinxhyphen{}10\sphinxhyphen{}16), \sphinxhref{https://doi.org/10.1029/2020WR028079}{doi:10.1029/2020WR028079}.
\bibitem[15]{index:id48}
\sphinxAtStartPar
Andrea Saltelli, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. \sphinxstyleemphasis{Global Sensitivity Analysis: The Primer}. Wiley\sphinxhyphen{}Interscience, Chichester, England ; Hoboken, NJ, 1 edition edition, February 2008. ISBN 978\sphinxhyphen{}0\sphinxhyphen{}470\sphinxhyphen{}05997\sphinxhyphen{}5.
\bibitem[16]{index:id54}
\sphinxAtStartPar
Keith Beven. Towards a coherent philosophy for modelling the environment. \sphinxstyleemphasis{Proceedings of the royal society of London. Series A: mathematical, physical and engineering sciences}, 458(2026):2465\textendash{}2484, 2002.
\bibitem[17]{index:id118}
\sphinxAtStartPar
Naomi Oreskes, Kristin Shrader\sphinxhyphen{}Frechette, and Kenneth Belitz. Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences. \sphinxstyleemphasis{Science}, 263(5147):641\textendash{}646, February 1994. URL: \sphinxurl{https://science.sciencemag.org/content/263/5147/641} (visited on 2020\sphinxhyphen{}04\sphinxhyphen{}15), \sphinxhref{https://doi.org/10.1126/science.263.5147.641}{doi:10.1126/science.263.5147.641}.
\bibitem[18]{index:id49}
\sphinxAtStartPar
Keith Beven. Prophecy, reality and uncertainty in distributed hydrological modelling. \sphinxstyleemphasis{Advances in water resources}, 16(1):41\textendash{}51, 1993.
\bibitem[19]{index:id38}
\sphinxAtStartPar
Keith Beven and Andrew Binley. The future of distributed models: Model calibration and uncertainty prediction. \sphinxstyleemphasis{Hydrological Processes}, 6(3):279\textendash{}298, 1992. \sphinxhref{https://doi.org/10.1002/hyp.3360060305}{doi:10.1002/hyp.3360060305}.
\bibitem[20]{index:id119}
\sphinxAtStartPar
Yaman Barlas and Stanley Carpenter. Philosophical roots of model validation: Two paradigms. \sphinxstyleemphasis{System Dynamics Review}, 6(2):148\textendash{}166, 1990. \sphinxhref{https://doi.org/10.1002/sdr.4260060203}{doi:10.1002/sdr.4260060203}.
\bibitem[21]{index:id120}
\sphinxAtStartPar
Stephen Toulmin. From form to function: philosophy and history of science in the 1950s and now. \sphinxstyleemphasis{Daedalus}, pages 143\textendash{}162, 1977. Publisher: JSTOR.
\bibitem[22]{index:id121}
\sphinxAtStartPar
George B. Kleindorfer, Liam O\textquotesingle{}Neill, and Ram Ganeshan. Validation in simulation: Various positions in the philosophy of science. \sphinxstyleemphasis{Management Science}, 44(8):1087\textendash{}1099, 1998. Publisher: INFORMS.
\bibitem[23]{index:id64}
\sphinxAtStartPar
Sibel Eker, Elena Rovenskaya, Michael Obersteiner, and Simon Langan. Practice and perspectives in the validation of resource management models. \sphinxstyleemphasis{Nature communications}, 9(1):1\textendash{}10, 2018.
\bibitem[24]{index:id122}
\sphinxAtStartPar
Yaman Barlas. Formal aspects of model validity and validation in system dynamics. \sphinxstyleemphasis{System Dynamics Review: The Journal of the System Dynamics Society}, 12(3):183\textendash{}210, 1996. Publisher: Wiley Online Library.
\bibitem[25]{index:id123}
\sphinxAtStartPar
Thomas H. Naylor and Joseph Michael Finger. Verification of computer simulation models. \sphinxstyleemphasis{Management science}, 14(2):B\textendash{}92, 1967. Publisher: INFORMS.
\bibitem[26]{index:id52}
\sphinxAtStartPar
Keith J Beven. On hypothesis testing in hydrology: why falsification of models is still a really good idea. \sphinxstyleemphasis{Wiley Interdisciplinary Reviews: Water}, 5(3):e1278, 2018.
\bibitem[27]{index:id51}
\sphinxAtStartPar
Hoshin V Gupta, Martyn P Clark, Jasper A Vrugt, Gab Abramowitz, and Ming Ye. Towards a comprehensive assessment of model structural adequacy. \sphinxstyleemphasis{Water Resources Research}, 2012.
\bibitem[28]{index:id124}
\sphinxAtStartPar
Praveen Kumar. Typology of hydrologic predictability. \sphinxstyleemphasis{Water Resources Research}, 2011. URL: \sphinxurl{https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2010WR009769} (visited on 2020\sphinxhyphen{}04\sphinxhyphen{}15), \sphinxhref{https://doi.org/10.1029/2010WR009769}{doi:10.1029/2010WR009769}.
\bibitem[29]{index:id125}
\sphinxAtStartPar
Grey S. Nearing, Benjamin L. Ruddell, Andrew R. Bennett, Cristina Prieto, and Hoshin V. Gupta. Does Information Theory Provide a New Paradigm for Earth Science? Hypothesis Testing. \sphinxstyleemphasis{Water Resources Research}, 56(2):e2019WR024918, 2020. \sphinxhref{https://doi.org/10.1029/2019WR024918}{doi:10.1029/2019WR024918}.
\bibitem[30]{index:id126}
\sphinxAtStartPar
Hoshin Vijai Gupta, Soroosh Sorooshian, and Patrice Ogou Yapo. Toward improved calibration of hydrologic models: Multiple and noncommensurable measures of information. \sphinxstyleemphasis{Water Resources Research}, 34(4):751\textendash{}763, 1998. URL: \sphinxurl{http://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/97WR03495} (visited on 2020\sphinxhyphen{}04\sphinxhyphen{}07), \sphinxhref{https://doi.org/10.1029/97WR03495}{doi:10.1029/97WR03495}.
\bibitem[31]{index:id168}
\sphinxAtStartPar
Francesca Pianosi and Thorsten Wagener. Understanding the time\sphinxhyphen{}varying importance of different uncertainty sources in hydrological modelling using global sensitivity analysis. \sphinxstyleemphasis{Hydrological Processes}, pages 3991\textendash{}4003, November 2017. URL: \sphinxurl{https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.10968\%4010.1111/\%28ISSN\%291099-1085.Kieth-Beven}, \sphinxhref{https://doi.org/10.1002/hyp.10968@10.1111/(ISSN)1099-1085.Kieth-Beven}{doi:10.1002/hyp.10968@10.1111/(ISSN)1099\sphinxhyphen{}1085.Kieth\sphinxhyphen{}Beven}.
\bibitem[32]{index:id167}
\sphinxAtStartPar
Charles Rougé, Patrick M. Reed, Danielle S. Grogan, Shan Zuidema, Alexander Prusevich, Stanley Glidden, Jonathan R. Lamontagne, and Richard B. Lammers. Coordination and Control: Limits in Standard Representations of Multi\sphinxhyphen{}Reservoir Operations in Hydrological Modeling. \sphinxstyleemphasis{Hydrology and Earth System Sciences Discussions}, pages 1\textendash{}37, November 2019. URL: \sphinxurl{https://www.hydrol-earth-syst-sci-discuss.net/hess-2019-589/}, \sphinxhref{https://doi.org/https://doi.org/10.5194/hess-2019-589}{doi:https://doi.org/10.5194/hess\sphinxhyphen{}2019\sphinxhyphen{}589}.
\bibitem[33]{index:id127}
\sphinxAtStartPar
David W. Cash, William C. Clark, Frank Alcock, Nancy M. Dickson, Noelle Eckley, David H. Guston, Jill Jäger, and Ronald B. Mitchell. Knowledge systems for sustainable development. \sphinxstyleemphasis{Proceedings of the national academy of sciences}, 100(14):8086\textendash{}8091, 2003. Publisher: National Acad Sciences.
\bibitem[34]{index:id128}
\sphinxAtStartPar
Dave D. White, Amber Wutich, Kelli L. Larson, Patricia Gober, Timothy Lant, and Clea Senneville. Credibility, salience, and legitimacy of boundary objects: water managers\textquotesingle{} assessment of a simulation model in an immersive decision theater. \sphinxstyleemphasis{Science and Public Policy}, 37(3):219\textendash{}232, April 2010. Publisher: Oxford Academic. URL: \sphinxurl{https://academic.oup.com/spp/article/37/3/219/1626552} (visited on 2020\sphinxhyphen{}05\sphinxhyphen{}12), \sphinxhref{https://doi.org/10.3152/030234210X497726}{doi:10.3152/030234210X497726}.
\bibitem[35]{index:id129}
\sphinxAtStartPar
Andrea Saltelli and Silvio Funtowicz. When all models are wrong. \sphinxstyleemphasis{Issues in Science and Technology}, 30(2):79\textendash{}85, 2014. Publisher: JSTOR.
\bibitem[36]{index:id15}
\sphinxAtStartPar
Steve Bankes. Exploratory Modeling for Policy Analysis. \sphinxstyleemphasis{Operations Research}, 41(3):435\textendash{}449, June 1993. URL: \sphinxurl{https://pubsonline.informs.org/doi/abs/10.1287/opre.41.3.435} (visited on 2018\sphinxhyphen{}09\sphinxhyphen{}11), \sphinxhref{https://doi.org/10.1287/opre.41.3.435}{doi:10.1287/opre.41.3.435}.
\bibitem[37]{index:id130}
\sphinxAtStartPar
Christopher P. Weaver, Robert J. Lempert, Casey Brown, John A. Hall, David Revell, and Daniel Sarewitz. Improving the contribution of climate model information to decision making: the value and demands of robust decision frameworks. \sphinxstyleemphasis{Wiley Interdisciplinary Reviews: Climate Change}, 4(1):39\textendash{}60, 2013. URL: \sphinxurl{https://onlinelibrary.wiley.com/doi/abs/10.1002/wcc.202} (visited on 2019\sphinxhyphen{}10\sphinxhyphen{}01), \sphinxhref{https://doi.org/10.1002/wcc.202}{doi:10.1002/wcc.202}.
\bibitem[38]{index:id17}
\sphinxAtStartPar
Andrea Saltelli, Stefano Tarantola, Francesca Campolongo, and Marco Ratto. \sphinxstyleemphasis{Sensitivity analysis in practice: a guide to assessing scientific models}. Volume 1. Wiley Online Library, 2004.
\bibitem[39]{index:id22}
\sphinxAtStartPar
Emanuele Borgonovo and Elmar Plischke. Sensitivity analysis: a review of recent advances. \sphinxstyleemphasis{European Journal of Operational Research}, 248(3):869\textendash{}887, 2016.
\bibitem[40]{index:id18}
\sphinxAtStartPar
O Rakovec, Mary C Hill, MP Clark, AH Weerts, AJ Teuling, and R Uijlenhoet. Distributed evaluation of local sensitivity analysis (delsa), with application to hydrologic models. \sphinxstyleemphasis{Water Resources Research}, 50(1):409\textendash{}426, 2014.
\bibitem[41]{index:id19}
\sphinxAtStartPar
Andrea Saltelli and Paola Annoni. How to avoid a perfunctory sensitivity analysis. \sphinxstyleemphasis{Environmental Modelling \& Software}, 25(12):1508\textendash{}1517, 2010.
\bibitem[42]{index:id20}
\sphinxAtStartPar
Yong Tang, Patrick Reed, Thibaut Wagener, and K van Werkhoven. Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation. \sphinxstyleemphasis{Hydrology and Earth System Sciences}, 11(2):793\textendash{}817, 2007.
\bibitem[43]{index:id23}
\sphinxAtStartPar
Nicholas AS Hamm, Jim W Hall, and MG Anderson. Variance\sphinxhyphen{}based sensitivity analysis of the probability of hydrologically induced slope instability. \sphinxstyleemphasis{Computers \& geosciences}, 32(6):803\textendash{}817, 2006.
\bibitem[44]{index:id24}
\sphinxAtStartPar
Andrea Saltelli, Ksenia Aleksankina, William Becker, Pamela Fennell, Federico Ferretti, Niels Holst, Sushan Li, and Qiongli Wu. Why so many published sensitivity analyses are false: a systematic review of sensitivity analysis practices. \sphinxstyleemphasis{Environmental modelling \& software}, 114:29\textendash{}39, 2019.
\bibitem[45]{index:id16}
\sphinxAtStartPar
Benjamin P Bryant and Robert J Lempert. Thinking inside the box: a participatory, computer\sphinxhyphen{}assisted approach to scenario discovery. \sphinxstyleemphasis{Technological Forecasting and Social Change}, 77(1):34\textendash{}49, 2010.
\bibitem[46]{index:id25}
\sphinxAtStartPar
Andrea Saltelli and Stefano Tarantola. On the relative importance of input factors in mathematical models: safety assessment for nuclear waste disposal. \sphinxstyleemphasis{Journal of the American Statistical Association}, 97(459):702\textendash{}709, 2002.
\bibitem[47]{index:id26}
\sphinxAtStartPar
Barry Anderson, Emanuele Borgonovo, Marzio Galeotti, and Roberto Roson. Uncertainty in climate change modeling: can global sensitivity analysis be of help? \sphinxstyleemphasis{Risk analysis}, 34(2):271\textendash{}293, 2014.
\bibitem[48]{index:id27}
\sphinxAtStartPar
Emanuele Borgonovo. Sensitivity analysis with finite changes: an application to modified eoq models. \sphinxstyleemphasis{European Journal of Operational Research}, 200(1):127\textendash{}138, 2010.
\bibitem[49]{index:id28}
\sphinxAtStartPar
Andrea Saltelli, Marco Ratto, Terry Andres, Francesca Campolongo, Jessica Cariboni, Debora Gatelli, Michaela Saisana, and Stefano Tarantola. \sphinxstyleemphasis{Global sensitivity analysis: the primer}. John Wiley \& Sons, 2008.
\bibitem[50]{index:id29}
\sphinxAtStartPar
Neil R Edwards, David Cameron, and Jonathan Rougier. Precalibrating an intermediate complexity climate model. \sphinxstyleemphasis{Climate dynamics}, 37(7):1469\textendash{}1482, 2011.
\bibitem[51]{index:id30}
\sphinxAtStartPar
Francesca Pianosi, Keith Beven, Jim Freer, Jim W Hall, Jonathan Rougier, David B Stephenson, and Thorsten Wagener. Sensitivity analysis of environmental models: a systematic review with practical workflow. \sphinxstyleemphasis{Environmental Modelling \& Software}, 79:214\textendash{}232, 2016.
\bibitem[52]{index:id31}
\sphinxAtStartPar
RC Spear and GM Hornberger. Eutrophication in peel inlet—ii. identification of critical uncertainties via generalized sensitivity analysis. \sphinxstyleemphasis{Water research}, 14(1):43\textendash{}49, 1980.
\bibitem[53]{index:id111}
\sphinxAtStartPar
Jon C Helton, Jay Dean Johnson, Cedric J Sallaberry, and Curt B Storlie. Survey of sampling\sphinxhyphen{}based methods for uncertainty and sensitivity analysis. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 91(10\sphinxhyphen{}11):1175\textendash{}1209, 2006.
\bibitem[54]{index:id112}
\sphinxAtStartPar
Ronald Aylmer Fisher. Design of experiments. \sphinxstyleemphasis{Br Med J}, 1(3923):554\textendash{}554, 1936.
\bibitem[55]{index:id53}
\sphinxAtStartPar
JD Herman, PM Reed, and T Wagener. Time\sphinxhyphen{}varying sensitivity analysis clarifies the effects of watershed model formulation on model behavior. \sphinxstyleemphasis{Water Resources Research}, 49(3):1400\textendash{}1414, 2013.
\bibitem[56]{index:id113}
\sphinxAtStartPar
Carolina Massmann, Thorsten Wagener, and Hubert Holzmann. A new approach to visualizing time\sphinxhyphen{}varying sensitivity indices for environmental model diagnostics across evaluation time\sphinxhyphen{}scales. \sphinxstyleemphasis{Environmental modelling \& software}, 51:190\textendash{}194, 2014.
\bibitem[57]{index:id114}
\sphinxAtStartPar
An Van Schepdael, Aurélie Carlier, and Liesbet Geris. Sensitivity analysis by design of experiments. In \sphinxstyleemphasis{Uncertainty in Biology}, pages 327\textendash{}366. Springer, 2016.
\bibitem[58]{index:id115}
\sphinxAtStartPar
Nicholas Metropolis and Stanislaw Ulam. The monte carlo method. \sphinxstyleemphasis{Journal of the American statistical association}, 44(247):335\textendash{}341, 1949.
\bibitem[59]{index:id116}
\sphinxAtStartPar
John Norton. An introduction to sensitivity assessment of simulation models. \sphinxstyleemphasis{Environmental Modelling \& Software}, 69:166\textendash{}174, 2015.
\bibitem[60]{index:id32}
\sphinxAtStartPar
Douglas C Montgomery. \sphinxstyleemphasis{Design and analysis of experiments}. John wiley \& sons, 2017.
\bibitem[61]{index:id34}
\sphinxAtStartPar
George EP Box and J Stuart Hunter. The 2 k—p fractional factorial designs. \sphinxstyleemphasis{Technometrics}, 3(3):311\textendash{}351, 1961.
\bibitem[62]{index:id33}
\sphinxAtStartPar
Izabella Surowiec, Ludvig Vikstrom, Gustaf Hector, Erik Johansson, Conny Vikstrom, and Johan Trygg. Generalized subset designs in analytical chemistry. \sphinxstyleemphasis{Analytical chemistry}, 89(12):6491\textendash{}6497, 2017.
\bibitem[63]{index:id91}
\sphinxAtStartPar
Michael D McKay, Richard J Beckman, and William J Conover. A comparison of three methods for selecting values of input variables in the analysis of output from a computer code. \sphinxstyleemphasis{Technometrics}, 21(1):239\textendash{}2451, 1979.
\bibitem[64]{index:id21}
\sphinxAtStartPar
Boxin Tang. Orthogonal array\sphinxhyphen{}based latin hypercubes. \sphinxstyleemphasis{Journal of the American statistical association}, 88(424):1392\textendash{}1397, 1993.
\bibitem[65]{index:id81}
\sphinxAtStartPar
Ishaan L Dalal, Deian Stefan, and Jared Harwayne\sphinxhyphen{}Gidansky. Low discrepancy sequences for monte carlo simulations on reconfigurable platforms. In \sphinxstyleemphasis{2008 International Conference on Application\sphinxhyphen{}Specific Systems, Architectures and Processors}, 108\textendash{}113. IEEE, 2008.
\bibitem[66]{index:id92}
\sphinxAtStartPar
SK Zaremba. The mathematical basis of monte carlo and quasi\sphinxhyphen{}monte carlo methods. \sphinxstyleemphasis{SIAM review}, 10(3):303\textendash{}314, 1968.
\bibitem[67]{index:id95}
\sphinxAtStartPar
Sergei Kucherenko, Daniel Albrecht, and Andrea Saltelli. Exploring multi\sphinxhyphen{}dimensional spaces: a comparison of latin hypercube and quasi monte carlo sampling techniques. \sphinxstyleemphasis{arXiv preprint arXiv:1505.02350}, 2015.
\bibitem[68]{index:id93}
\sphinxAtStartPar
Bertrand Iooss, Loïc Boussouf, Vincent Feuillard, and Amandine Marrel. Numerical studies of the metamodel fitting and validation processes. \sphinxstyleemphasis{arXiv preprint arXiv:1001.1049}, 2010.
\bibitem[69]{index:id94}
\sphinxAtStartPar
Ruichen Jin, Wei Chen, and Agus Sudjianto. An efficient algorithm for constructing optimal design of computer experiments. In \sphinxstyleemphasis{International Design Engineering Technical Conferences and Computers and Information in Engineering Conference}, volume 37009, 545\textendash{}554. 2003.
\bibitem[70]{index:id96}
\sphinxAtStartPar
Max D Morris and Toby J Mitchell. Exploratory designs for computational experiments. \sphinxstyleemphasis{Journal of statistical planning and inference}, 43(3):381\textendash{}402, 1995.
\bibitem[71]{index:id97}
\sphinxAtStartPar
Jeong\sphinxhyphen{}Soo Park. Optimal latin\sphinxhyphen{}hypercube designs for computer experiments. \sphinxstyleemphasis{Journal of statistical planning and inference}, 39(1):95\textendash{}111, 1994.
\bibitem[72]{index:id98}
\sphinxAtStartPar
Ilya M Sobol. Uniformly distributed sequences with an additional uniform property. \sphinxstyleemphasis{USSR Computational Mathematics and Mathematical Physics}, 16(5):236\textendash{}242, 1976.
\bibitem[73]{index:id99}
\sphinxAtStartPar
Il\textquotesingle{}ya Meerovich Sobol\textquotesingle{}. On the distribution of points in a cube and the approximate evaluation of integrals. \sphinxstyleemphasis{Zhurnal Vychislitel\textquotesingle{}noi Matematiki i Matematicheskoi Fiziki}, 7(4):784\textendash{}802, 1967.
\bibitem[74]{index:id132}
\sphinxAtStartPar
Max D Morris. Factorial sampling plans for preliminary computational experiments. \sphinxstyleemphasis{Technometrics}, 33(2):161\textendash{}174, 1991.
\bibitem[75]{index:id100}
\sphinxAtStartPar
RI Cukier, CM Fortuin, Kurt E Shuler, AG Petschek, and JH Schaibly. Study of the sensitivity of coupled reaction systems to uncertainties in rate coefficients. i theory. \sphinxstyleemphasis{The Journal of chemical physics}, 59(8):3873\textendash{}3878, 1973.
\bibitem[76]{index:id101}
\sphinxAtStartPar
Andrea Saltelli, Stefano Tarantola, and KP\sphinxhyphen{}S Chan. A quantitative model\sphinxhyphen{}independent method for global sensitivity analysis of model output. \sphinxstyleemphasis{Technometrics}, 41(1):39\textendash{}56, 1999.
\bibitem[77]{index:id102}
\sphinxAtStartPar
Ilya M Sobol. Global sensitivity indices for nonlinear mathematical models and their monte carlo estimates. \sphinxstyleemphasis{Mathematics and computers in simulation}, 55(1\sphinxhyphen{}3):271\textendash{}280, 2001.
\bibitem[78]{index:id103}
\sphinxAtStartPar
Jonathan D Herman, Harrison B Zeff, Jonathan R Lamontagne, Patrick M Reed, and Gregory W Characklis. Synthetic drought scenario generation to support bottom\sphinxhyphen{}up water supply vulnerability assessments. \sphinxstyleemphasis{Journal of Water Resources Planning and Management}, 142(11):04016050, 2016.
\bibitem[79]{index:id104}
\sphinxAtStartPar
PCD Milly, Julio Betancourt, Malin Falkenmark, Robert M Hirsch, Zbigniew W Kundzewicz, Dennis P Lettenmaier, and Ronald J Stouffer. Stationarity is dead: whither water management? \sphinxstyleemphasis{Earth}, 4:20, 2008.
\bibitem[80]{index:id105}
\sphinxAtStartPar
Edoardo Borgomeo, Christopher L Farmer, and Jim W Hall. Numerical rivers: a synthetic streamflow generator for water resources vulnerability assessments. \sphinxstyleemphasis{Water Resources Research}, 51(7):5382\textendash{}5405, 2015.
\bibitem[81]{index:id74}
\sphinxAtStartPar
Manuel Herrera, Sukumar Natarajan, David A Coley, Tristan Kershaw, Alfonso P Ramallo\sphinxhyphen{}González, Matthew Eames, Daniel Fosas, and Michael Wood. A review of current and future weather data for building simulation. \sphinxstyleemphasis{Building Services Engineering Research and Technology}, 38(5):602\textendash{}627, 2017.
\bibitem[82]{index:id75}
\sphinxAtStartPar
Daniel S Wilks and Robert L Wilby. The weather generation game: a review of stochastic weather models. \sphinxstyleemphasis{Progress in physical geography}, 23(3):329\textendash{}357, 1999.
\bibitem[83]{index:id76}
\sphinxAtStartPar
JR Lamontagne and JR Stedinger. Generating synthetic streamflow forecasts with specified precision. \sphinxstyleemphasis{Journal of Water Resources Planning and Management}, 144(4):04018007, 2018.
\bibitem[84]{index:id77}
\sphinxAtStartPar
Sanghamitra Medda and Kalyan Kumar Bhar. Comparison of single\sphinxhyphen{}site and multi\sphinxhyphen{}site stochastic models for streamflow generation. \sphinxstyleemphasis{Applied Water Science}, 9(3):67, 2019.
\bibitem[85]{index:id106}
\sphinxAtStartPar
Brian R Kirsch, Gregory W Characklis, and Harrison B Zeff. Evaluating the impact of alternative hydro\sphinxhyphen{}climate scenarios on transfer agreements: practical improvement for generating synthetic streamflows. \sphinxstyleemphasis{Journal of Water Resources Planning and Management}, 139(4):396\textendash{}406, 2013.
\bibitem[86]{index:id107}
\sphinxAtStartPar
Daniel P Loucks and Eelco Van Beek. \sphinxstyleemphasis{Water resource systems planning and management: An introduction to methods, models, and applications}. Springer, 2017.
\bibitem[87]{index:id108}
\sphinxAtStartPar
Scott Steinschneider, Sungwook Wi, and Casey Brown. The integrated effects of climate and hydrologic uncertainty on future flood risk assessments. \sphinxstyleemphasis{Hydrological Processes}, 29(12):2823\textendash{}2839, 2015.
\bibitem[88]{index:id109}
\sphinxAtStartPar
Richard M Vogel. Stochastic watershed models for hydrologic risk management. \sphinxstyleemphasis{Water Security}, 1:28\textendash{}35, 2017.
\bibitem[89]{index:id110}
\sphinxAtStartPar
Richard M Vogel and Jery R Stedinger. The value of stochastic streamflow models in overyear reservoir design applications. \sphinxstyleemphasis{Water Resources Research}, 24(9):1483\textendash{}1490, 1988.
\bibitem[90]{index:id117}
\sphinxAtStartPar
Emanuele Borgonovo. Sensitivity analysis of model output with input constraints: a generalized rationale for local methods. \sphinxstyleemphasis{Risk Analysis: An International Journal}, 28(3):667\textendash{}680, 2008.
\bibitem[91]{index:id131}
\sphinxAtStartPar
Bertrand Iooss and Paul Lemaître. A review on global sensitivity analysis methods. In \sphinxstyleemphasis{Uncertainty management in simulation\sphinxhyphen{}optimization of complex systems}, pages 101\textendash{}122. Springer, 2015.
\bibitem[92]{index:id133}
\sphinxAtStartPar
Francesca Campolongo and Roger Braddock. The use of graph theory in the sensitivity analysis of the model output: a second order screening method. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 64(1):1\textendash{}12, 1999.
\bibitem[93]{index:id134}
\sphinxAtStartPar
Roger A Cropp and Roger D Braddock. The new morris method: an efficient second\sphinxhyphen{}order screening method. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 78(1):77\textendash{}83, 2002.
\bibitem[94]{index:id135}
\sphinxAtStartPar
Jon C Helton. Uncertainty and sensitivity analysis techniques for use in performance assessment for radioactive waste disposal. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 42(2\sphinxhyphen{}3):327\textendash{}367, 1993.
\bibitem[95]{index:id136}
\sphinxAtStartPar
Gemma Manache and Charles S Melching. Identification of reliable regression\sphinxhyphen{}and correlation\sphinxhyphen{}based sensitivity measures for importance ranking of water\sphinxhyphen{}quality model parameters. \sphinxstyleemphasis{Environmental Modelling \& Software}, 23(5):549\textendash{}562, 2008.
\bibitem[96]{index:id137}
\sphinxAtStartPar
F Pappenberger and Keith J Beven. Ignorance is bliss: or seven reasons not to use uncertainty analysis. \sphinxstyleemphasis{Water resources research}, 2006.
\bibitem[97]{index:id79}
\sphinxAtStartPar
Jerome H Friedman and Nicholas I Fisher. Bump hunting in high\sphinxhyphen{}dimensional data. \sphinxstyleemphasis{Statistics and Computing}, 9(2):123\textendash{}143, 1999.
\bibitem[98]{index:id80}
\sphinxAtStartPar
Leo Breiman, Jerome Friedman, Charles J Stone, and Richard A Olshen. \sphinxstyleemphasis{Classification and regression trees}. CRC press, 1984.
\bibitem[99]{index:id89}
\sphinxAtStartPar
Yoav Freund, Robert Schapire, and Naoki Abe. A short introduction to boosting. \sphinxstyleemphasis{Journal\sphinxhyphen{}Japanese Society For Artificial Intelligence}, 14(771\sphinxhyphen{}780):1612, 1999.
\bibitem[100]{index:id138}
\sphinxAtStartPar
Leo Breiman. Bagging predictors. \sphinxstyleemphasis{Machine learning}, 24(2):123\textendash{}140, 1996.
\bibitem[101]{index:id157}
\sphinxAtStartPar
George M Hornberger and Robert C Spear. Approach to the preliminary analysis of environmental systems. \sphinxstyleemphasis{J. Environ. Mgmt.}, 12(1):7\textendash{}18, 1981.
\bibitem[102]{index:id71}
\sphinxAtStartPar
Robert J Lempert, David G Groves, Steven W Popper, and Steve C Bankes. A general, analytic method for generating robust strategies and narrative scenarios. \sphinxstyleemphasis{Management science}, 52(4):514\textendash{}528, 2006.
\bibitem[103]{index:id70}
\sphinxAtStartPar
David G Groves and Robert J Lempert. A new analytic method for finding policy\sphinxhyphen{}relevant scenarios. \sphinxstyleemphasis{Global Environmental Change}, 17(1):73\textendash{}85, 2007.
\bibitem[104]{index:id158}
\sphinxAtStartPar
Keith Beven and Andrew Binley. Glue: 20 years on. \sphinxstyleemphasis{Hydrological processes}, 28(24):5897\textendash{}5918, 2014.
\bibitem[105]{index:id159}
\sphinxAtStartPar
Roberta\sphinxhyphen{}Serena Blasone, Jasper A Vrugt, Henrik Madsen, Dan Rosbjerg, Bruce A Robinson, and George A Zyvoloski. Generalized likelihood uncertainty estimation (glue) using adaptive markov chain monte carlo sampling. \sphinxstyleemphasis{Advances in Water Resources}, 31(4):630\textendash{}648, 2008.
\bibitem[106]{index:id160}
\sphinxAtStartPar
SA Cryer and PL Havens. Regional sensitivity analysis using a fractional factorial method for the usda model gleams. \sphinxstyleemphasis{Environmental modelling \& software}, 14(6):613\textendash{}624, 1999.
\bibitem[107]{index:id161}
\sphinxAtStartPar
Pengfei Wei, Zhenzhou Lu, and Xiukai Yuan. Monte carlo simulation for moment\sphinxhyphen{}independent sensitivity analysis. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 110:60\textendash{}67, 2013.
\bibitem[108]{index:id162}
\sphinxAtStartPar
Peter Young. Data\sphinxhyphen{}based mechanistic modelling, generalised sensitivity and dominant mode analysis. \sphinxstyleemphasis{Computer Physics Communications}, 117(1\sphinxhyphen{}2):113\textendash{}129, 1999.
\bibitem[109]{index:id148}
\sphinxAtStartPar
Andrea Saltelli. Making best use of model evaluations to compute sensitivity indices. \sphinxstyleemphasis{Computer physics communications}, 145(2):280\textendash{}297, 2002.
\bibitem[110]{index:id149}
\sphinxAtStartPar
Andrea Saltelli. Sensitivity analysis for importance assessment. \sphinxstyleemphasis{Risk analysis}, 22(3):579\textendash{}590, 2002.
\bibitem[111]{index:id150}
\sphinxAtStartPar
Toshimitsu Homma and Andrea Saltelli. Importance measures in global sensitivity analysis of nonlinear models. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 52(1):1\textendash{}17, 1996.
\bibitem[112]{index:id151}
\sphinxAtStartPar
Gregory J McRae, William R Goodin, and John H Seinfeld. Development of a second\sphinxhyphen{}generation mathematical model for urban air pollution—i. model formulation. \sphinxstyleemphasis{Atmospheric Environment (1967)}, 16(4):679\textendash{}696, 1982.
\bibitem[113]{index:id152}
\sphinxAtStartPar
Andrea Saltelli and Ricardo Bolado. An alternative way to compute fourier amplitude sensitivity test (fast). \sphinxstyleemphasis{Computational Statistics \& Data Analysis}, 26(4):445\textendash{}460, 1998.
\bibitem[114]{index:id153}
\sphinxAtStartPar
MA Vazquez\sphinxhyphen{}Cruz, R Guzman\sphinxhyphen{}Cruz, IL Lopez\sphinxhyphen{}Cruz, O Cornejo\sphinxhyphen{}Perez, I Torres\sphinxhyphen{}Pacheco, and RG Guevara\sphinxhyphen{}Gonzalez. Global sensitivity analysis by means of efast and sobol’methods and calibration of reduced state\sphinxhyphen{}variable tomgro model using genetic algorithms. \sphinxstyleemphasis{Computers and Electronics in Agriculture}, 100:1\textendash{}12, 2014.
\bibitem[115]{index:id154}
\sphinxAtStartPar
Benjamin Auder and Bertrand Iooss. Global sensitivity analysis based on entropy. In \sphinxstyleemphasis{Safety, reliability and risk analysis\sphinxhyphen{}Proceedings of the ESREL 2008 Conference}, 2107\textendash{}2115. 2008.
\bibitem[116]{index:id155}
\sphinxAtStartPar
Farkhondeh Khorashadi Zadeh, Jiri Nossent, Fanny Sarrazin, Francesca Pianosi, Ann van Griensven, Thorsten Wagener, and Willy Bauwens. Comparison of variance\sphinxhyphen{}based and moment\sphinxhyphen{}independent global sensitivity analysis approaches by application to the swat model. \sphinxstyleemphasis{Environmental Modelling \& Software}, 91:210\textendash{}222, 2017.
\bibitem[117]{index:id156}
\sphinxAtStartPar
Francesca Pianosi and Thorsten Wagener. A simple and efficient method for global sensitivity analysis based on cumulative distribution functions. \sphinxstyleemphasis{Environmental Modelling \& Software}, 67:1\textendash{}11, 2015.
\bibitem[118]{index:id144}
\sphinxAtStartPar
Ronald Aylmer Fisher and others. Statistical methods for research workers. \sphinxstyleemphasis{Statistical methods for research workers.}, 1934.
\bibitem[119]{index:id145}
\sphinxAtStartPar
Loıc Brevault, Mathieu Balesdent, Nicolas Bérend, and Rodolphe Le Riche. Comparison of different global sensitivity analysis methods for aerospace vehicle optimal design. In \sphinxstyleemphasis{10th World Congress on Structural and Multidisciplinary Optimization, WCSMO\sphinxhyphen{}10}. 2013.
\bibitem[120]{index:id146}
\sphinxAtStartPar
GEB Archer, Andrea Saltelli, and IM Sobol. Sensitivity measures, anova\sphinxhyphen{}like techniques and the use of bootstrap. \sphinxstyleemphasis{Journal of Statistical Computation and Simulation}, 58(2):99\textendash{}120, 1997.
\bibitem[121]{index:id147}
\sphinxAtStartPar
Art B Owen. Variance components and generalized sobol\textquotesingle{}indices. \sphinxstyleemphasis{SIAM/ASA Journal on Uncertainty Quantification}, 1(1):19\textendash{}41, 2013.
\bibitem[122]{index:id142}
\sphinxAtStartPar
Emanuele Borgonovo. Measuring uncertainty importance: investigation and comparison of alternative approaches. \sphinxstyleemphasis{Risk analysis}, 26(5):1349\textendash{}1361, 2006.
\bibitem[123]{index:id141}
\sphinxAtStartPar
Emanuele Borgonovo. A new uncertainty importance measure. \sphinxstyleemphasis{Reliability Engineering \& System Safety}, 92(6):771\textendash{}784, 2007.
\bibitem[124]{index:id143}
\sphinxAtStartPar
Elmar Plischke, Emanuele Borgonovo, and Curtis L Smith. Global sensitivity measures from given data. \sphinxstyleemphasis{European Journal of Operational Research}, 226(3):536\textendash{}550, 2013.
\bibitem[125]{index:id140}
\sphinxAtStartPar
Jiri Nossent, Pieter Elsen, and Willy Bauwens. Sobol’ sensitivity analysis of a complex environmental model. \sphinxstyleemphasis{Environmental Modelling \& Software}, 26(12):1515\textendash{}1525, 2011. URL: \sphinxurl{https://www.sciencedirect.com/science/article/pii/S1364815211001939}, \sphinxhref{https://doi.org/https://doi.org/10.1016/j.envsoft.2011.08.010}{doi:https://doi.org/10.1016/j.envsoft.2011.08.010}.
\bibitem[126]{index:id139}
\sphinxAtStartPar
Jon Herman and Will Usher. Salib: an open\sphinxhyphen{}source python library for sensitivity analysis. \sphinxstyleemphasis{Journal of Open Source Software}, 2(9):97, 2017.
\bibitem[127]{index:id50}
\sphinxAtStartPar
Hoshin V Gupta, Thorsten Wagener, and Yuqiong Liu. Reconciling theory with observations: elements of a diagnostic approach to model evaluation. \sphinxstyleemphasis{Hydrological Processes: An International Journal}, 22(18):3802\textendash{}3813, 2008.
\bibitem[128]{index:id56}
\sphinxAtStartPar
Keith Beven and Jim Freer. Equifinality, data assimilation, and uncertainty estimation in mechanistic modelling of complex environmental systems using the glue methodology. \sphinxstyleemphasis{Journal of hydrology}, 249(1\sphinxhyphen{}4):11\textendash{}29, 2001.
\bibitem[129]{index:id55}
\sphinxAtStartPar
Cameron McPhail, HR Maier, JH Kwakkel, M Giuliani, A Castelletti, and S Westra. Robustness metrics: how are they calculated, when should they be used and why do they give different results? \sphinxstyleemphasis{Earth\textquotesingle{}s Future}, 6(2):169\textendash{}191, 2018.
\bibitem[130]{index:id57}
\sphinxAtStartPar
John D Sterman. System dynamics modeling: tools for learning in a complex world. \sphinxstyleemphasis{California management review}, 43(4):8\textendash{}25, 2001.
\bibitem[131]{index:id59}
\sphinxAtStartPar
John M Anderies, Jean\sphinxhyphen{}Denis Mathias, and Marco A Janssen. Knowledge infrastructure and safe operating spaces in social\textendash{}ecological systems. \sphinxstyleemphasis{Proceedings of the National Academy of Sciences}, 116(12):5277\textendash{}5284, 2019.
\bibitem[132]{index:id58}
\sphinxAtStartPar
Rachata Muneepeerakul and John M Anderies. The emergence and resilience of self\sphinxhyphen{}organized governance in coupled infrastructure systems. \sphinxstyleemphasis{Proceedings of the National Academy of Sciences}, 117(9):4617\textendash{}4622, 2020.
\bibitem[133]{index:id60}
\sphinxAtStartPar
Antonia Hadjimichael, Patrick M Reed, and Julianne D Quinn. Navigating deeply uncertain tradeoffs in harvested predator\sphinxhyphen{}prey systems. \sphinxstyleemphasis{Complexity}, 2020.
\bibitem[134]{index:id61}
\sphinxAtStartPar
Julianne D Quinn, Patrick M Reed, and Klaus Keller. Direct policy search for robust multi\sphinxhyphen{}objective management of deeply uncertain socio\sphinxhyphen{}ecological tipping points. \sphinxstyleemphasis{Environmental modelling \& software}, 92:125\textendash{}141, 2017.
\bibitem[135]{index:id163}
\sphinxAtStartPar
S. R. Carpenter, D. Ludwig, and W. A. Brock. Management of Eutrophication for Lakes Subject to Potentially Irreversible Change. \sphinxstyleemphasis{Ecological Applications}, 9(3):751\textendash{}771, August 1999. URL: \sphinxurl{http://onlinelibrary.wiley.com/doi/10.1890/1051-0761(1999)009{[}0751:MOEFLS{]}2.0.CO;2/abstract}, \sphinxhref{https://doi.org/10.1890/1051-0761(1999)009{[}0751:MOEFLS{]}2.0.CO;2}{doi:10.1890/1051\sphinxhyphen{}0761(1999)009{[}0751:MOEFLS{]}2.0.CO;2}.
\bibitem[136]{index:id164}
\sphinxAtStartPar
Julianne Quinn. Julianneq/Lake\_problem\_dps. December 2017. original\sphinxhyphen{}date: 2017\sphinxhyphen{}02\sphinxhyphen{}06T18:33:54Z. URL: \sphinxurl{https://github.com/julianneq/Lake\_Problem\_DPS} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[137]{index:id165}
\sphinxAtStartPar
David Hadka. Project\sphinxhyphen{}Platypus/Rhodium. 2017. original\sphinxhyphen{}date: 2015\sphinxhyphen{}10\sphinxhyphen{}29T18:08:43Z. URL: \sphinxurl{https://github.com/Project-Platypus/Rhodium} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[138]{index:id166}
\sphinxAtStartPar
Stephen R. Carpenter, William A. Brock, Carl Folke, Egbert H. van Nes, and Marten Scheffer. Allowing variance may enlarge the safe operating space for exploited ecosystems. \sphinxstyleemphasis{Proceedings of the National Academy of Sciences}, 112(46):14384\textendash{}14389, November 2015. URL: \sphinxurl{http://www.pnas.org/content/112/46/14384} (visited on 2017\sphinxhyphen{}08\sphinxhyphen{}18), \sphinxhref{https://doi.org/10.1073/pnas.1511804112}{doi:10.1073/pnas.1511804112}.
\bibitem[139]{index:id63}
\sphinxAtStartPar
Mustafa Hekimoğlu and Yaman Barlas. Sensitivity analysis for models with multiple behavior modes: a method based on behavior pattern measures. \sphinxstyleemphasis{System Dynamics Review}, 32(3\sphinxhyphen{}4):332\textendash{}362, 2016.
\bibitem[140]{index:id65}
\sphinxAtStartPar
Patrick Steinmann, Willem L Auping, and Jan H Kwakkel. Behavior\sphinxhyphen{}based scenario discovery using time series clustering. \sphinxstyleemphasis{Technological Forecasting and Social Change}, 156:120052, 2020.
\bibitem[141]{index:id67}
\sphinxAtStartPar
Steven C Bankes, Robert J Lempert, and Steven W Popper. Computer\sphinxhyphen{}assisted reasoning. \sphinxstyleemphasis{Computing in Science \& Engineering}, 3(2):71\textendash{}77, 2001.
\bibitem[142]{index:id66}
\sphinxAtStartPar
Robert J. Lempert, Steven W. Popper, and Steven C. Bankes. \sphinxstyleemphasis{Shaping the Next One Hundred Years}. RAND Corporation, 2003. URL: \sphinxurl{https://www.rand.org/pubs/monograph\_reports/MR1626.html} (visited on 2017\sphinxhyphen{}09\sphinxhyphen{}14).
\bibitem[143]{index:id68}
\sphinxAtStartPar
Jonathan R Lamontagne, Patrick M Reed, Robert Link, Katherine V Calvin, Leon E Clarke, and James A Edmonds. Large ensemble analytic framework for consequence\sphinxhyphen{}driven discovery of climate change scenarios. \sphinxstyleemphasis{Earth\textquotesingle{}s Future}, 6(3):488\textendash{}504, 2018.
\bibitem[144]{index:id69}
\sphinxAtStartPar
Brian C O’Neill, Elmar Kriegler, Keywan Riahi, Kristie L Ebi, Stephane Hallegatte, Timothy R Carter, Ritu Mathur, and Detlef P van Vuuren. A new scenario framework for climate change research: the concept of shared socioeconomic pathways. \sphinxstyleemphasis{Climatic change}, 122(3):387\textendash{}400, 2014.
\bibitem[145]{index:id72}
\sphinxAtStartPar
Warren E Walker, Marjolijn Haasnoot, and Jan H Kwakkel. Adapt or perish: a review of planning approaches for adaptation under deep uncertainty. \sphinxstyleemphasis{Sustainability}, 5(3):955\textendash{}979, 2013.
\bibitem[146]{index:id73}
\sphinxAtStartPar
Suraje Dessai, Mike Hulme, Robert Lempert, and Roger Pielke Jr. Climate prediction: a limit to adaptation. \sphinxstyleemphasis{Adapting to climate change: thresholds, values, governance}, 64:78, 2009.
\bibitem[147]{index:id78}
\sphinxAtStartPar
Jonathan D Herman, Patrick M Reed, Harrison B Zeff, and Gregory W Characklis. How should robustness be defined for water systems planning under change? \sphinxstyleemphasis{Journal of Water Resources Planning and Management}, 141(10):04015012, 2015.
\bibitem[148]{index:id82}
\sphinxAtStartPar
Robert J Lempert. Robust decision making (rdm). In \sphinxstyleemphasis{Decision making under deep uncertainty}, pages 23\textendash{}51. Springer, Cham, 2019.
\bibitem[149]{index:id83}
\sphinxAtStartPar
Jan H Kwakkel and Marjolijn Haasnoot. Supporting dmdu: a taxonomy of approaches and tools. In \sphinxstyleemphasis{Decision Making under Deep Uncertainty}, pages 355\textendash{}374. Springer, Cham, 2019.
\bibitem[150]{index:id84}
\sphinxAtStartPar
BC Trindade, PM Reed, and GW Characklis. Deeply uncertain pathways: integrated multi\sphinxhyphen{}city regional water supply infrastructure investment and portfolio management. \sphinxstyleemphasis{Advances in Water Resources}, 134:103442, 2019.
\bibitem[151]{index:id85}
\sphinxAtStartPar
Julianne D Quinn, Patrick M Reed, Matteo Giuliani, Andrea Castelletti, Jared W Oyler, and Robert E Nicholas. Exploring how changing monsoonal dynamics and human pressures challenge multireservoir management for flood protection, hydropower production, and agricultural water supply. \sphinxstyleemphasis{Water Resources Research}, 54(7):4638\textendash{}4662, 2018.
\bibitem[152]{index:id86}
\sphinxAtStartPar
DF Gold, PM Reed, BC Trindade, and GW Characklis. Identifying actionable compromises: navigating multi\sphinxhyphen{}city robustness conflicts to discover cooperative safe operating spaces for regional water supply portfolios. \sphinxstyleemphasis{Water Resources Research}, 55(11):9024\textendash{}9050, 2019.
\bibitem[153]{index:id87}
\sphinxAtStartPar
JR Lamontagne, PM Reed, G Marangoni, K Keller, and GG Garner. Robust abatement pathways to tolerable climate futures require immediate global action. \sphinxstyleemphasis{Nature Climate Change}, 9(4):290\textendash{}294, 2019.
\bibitem[154]{index:id202}
\sphinxAtStartPar
Antonia Hadjimichael, Julianne Quinn, Erin Wilson, Patrick Reed, Leon Basdekas, David Yates, and Michelle Garrison. Defining Robustness, Vulnerabilities, and Consequential Scenarios for Diverse Stakeholder Interests in Institutionally Complex River Basins. \sphinxstyleemphasis{Earth\textquotesingle{}s Future}, 8(7):e2020EF001503, 2020. URL: \sphinxurl{https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020EF001503} (visited on 2020\sphinxhyphen{}07\sphinxhyphen{}13), \sphinxhref{https://doi.org/10.1029/2020EF001503}{doi:10.1029/2020EF001503}.
\bibitem[155]{index:id88}
\sphinxAtStartPar
Harris Drucker and Corinna Cortes. Boosting decision trees. \sphinxstyleemphasis{Advances in neural information processing systems}, pages 479\textendash{}485, 1996.
\bibitem[156]{index:id90}
\sphinxAtStartPar
Kevin P Murphy. \sphinxstyleemphasis{Machine learning: a probabilistic perspective}. MIT press, 2012.
\bibitem[157]{index:id37}
\sphinxAtStartPar
Kelsey L. Ruckert, Gary Shaffer, David Pollard, Yawen Guan, Tony E. Wong, Chris E. Forest, and Klaus Keller. Assessing the Impact of Retreat Mechanisms in a Simple Antarctic Ice Sheet Model Using Bayesian Calibration. \sphinxstyleemphasis{PLOS ONE}, 12(1):e0170052, January 2017. \sphinxhref{https://doi.org/10.1371/journal.pone.0170052}{doi:10.1371/journal.pone.0170052}.
\bibitem[158]{index:id14}
\sphinxAtStartPar
B. Efron and R. Tibshirani. Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy. \sphinxstyleemphasis{Statistical Science}, 1(1):54\textendash{}75, February 1986. \sphinxhref{https://doi.org/10.1214/ss/1177013815}{doi:10.1214/ss/1177013815}.
\bibitem[159]{index:id35}
\sphinxAtStartPar
Ryan L. Sriver, Robert J. Lempert, Per Wikman\sphinxhyphen{}Svahn, and Klaus Keller. Characterizing uncertain sea\sphinxhyphen{}level rise projections to support investment decisions. \sphinxstyleemphasis{PLOS ONE}, 13(2):e0190641, February 2018. URL: \sphinxurl{http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0190641} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}09), \sphinxhref{https://doi.org/10.1371/journal.pone.0190641}{doi:10.1371/journal.pone.0190641}.
\bibitem[160]{index:id36}
\sphinxAtStartPar
Kelsey L. Ruckert, Yawen Guan, Alexander M. R. Bakker, Chris E. Forest, and Klaus Keller. The effects of time\sphinxhyphen{}varying observation errors on semi\sphinxhyphen{}empirical sea\sphinxhyphen{}level projections. \sphinxstyleemphasis{Climatic Change}, 140(3):349\textendash{}360, February 2017. URL: \sphinxurl{https://doi.org/10.1007/s10584-016-1858-z} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}09), \sphinxhref{https://doi.org/10.1007/s10584-016-1858-z}{doi:10.1007/s10584\sphinxhyphen{}016\sphinxhyphen{}1858\sphinxhyphen{}z}.
\bibitem[161]{index:id39}
\sphinxAtStartPar
Neil R Edwards, David Cameron, and Jonathan Rougier. Precalibrating an intermediate complexity climate model. \sphinxstyleemphasis{Clim. Dyn.}, 37(7\sphinxhyphen{}8):1469\textendash{}1482, 2011. URL: \sphinxurl{http://dx.doi.org/10.1007/s00382-010-0921-0}, \sphinxhref{https://doi.org/10.1007/s00382-010-0921-0}{doi:10.1007/s00382\sphinxhyphen{}010\sphinxhyphen{}0921\sphinxhyphen{}0}.
\bibitem[162]{index:id40}
\sphinxAtStartPar
Alexis Boukouvalas, Pete Sykes, Dan Cornford, and Hugo Maruri\sphinxhyphen{}Aguilar. Bayesian Precalibration of a Large Stochastic Microsimulation Model. \sphinxstyleemphasis{IEEE Transactions on Intelligent Transportation Systems}, 15(3):1337\textendash{}1347, June 2014. \sphinxhref{https://doi.org/10.1109/TITS.2014.2304394}{doi:10.1109/TITS.2014.2304394}.
\bibitem[163]{index:id41}
\sphinxAtStartPar
David Makowski, Daniel Wallach, and Marie Tremblay. Using a Bayesian approach to parameter estimation; comparison of the GLUE and MCMC methods. \sphinxstyleemphasis{Agronomie}, 22(2):191\textendash{}203, 2002. Publisher: EDP Sciences.
\bibitem[164]{index:id42}
\sphinxAtStartPar
Mahyar Shafii, Bryan Tolson, and Loren Shawn Matott. Uncertainty\sphinxhyphen{}based multi\sphinxhyphen{}criteria calibration of rainfall\sphinxhyphen{}runoff models: a comparative study. \sphinxstyleemphasis{Stochastic Environmental Research and Risk Assessment}, 28(6):1493\textendash{}1510, August 2014. \sphinxhref{https://doi.org/10.1007/s00477-014-0855-x}{doi:10.1007/s00477\sphinxhyphen{}014\sphinxhyphen{}0855\sphinxhyphen{}x}.
\bibitem[165]{index:id43}
\sphinxAtStartPar
Keith Beven and Jim Freer. Equifinality, data assimilation, and uncertainty estimation in mechanistic modelling of complex environmental systems using the GLUE methodology. \sphinxstyleemphasis{Journal of hydrology}, 249(1\sphinxhyphen{}4):11\textendash{}29, 2001. Publisher: Elsevier.
\bibitem[166]{index:id44}
\sphinxAtStartPar
Jasper A. Vrugt and Keith J. Beven. Embracing equifinality with efficiency: Limits of Acceptability sampling using the DREAM(LOA) algorithm. \sphinxstyleemphasis{Journal of Hydrology}, 559:954\textendash{}971, April 2018. \sphinxhref{https://doi.org/10.1016/j.jhydrol.2018.02.026}{doi:10.1016/j.jhydrol.2018.02.026}.
\bibitem[167]{index:id45}
\sphinxAtStartPar
Jery R. Stedinger, Richard M. Vogel, Seung Uk Lee, and Rebecca Batchelder. Appraisal of the generalized likelihood uncertainty estimation (GLUE) method. \sphinxstyleemphasis{Water Resources Research}, 2008. \sphinxhref{https://doi.org/10.1029/2008WR006822}{doi:10.1029/2008WR006822}.
\bibitem[168]{index:id170}
\sphinxAtStartPar
Christian Robert and George Casella. \sphinxstyleemphasis{Monte Carlo Statistical Methods}. Springer Science \& Business Media, March 2013. ISBN 978\sphinxhyphen{}1\sphinxhyphen{}4757\sphinxhyphen{}3071\sphinxhyphen{}5.
\bibitem[169]{index:id169}
\sphinxAtStartPar
Christian P. Robert. The Metropolis\textendash{}Hastings Algorithm. In \sphinxstyleemphasis{Wiley StatsRef: Statistics Reference Online}, pages 1\textendash{}15. American Cancer Society, 2015. URL: \sphinxurl{https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118445112.stat07834} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1002/9781118445112.stat07834}{doi:10.1002/9781118445112.stat07834}.
\bibitem[170]{index:id171}
\sphinxAtStartPar
James M. Flegal, Murali Haran, and Galin L. Jones. Markov Chain Monte Carlo: Can We Trust the Third Significant Figure? \sphinxstyleemphasis{Statistical Science}, 23(2):250\textendash{}260, May 2008. Publisher: Institute of Mathematical Statistics. URL: \sphinxurl{https://projecteuclid.org/journals/statistical-science/volume-23/issue-2/Markov-Chain-Monte-Carlo--Can-We-Trust-the-Third/10.1214/08-STS257.full} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1214/08-STS257}{doi:10.1214/08\sphinxhyphen{}STS257}.
\bibitem[171]{index:id172}
\sphinxAtStartPar
Carla Currin, Toby Mitchell, Max Morris, and Don Ylvisaker. Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments. \sphinxstyleemphasis{Journal of the American Statistical Association}, 86(416):953\textendash{}963, December 1991. Publisher: Taylor \& Francis \_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1991.10475138. URL: \sphinxurl{https://www.tandfonline.com/doi/abs/10.1080/01621459.1991.10475138} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1080/01621459.1991.10475138}{doi:10.1080/01621459.1991.10475138}.
\bibitem[172]{index:id173}
\sphinxAtStartPar
Jerome Sacks, William J. Welch, Toby J. Mitchell, and Henry P. Wynn. Design and Analysis of Computer Experiments. \sphinxstyleemphasis{Statistical Science}, 4(4):409\textendash{}423, 1989. Publisher: Institute of Mathematical Statistics. URL: \sphinxurl{https://www.jstor.org/stable/2245858} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[173]{index:id174}
\sphinxAtStartPar
Roger G. Ghanem and Pol D. Spanos. Spectral Stochastic Finite‐Element Formulation for Reliability Analysis. \sphinxstyleemphasis{Journal of Engineering Mechanics}, 117(10):2351\textendash{}2372, October 1991. Publisher: American Society of Civil Engineers. URL: \sphinxurl{https://ascelibrary.org/doi/abs/10.1061/\%28ASCE\%290733-9399\%281991\%29117\%3A10\%282351\%29} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1061/(ASCE)0733-9399(1991)117:10(2351)}{doi:10.1061/(ASCE)0733\sphinxhyphen{}9399(1991)117:10(2351)}.
\bibitem[174]{index:id175}
\sphinxAtStartPar
Dongbin Xiu and George Em Karniadakis. The Wiener\textendash{}Askey Polynomial Chaos for Stochastic Differential Equations. \sphinxstyleemphasis{SIAM Journal on Scientific Computing}, 24(2):619\textendash{}644, January 2002. Publisher: Society for Industrial and Applied Mathematics. URL: \sphinxurl{https://epubs.siam.org/doi/abs/10.1137/S1064827501387826} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1137/S1064827501387826}{doi:10.1137/S1064827501387826}.
\bibitem[175]{index:id178}
\sphinxAtStartPar
John Eason and Selen Cremaschi. Adaptive sequential sampling for surrogate model generation with artificial neural networks. \sphinxstyleemphasis{Computers \& Chemical Engineering}, 68:220\textendash{}232, September 2014. URL: \sphinxurl{https://www.sciencedirect.com/science/article/pii/S0098135414001719} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1016/j.compchemeng.2014.05.021}{doi:10.1016/j.compchemeng.2014.05.021}.
\bibitem[176]{index:id179}
\sphinxAtStartPar
Dirk Gorissen, Luciano De Tommasi, Karel Crombecq, and Tom Dhaene. Sequential modeling of a low noise amplifier with neural networks and active learning. \sphinxstyleemphasis{Neural Computing and Applications}, 18(5):485\textendash{}494, June 2009. URL: \sphinxurl{https://doi.org/10.1007/s00521-008-0223-1} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1007/s00521-008-0223-1}{doi:10.1007/s00521\sphinxhyphen{}008\sphinxhyphen{}0223\sphinxhyphen{}1}.
\bibitem[177]{index:id180}
\sphinxAtStartPar
Jenný Brynjarsdóttir and Anthony O\textquotesingle{}Hagan. Learning about physical parameters: the importance of model discrepancy. \sphinxstyleemphasis{Inverse Problems}, 30(11):114007, October 2014. Publisher: IOP Publishing. URL: \sphinxurl{https://doi.org/10.1088/0266-5611/30/11/114007} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1088/0266-5611/30/11/114007}{doi:10.1088/0266\sphinxhyphen{}5611/30/11/114007}.
\bibitem[178]{index:id181}
\sphinxAtStartPar
Michael Betancourt. A conceptual introduction to Hamiltonian Monte Carlo. \sphinxstyleemphasis{arXiv preprint arXiv:1701.02434}, 2017.
\bibitem[179]{index:id182}
\sphinxAtStartPar
Radford M. Neal. MCMC using Hamiltonian dynamics. \sphinxstyleemphasis{Handbook of markov chain monte carlo}, 2(11):2, 2011.
\bibitem[180]{index:id183}
\sphinxAtStartPar
Matti Vihola. Robust adaptive Metropolis algorithm with coerced acceptance rate. \sphinxstyleemphasis{Statistics and Computing}, 22(5):997\textendash{}1008, September 2012. URL: \sphinxurl{https://doi.org/10.1007/s11222-011-9269-5} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1007/s11222-011-9269-5}{doi:10.1007/s11222\sphinxhyphen{}011\sphinxhyphen{}9269\sphinxhyphen{}5}.
\bibitem[181]{index:id184}
\sphinxAtStartPar
Perry de Valpine, Daniel Turek, Christopher J. Paciorek, Clifford Anderson\sphinxhyphen{}Bergman, Duncan Temple Lang, and Rastislav Bodik. Programming With Models: Writing Statistical Algorithms for General Model Structures With NIMBLE. \sphinxstyleemphasis{Journal of Computational and Graphical Statistics}, 26(2):403\textendash{}413, April 2017. Publisher: Taylor \& Francis \_eprint: https://doi.org/10.1080/10618600.2016.1172487. URL: \sphinxurl{https://doi.org/10.1080/10618600.2016.1172487} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1080/10618600.2016.1172487}{doi:10.1080/10618600.2016.1172487}.
\bibitem[182]{index:id185}
\sphinxAtStartPar
NIMBLE Development Team. NIMBLE: MCMC, Particle Filtering, and Programmable Hierarchical Modeling. May 2021. URL: \sphinxurl{https://zenodo.org/record/4829693} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.5281/zenodo.4829693}{doi:10.5281/zenodo.4829693}.
\bibitem[183]{index:id186}
\sphinxAtStartPar
Stan Development Team. Stan Modeling Language Users Guide and Reference Manual. 2021. URL: \sphinxurl{https://mc-stan.org/docs/2\_27/stan-users-guide/index.html} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[184]{index:id187}
\sphinxAtStartPar
John Salvatier, Thomas V. Wiecki, and Christopher Fonnesbeck. Probabilistic programming in Python using PyMC3. \sphinxstyleemphasis{PeerJ Computer Science}, 2:e55, April 2016. Publisher: PeerJ Inc. URL: \sphinxurl{https://peerj.com/articles/cs-55} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.7717/peerj-cs.55}{doi:10.7717/peerj\sphinxhyphen{}cs.55}.
\bibitem[185]{index:id188}
\sphinxAtStartPar
Hong Ge, Kai Xu, and Zoubin Ghahramani. Turing: A Language for Flexible Probabilistic Inference. In \sphinxstyleemphasis{International Conference on Artificial Intelligence and Statistics}, 1682\textendash{}1690. PMLR, March 2018. ISSN: 2640\sphinxhyphen{}3498. URL: \sphinxurl{http://proceedings.mlr.press/v84/ge18b.html} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[186]{index:id189}
\sphinxAtStartPar
Andrew Gelman and Donald B. Rubin. Inference from Iterative Simulation Using Multiple Sequences. \sphinxstyleemphasis{Statistical Science}, 7(4):457\textendash{}472, November 1992. Publisher: Institute of Mathematical Statistics. URL: \sphinxurl{https://projecteuclid.org/journals/statistical-science/volume-7/issue-4/Inference-from-Iterative-Simulation-Using-Multiple-Sequences/10.1214/ss/1177011136.full} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1214/ss/1177011136}{doi:10.1214/ss/1177011136}.
\bibitem[187]{index:id190}
\sphinxAtStartPar
Pierre Del Moral, Arnaud Doucet, and Ajay Jasra. Sequential Monte Carlo samplers. \sphinxstyleemphasis{Journal of the Royal Statistical Society: Series B (Statistical Methodology)}, 68(3):411\textendash{}436, 2006. \_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467\sphinxhyphen{}9868.2006.00553.x. URL: \sphinxurl{http://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2006.00553.x} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1111/j.1467-9868.2006.00553.x}{doi:10.1111/j.1467\sphinxhyphen{}9868.2006.00553.x}.
\bibitem[188]{index:id191}
\sphinxAtStartPar
Arnaud Doucet, Simon Godsill, and Christophe Andrieu. On sequential Monte Carlo sampling methods for Bayesian filtering. \sphinxstyleemphasis{Statistics and Computing}, 10(3):197\textendash{}208, July 2000. URL: \sphinxurl{https://doi.org/10.1023/A:1008935410038} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1023/A:1008935410038}{doi:10.1023/A:1008935410038}.
\bibitem[189]{index:id192}
\sphinxAtStartPar
Jane Liu and Mike West. Combined Parameter and State Estimation in Simulation\sphinxhyphen{}Based Filtering. In Arnaud Doucet, Nando de Freitas, and Neil Gordon, editors, \sphinxstyleemphasis{Sequential Monte Carlo Methods in Practice}, Statistics for Engineering and Information Science, pages 197\textendash{}223. Springer, New York, NY, 2001. URL: \sphinxurl{https://doi.org/10.1007/978-1-4757-3437-9\_10} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1007/978-1-4757-3437-9\_10}{doi:10.1007/978\sphinxhyphen{}1\sphinxhyphen{}4757\sphinxhyphen{}3437\sphinxhyphen{}9\_10}.
\bibitem[190]{index:id193}
\sphinxAtStartPar
Stefano Cabras, Maria Eugenia Castellanos Nueda, and Erlis Ruli. Approximate Bayesian Computation by Modelling Summary Statistics in a Quasi\sphinxhyphen{}likelihood Framework. \sphinxstyleemphasis{Bayesian Analysis}, 10(2):411\textendash{}439, June 2015. Publisher: International Society for Bayesian Analysis. URL: \sphinxurl{https://projecteuclid.org/journals/bayesian-analysis/volume-10/issue-2/Approximate-Bayesian-Computation-by-Modelling-Summary-Statistics-in-a-Quasi/10.1214/14-BA921.full} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1214/14-BA921}{doi:10.1214/14\sphinxhyphen{}BA921}.
\bibitem[191]{index:id194}
\sphinxAtStartPar
Jarno Lintusaari, Michael U. Gutmann, Ritabrata Dutta, Samuel Kaski, and Jukka Corander. Fundamentals and Recent Developments in Approximate Bayesian Computation. \sphinxstyleemphasis{Systematic Biology}, 66(1):e66\textendash{}e82, January 2017. URL: \sphinxurl{https://doi.org/10.1093/sysbio/syw077} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1093/sysbio/syw077}{doi:10.1093/sysbio/syw077}.
\bibitem[192]{index:id195}
\sphinxAtStartPar
Mikael Sunnåker, Alberto Giovanni Busetto, Elina Numminen, Jukka Corander, Matthieu Foll, and Christophe Dessimoz. Approximate Bayesian Computation. \sphinxstyleemphasis{PLOS Computational Biology}, 9(1):e1002803, January 2013. Publisher: Public Library of Science. URL: \sphinxurl{https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1002803} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1371/journal.pcbi.1002803}{doi:10.1371/journal.pcbi.1002803}.
\bibitem[193]{index:id196}
\sphinxAtStartPar
Edwin T. Jaynes. \sphinxstyleemphasis{Probability theory: the logic of science}. Washington University St. Louis, MO, 1996.
\bibitem[194]{index:id197}
\sphinxAtStartPar
Andrew Gelman, Daniel Simpson, and Michael Betancourt. The Prior Can Often Only Be Understood in the Context of the Likelihood. \sphinxstyleemphasis{Entropy}, 19(10):555, October 2017. Number: 10 Publisher: Multidisciplinary Digital Publishing Institute. URL: \sphinxurl{https://www.mdpi.com/1099-4300/19/10/555} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.3390/e19100555}{doi:10.3390/e19100555}.
\bibitem[195]{index:id198}
\sphinxAtStartPar
Christian Robert. \sphinxstyleemphasis{The Bayesian choice: from decision\sphinxhyphen{}theoretic foundations to computational implementation}. Springer Science \& Business Media, 2007.
\bibitem[196]{index:id200}
\sphinxAtStartPar
Andrew Gelman, Xiao\sphinxhyphen{}Li Meng, and Hal Stern. Posterior Predictive Assessment of Model Fitness via Realized Discrepancies. \sphinxstyleemphasis{Statistica Sinica}, 6(4):733\textendash{}760, 1996. Publisher: Institute of Statistical Science, Academia Sinica. URL: \sphinxurl{https://www.jstor.org/stable/24306036} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[197]{index:id199}
\sphinxAtStartPar
Andrew Gelman, Aki Vehtari, Daniel Simpson, Charles C. Margossian, Bob Carpenter, Yuling Yao, Lauren Kennedy, Jonah Gabry, Paul\sphinxhyphen{}Christian Bürkner, and Martin Modrák. Bayesian Workflow. \sphinxstyleemphasis{arXiv:2011.01808 {[}stat{]}}, November 2020. arXiv: 2011.01808. URL: \sphinxurl{http://arxiv.org/abs/2011.01808} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14).
\bibitem[198]{index:id201}
\sphinxAtStartPar
Andrew Gelman and Cosma Rohilla Shalizi. Philosophy and the practice of Bayesian statistics. \sphinxstyleemphasis{British Journal of Mathematical and Statistical Psychology}, 66(1):8\textendash{}38, 2013. \_eprint: https://bpspsychub.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2044\sphinxhyphen{}8317.2011.02037.x. URL: \sphinxurl{https://bpspsychub.onlinelibrary.wiley.com/doi/abs/10.1111/j.2044-8317.2011.02037.x} (visited on 2021\sphinxhyphen{}06\sphinxhyphen{}14), \sphinxhref{https://doi.org/10.1111/j.2044-8317.2011.02037.x}{doi:10.1111/j.2044\sphinxhyphen{}8317.2011.02037.x}.
\end{sphinxthebibliography}



\renewcommand{\indexname}{Index}
\printindex
\end{document}