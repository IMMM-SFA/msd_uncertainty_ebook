%% Generated by Sphinx.
\def\sphinxdocclass{report}
\documentclass[letterpaper,10pt,english]{sphinxmanual}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsable pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}

\PassOptionsToPackage{warn}{textcomp}
\usepackage[utf8]{inputenc}
\ifdefined\DeclareUnicodeCharacter
% support both utf8 and utf8x syntaxes
  \ifdefined\DeclareUnicodeCharacterAsOptional
    \def\sphinxDUC#1{\DeclareUnicodeCharacter{"#1}}
  \else
    \let\sphinxDUC\DeclareUnicodeCharacter
  \fi
  \sphinxDUC{00A0}{\nobreakspace}
  \sphinxDUC{2500}{\sphinxunichar{2500}}
  \sphinxDUC{2502}{\sphinxunichar{2502}}
  \sphinxDUC{2514}{\sphinxunichar{2514}}
  \sphinxDUC{251C}{\sphinxunichar{251C}}
  \sphinxDUC{2572}{\textbackslash}
\fi
\usepackage{cmap}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amssymb,amstext}
\usepackage{babel}



\usepackage{tgtermes}
\usepackage{tgheros}
\renewcommand{\ttdefault}{txtt}



\usepackage[Bjarne]{fncychap}
\usepackage{sphinx}

\fvset{fontsize=auto}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}
\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}


\title{Addressing Uncertainty in MultiSector Dynamics Research}
\date{May 28, 2021}
\release{v0.1.0}
\author{Patrick M.\@{} Reed, ...\@{}}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{Release}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{index::doc}}



\chapter{Introduction}
\label{\detokenize{1_introduction:introduction}}\label{\detokenize{1_introduction::doc}}
\sphinxAtStartPar
This guidance text has been developed in support of the Integrated Multisector Multiscale Modeling (IM3) Science Focus Area’s objective to formally integrate uncertainty into its research tasks. IM3 is focused on innovative modeling to explore how human and natural system landscapes in the United States co\sphinxhyphen{}evolve in response to short\sphinxhyphen{}term shocks and long\sphinxhyphen{}term influences. The project’s challenging scope is to advance our ability to study the interactions between energy, water, land, and urban systems, at scales ranging from local (\textasciitilde{}1km) to the contiguous United States, while consistently addressing influences such as population change, technology change, heat waves, and drought. Uncertainty and careful model\sphinxhyphen{}driven scientific insights are central to IM3’s key MultiSector Dynamics (MSD) science objectives shown below.

\sphinxAtStartPar
\sphinxstylestrong{IM3 key MSD science objectives include:}

\sphinxAtStartPar
\sphinxstyleemphasis{Develop flexible, open\sphinxhyphen{}source, and integrated modeling capabilities that capture the structure, dynamic behavior, and emergent properties of the multiscale interactions within and between human and natural systems.}

\sphinxAtStartPar
\sphinxstyleemphasis{Use these capabilities to study the evolution, vulnerability, and resilience of interacting human and natural systems and landscapes from local to continental scales, including their responses to the compounding effects of long\sphinxhyphen{}term influences and short\sphinxhyphen{}term shocks.}

\sphinxAtStartPar
\sphinxstyleemphasis{Understand the implications of uncertainty in data, observations, models, and model coupling approaches for projections of human\sphinxhyphen{}natural system dynamics.}

\sphinxAtStartPar
Addressing the objectives above poses a strong transdisciplinary challenge that heavily depends on a diversity of models and, more specifically, a consistent framing for making model\sphinxhyphen{}based science inferences. The term transdisciplinary science as used here formally implies a deep integration of disciplines to aid our hypothesis driven understanding of coupled human\sphinxhyphen{}natural systems\textendash{}bridging differences in theory, hypothesis generation, modeling, and modes of inference (National Research Council, 2014). The IM3 MSD research focus and questions require a deep integration across disciplines, where new modes of analysis can emerge that rapidly synthesize and exploit advances for making decision relevant insights that at minimum acknowledge uncertainty and more ideally promote a rigorous quantitative mapping of its effects on the generality of claimed scientific insights. More broadly, diverse scientific disciplines engaged in the science of coupled human\sphinxhyphen{}natural systems, ranging from natural sciences to engineering and economics, employ a diversity of numerical computer models to study and understand their underlying systems of focus. The utility of these computer models hinges on their ability to represent the underlying real systems with sufficient fidelity and enable the inference of novel insights. This is particularly challenging in the case of coupled human\sphinxhyphen{}natural systems where there exists a multitude of interdependent human and natural processes taking place that could potentially be represented. These processes usually translate into modeled representations that are highly complex, non\sphinxhyphen{}linear, and exhibit strong interactions and threshold behaviors (Elsawah et al., 2020; Haimes, 2018; Helbing, 2013). Model complexity and detail have also been increasing as a result of our improving understanding of these processes, the availability of data, and the rapid growth in computing power (Saltelli et al., 2019). As model complexity grows, modelers need to specify a lot more information than before: additional model inputs and relationships as more processes are represented, higher resolution data as more observations are collected, new coupling relationships and interactions as models are put together to answer multisector questions (e.g., the land\sphinxhyphen{}water\sphinxhyphen{}energy nexus). Typically, not all of this information is well known, nor is the impact of these many uncertainties on model outputs well understood. It is further especially difficult to distinguish the effects of individual as well as interacting sources of uncertainty when modeling coupled systems with multisector and multiscale dynamics (Wirtz and Nowak, 2017).

\sphinxAtStartPar
Given the challenge and opportunity posed by the disciplinary diversity of IM3, we utilized a team\sphinxhyphen{}wide survey to allow the project’s membership to provide their views on how their areas typically address uncertainty, emphasizing key literature examples and domain\sphinxhyphen{}specific reviews. Our synthesis of this survey information in Figure 1 summaries the team’s perspectives, enabling a summary of the commonalities and differences for how different disciplinary areas are typically addressing uncertainty. Figure 1 highlights the non\sphinxhyphen{}trivial challenge posed by seeking to carefully consider uncertainty across an MSD focused transdisciplinary team. There are significant differences across the team’s contributing disciplines in terms of the methodological approaches and tools used in the treatment of uncertainty. The horizontal axis of the figure represents a conceptual continuum of methodological approaches, ranging from deterministic (no uncertainty) modeling to the theoretical case of fully engaging in modeling all sources of uncertainty. The vertical axis of plot maps the analysis tools that are used in the disciplines’ literature, spanning error\sphinxhyphen{}driven historical analyses to full uncertainty quantification. Given that Figure 1 is a conceptual illustration, the mapping of each discipline’s boundaries is not meant to imply exactness. They encompass the scope of feedback attained in the team\sphinxhyphen{}wide survey responses. The color circles designate specific sources of uncertainty that could be considered. Within the mapped disciplinary approaches, the color circles distinguish those sources of uncertainty that are addressed in the bodies of literature reported by respondents. Note the complete absence of grey circles designating that at present few if any studies report results for understanding how model coupling relationships shape uncertainty. We can briefly distinguish the key terms of uncertainty quantification (UQ) and uncertainty characterization (UC). UQ refers to the formal focus on the full specification of likelihoods as well as distributional forms necessary to infer the joint  probabilistic response across all modeled factors of interest (Cooke, 1991). Alternatively, uncertainty characterization as defined here, refers to exploratory modeling of alternative hypotheses for the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path dependent changes in the form and function of modelled systems (Moallemi et al., 2020a; Walker et al., 2003). Uncertain factors are any model component which is affected by uncertainty: inputs, resolution levels, coupling relationships, model relationships and parameters. When a model has been established as a sufficiently accurate representation of the system some of these factors may reflect elements of the real\sphinxhyphen{}world system that the model represents (for example, a population level parameter would reflect a sufficiently accurate representation of the population level in the system under study). As discussed in later sections, the choice of UQ or UC depends on the specific goals of studies, the availability of data, the types of uncertainties (e.g., well\sphinxhyphen{}characterized or deep), the complexity of underlying models as well as the computational limits. Deep uncertainty (as opposed to well\sphinxhyphen{}characterized) refers to situations where expert opinions consulted on a decision do not know or cannot agree on system boundaries, or the outcomes of interest and their relative importance, or the prior probability distribution for the various uncertain factors present (Kwakkel et al., 2016; W. E. Walker et al., 2013).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure1_state_of_the_science}.png}
\caption{State\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art in different modeling communities, as reported in the survey distributed to IM3 teams. Deterministic Historical Evaluation: model evaluation under fully determined conditions defined using historical observations; Local Sensitivity Analysis: model evaluation performed by varying uncertain factors around specific reference values; Global Sensitivity Analysis: model evaluation performed by varying uncertain factors throughout their entire feasible value space; Uncertainty Characterization: model evaluation under alternative factor hypotheses to explore their implications for model output uncertainty; Uncertainty Quantification: representation of model output uncertainty using probability distributions; Traditional statistical inference: use of analysis results to describe deterministic or probabilistic outcomes resulting from the presence of uncertainty; Narrative scenarios: use of a limited decision\sphinxhyphen{}relevant number of scenarios to describe (sets of) changing system outcomes; Exploratory modeling for scenario discovery: use of large ensembles of uncertain conditions to discover decision\sphinxhyphen{}relevant combinations of uncertain factors}\label{\detokenize{1_introduction:id1}}\end{figure}

\sphinxAtStartPar
At present, there is no singular guide for confronting the computational and conceptual challenges of the multi\sphinxhyphen{}model, transdisciplinary workflows that characterize ambitious projects such as IM3 (Saltelli et al., 2015). The primary aim of this text is to begin to address this gap and provide guidance for facing these challenges. Chapter 2 provides an overview of diagnostic modeling and the different perspectives for how we should evaluate our models. Chapter 3 the basic methods and concepts for sensitivity analysis. Chapter 4 delves into more technical applications of sensitivity analysis to support diagnostic model evaluation and exploratory modeling. Chapter 5 transitions to an overview of the key concepts and tools for UQ. Chapter 6 transitions to the use of UQ to capture risks and extremes in MSD systems. Chapter 7 provides concluding remarks across the UC and UQ topics covered in this text. The appendices of this text include a glossary of the key concepts as well as example test cases and scripts to showcase various UC and UQ related tools.


\chapter{Diagnostic Modeling Overview and Perspectives}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:diagnostic-modeling-overview-and-perspectives}}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives::doc}}
\sphinxAtStartPar
This text prescribes a formal model diagnostic approach to IM3 computational experimentation that is a deliberative and iterative combination of state\sphinxhyphen{}of\sphinxhyphen{}the\sphinxhyphen{}art UC and global sensitivity analysis techniques that progresses from observed history\sphinxhyphen{}based fidelity evaluations to forward looking resilience and vulnerability inferences (Gupta et al., 2008; Hadjimichael et al., 2020).


\section{Overview of model diagnostics}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:overview-of-model-diagnostics}}
\sphinxAtStartPar
Model diagnostics provide a rich basis for hypothesis testing, model innovation, and improved inferences when classifying what is controlling highly consequential results (e.g., vulnerability or resilience in coupled human\sphinxhyphen{}natural systems). Figure 2, adapted from (Saltelli et al., 2019), presents idealized illustrations of the relationship between UC and global sensitivity analysis (GSA) for two coupled simulation models. The figure illustrates how UC can be used to address how uncertainties in various modeling decisions (data inputs, parameters, model structures, coupling relationships, and elsewhere) can be sampled and simulated to yield the empirical model output distribution(s) of interest. Monte Carlo frameworks allow us to sample and propagate (or integrate) the ensemble response of the model(s) of focus. The first step of any UC analysis is the specification of the initial input distributions as illustrated in Figure 2. The second step is to perform the Monte Carlo simulations. The question can then be raised, which of the modeling assumptions in our Monte Carlo experiment are the most responsible for the resulting output uncertainty. We can answer this question using “global sensitivity analysis” (GSA) as illustrated in Figure 2. GSA can be defined as a formal Monte Carlo sampling and analysis of modeling choices (structures, parameters, inputs) to quantify their influence on direct model outputs (or output\sphinxhyphen{}informed metrics). UC experiments by themselves do not explain why you get a particular uncertain outcome. The pie chart shown in Figure 2 is a conceptual representation of the results of GSA to identify those factors that are most dominantly influencing results, either individually or interactively (Saltelli et al., 2008).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure2_idealized_uc}.png}
\caption{Idealized uncertainty characterization and global sensitivity analysis for two coupled simulation models. Uncertainty coming from various sources (inputs, model structures, coupling relationships, and elsewhere) is propagated through the coupled model(s) to generate empirical distributions of outputs of interest (uncertainty characterization). This model output uncertainty can be decomposed to its origins, by means of sensitivity analysis. This figure has been adapted from Saltelli et al. (2019).}\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:id1}}\end{figure}

\sphinxAtStartPar
UC and GSA are not independent modeling analyses. As illustrated here, any GSA requires an initial UC hypothesis in the form of statistical assumptions and representations for the modeling choices of focus (structural, parametric, and data inputs). Information from these two model diagnostic tools can then be used to inform data needs for future model runs, experiments to reduce the uncertainty present, or the simplification or enhancement of the model where necessary. Together UC and GSA provide a foundation for diagnostic exploratory modeling that has a consistent focus on the assumptions, structural model forms, alternative parameterizations, and input data sets that are used to characterize the behavioral space of one or more models.


\section{Perspectives on diagnostic model evaluation}
\label{\detokenize{2_diagnostic_modeling_overview_and_perspectives:perspectives-on-diagnostic-model-evaluation}}
\sphinxAtStartPar
When we judge or diagnose models the terms “verification and validation” are commonly used. However, their appropriateness in the context of numerical models representing complex coupled human\sphinxhyphen{}natural systems is questionable (Beven, 2002; Oreskes et al., 1994). The core issue relates to the fact that these systems are often not fully known or perfectly implemented when modeled. Rather, they are defined within specific system framings and boundary conditions in an evolving learning process with the goal of making continual progress towards attaining higher levels of fidelity. For example, observations used to evaluate the fidelity of parameterized processes are often measured at a finer resolution than is represented in the model and then must be scaled up for the evaluation. In other cases, numerical models may neglect or simplify system processes because the data is not available or the physical mechanisms are not fully known. If sufficient agreement between prediction and observation is not achieved, it is challenging to know whether these types of modeling choices are the cause, or if other issues, such as deficiencies in the input parameters and/or other modeling assumptions are the true cause of errors. Even if there is high agreement between prediction and observation, the model cannot necessarily be considered validated, as it is always possible that the right values were produced for the wrong reasons. For example, low error can stem from a situation where different errors in underlying assumptions or parameters cancel each other out (“compensatory errors”). Furthermore, coupled human\sphinxhyphen{}natural system models are often subject to “equifinality”, a situation where multiple parameterized formulations can produce similar outputs or equally acceptable representations of the observed data. There is therefore no uniquely “true” or validated model, and the common practice of selecting “the best” deterministic calibration set is more of an assumption than a finding (Beven, 1993; Beven and Binley, 1992). The situation becomes even more tenuous when observational data is limited in its scope and/or quality to be insufficient to distinguish model representations or their performance differences.

\sphinxAtStartPar
These limitations on model verification undermine any purely positivist treatment of model validity: that a model should correctly and precisely represent reality to be valid. Under this perspective, closely related to empiricism, statistical tests should be used to compare the model’s output with observations and only through empirical verification can a model or theory be deemed credible. A criticism to this viewpoint (besides the aforementioned challenges for model verification) is that it reduces the justification of a model to the single criterion of predictive ability and accuracy (Barlas and Carpenter, 1990). Authors have argued that this ignores the explanatory power held in models and other procedures, which can also advance scientific knowledge (Toulmin, 1977). These views gave rise to relativist perspectives of science, which instead place more value on model utility in terms of fitness for a specific purpose or inquiry, rather than representational accuracy and predictive ability (Kleindorfer et al., 1998). This viewpoint appears to be most prevalent among practitioners seeking decision relevant insights (i.e., inspire new views vs. predict future conditions). The relativist perspective argues for the use of models as heuristics that can enhance our understanding and conceptions of system behaviors or possibilities (Eker et al., 2018). In contrast, natural sciences favor a positivist perspective, emphasizing similarity between simulation and observation even in application contexts where it is clear that projections are being made for conditions that have never been observed and the system of focus will have evolved structurally beyond the model representation being employed (e.g., decadal to centennial evolution of human\sphinxhyphen{}natural systems).

\sphinxAtStartPar
These differences in prevalent perspectives are mirrored in how model validation is defined by the two camps: From the relativist perspective, validation is seen as a process of incremental “confidence building” in a model as a mechanism for insight (Barlas, 1996), whereas in natural sciences validation is framed as a way to classify a model as having an acceptable representation of physical reality (Oreskes et al., 1994). Even though the relativist viewpoint does not dismiss the importance of representational accuracy, it does place it within a larger process of establishing confidence through a variety of tools. These tools, not necessarily quantitative, include communicating information between practitioners and modelers, interpreting a multitude of model outputs, and contrasting preferences and viewpoints.

\sphinxAtStartPar
On the technical side of the argument, differing views on the methodology of model validation appear as early as in the 1960’s. (Naylor and Finger, 1967) argue that model validation should not be limited to a single metric or test of performance (e.g., a single error metric), but should rather be extended to multiple tests that reflect different aspects of a model’s structure and behavior. This and similar arguments are made in literature to this day (Beven, 2018; Gupta et al., 2012, 2008; Kumar, 2011; Nearing et al., 2020) and are primarily founded on two premises. First, that even though modelers widely recognize that their models are abstractions of the truth, they still make truth claims based on traditional performance metrics that measure the divergence of their model from observation (Nearing et al., 2020). Second, that the natural systems mimicked by the models contain many processes that exhibit significant heterogeneity at various temporal and spatial scales. This heterogeneity is lost when a single performance measure is used, as a result of the inherent loss of process information occurring when transitioning from a highly dimensional and interactive system to the dimension of a single metric (Beven, 2002). These arguments are further elaborated in section 4.1 Understanding Errors.

\sphinxAtStartPar
Multiple authors have proposed that, instead, the evaluation of several model performance signatures (characteristics) should be considered to identify model structural errors and achieve a sufficient assessment of model performance (Gupta et al., 1998). There is however a point of departure here, especially when models are used to produce inferences that can inform decisions. When agencies and practitioners use models of their systems for public decisions, those models have already met sufficient conditions for credibility (e.g., acceptable representational fidelity), but may face broader tests on their salience and legitimacy in informing negotiated decisions (Cash et al., 2003; Eker et al., 2018; White et al., 2010). This presents a new challenge to model validation, that of selecting decision\sphinxhyphen{}relevant performance metrics, reflective of the system’s stakeholders’ viewpoints, so that the most consequential uncertainties are identified and addressed (Saltelli and Funtowicz, 2014). For complex multisector models at the intersection of climatic, hydrologic, agricultural, energy, or other processes, the output space is made up of a multitude of states and variables, with very different levels of salience to the system’s stakeholders and to their goals being achieved. This is further complicated when such systems are also institutionally and dynamically complex. As a result, a broader set of qualitative and quantitative performance metrics is necessary to evaluate models of such complex systems, one that embraces the plurality of value systems, agencies and perspectives present. For IM3, even though the goal is to develop better projections of future vulnerability and resilience in co\sphinxhyphen{}evolving human\sphinxhyphen{}natural systems and not to provide decision support per se, it is critical for our multisector, multiscale model evaluation processes to represent stakeholders’ adaptive decision processes credibly.

\sphinxAtStartPar
As a final point, when a model is used in a projection mode, its results are also subject to additional uncertainty, as there is no guarantee that the model’s functionality and predictive ability will stay the same as the baseline, where the verification and validation tests were conducted. This challenge requires an additional expansion of the scope of model evaluation: a broader set of uncertain conditions needs to be explored, spanning beyond historical observation and exploring a wide range of unprecedented conditions. This perspective on modeling, termed exploratory (Bankes, 1993), views models as computational experiments that can be used to explore vast ensembles of potential scenarios so as to identify those with consequential effects. Exploratory modeling literature explicitly orients experiments toward stakeholder consequences and decision\sphinxhyphen{}relevant inferences and shifts the focus from predicting future conditions to discovering which conditions lead to undesirable or desirable consequences.

\sphinxAtStartPar
This evolution in modeling perspectives can be mirrored by the IM3 family of models in a progression from evaluating models relative to observed history to advanced formalized analyses to make inferences on multisector, multiscale vulnerabilities and resilience. Exploratory modeling approaches can help fashion experiments with large numbers of alternative hypotheses on the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path\sphinxhyphen{}dependent changes in the form and function of human\sphinxhyphen{}natural systems (Weaver et al., 2013). The aim of this text is to therefore guide the reader through the use of sensitivity analysis and uncertainty methods across these perspectives on diagnostic and exploratory modeling.


\chapter{Sensitivity Analysis: The Basics}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-the-basics}}\label{\detokenize{3_sensitivity_analysis_the_basics::doc}}

\section{Global Versus Local Sensitivity}
\label{\detokenize{3_sensitivity_analysis_the_basics:global-versus-local-sensitivity}}
\sphinxAtStartPar
Out of the several definitions for sensitivity analysis presented in literature, the most widely used has been proposed by Saltelli et al. (2004) as “the study of how uncertainty in the output of a model (numerical or otherwise) can be apportioned to different sources of uncertainty in the model input”. In other words, sensitivity analysis explores the relationship between the model’s N input variables, x={[}x1,x2,…,xN{]}, and M output variables, y={[}y1,y2,…,yM{]} with y=g(x), where g is the model that maps the model inputs to the outputs (Borgonovo and Plischke, 2016). Therefore, sensitivity analysis provides us with a set of alternatives to conducting large empirical experiments which are costly and often, in practice, next to impossible.
Historically, there have been two broad categories of sensitivity analysis techniques: local and global. Local sensitivity analysis is performed by varying model parameters around specific reference values, with the goal of exploring how small input perturbations influence model performance. Due to its convenience, this approach has been widely used in literature, but has important limitations (Rakovec et al., 2014; Saltelli and Annoni, 2010). If the model is not linear, the results of local sensitivity analysis can be heavily biased, as they will vary depending on the range of the chosen input (e.g., (Tang et al., 2007)). If the model’s factors interact, local sensitivity analysis will underestimate their importance, as it does not account for those effects (e.g., (Hamm et al., 2006)). In general, as local sensitivity analysis only partially and locally explores the parametric space, it is not considered a valid approach for nonlinear models (Saltelli et al., 2019). This is illustrated in Fig. 3 (a\sphinxhyphen{}b), presenting contour plots of a model response (y1) with an additive linear model (in a) and with a nonlinear model (in b). In a linear model without interactions between the terms x1 and x2, local sensitivity analysis can produce appropriate sensitivity indices (Fig. 3 a). If however, factors x1 and x2 interact, the local and partial consideration of the space can not properly account for each factor’s effects on the model response (Fig. 3 b), as it is only informative at the base point where it is applied. In contrast, a global sensitivity analysis varies uncertain factors within the entire feasible space of variability (Fig. 3 c). This approach reveals the global effects of each parameter on the model output, including any interactive effects. For models that cannot be proven linear, global sensitivity analysis is preferred and this text is primarily discussing global sensitivity analysis methods. In general, whenever we use the term sensitivity analysis we are referring to its global application.

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=700\sphinxpxdimen]{{figure3_global_versus_local}.png}
\caption{Treatment of a two\sphinxhyphen{}dimensional space of variability by local (panels a\sphinxhyphen{}b) and global (panel c) sensitivity analyses. Local sensitivity analysis is only an appropriate approach to sensitivity in the case of linear models without interactions between terms (a). In the case of more complex models, local sensitivity will miscalculate sensitivity indices (b), and global sensitivity methods should be used instead (c).}\label{\detokenize{3_sensitivity_analysis_the_basics:id1}}\end{figure}


\section{Why Perform Sensitivity Analysis}
\label{\detokenize{3_sensitivity_analysis_the_basics:why-perform-sensitivity-analysis}}
\sphinxAtStartPar
It is important to understand the many ways in which a SA might be of use to your modeling effort which can help shape the framing of model\sphinxhyphen{}informed study. Most commonly, one might be motivated to perform sensitivity analysis for the following reasons, grouped into two major categories, Model Evaluation and Fidelity Testing, and Exploratory Modeling and Scenario Discovery.

\sphinxAtStartPar
\sphinxstyleemphasis{Model evaluation}: Sensitivity analysis can be used to gauge model inferences when assumptions about the structure of the model or its parameterization are dubious or have changed. For instance, consider a numerical model that uses a set of calibrated parameter values to produce outputs, which we then use to inform decisions about the real\sphinxhyphen{}world system represented. One might like to know if small changes in these parameter values significantly change this model’s output and the decisions it informs or if, instead, our parameter inferences yield stable model behavior regardless of the uncertainty present in the specific parameterized processes or properties. This can either discredit or lend credence to the model at hand, as well as any inferences drawn that are founded on its accurate representation of the system. Sensitivity analysis can identify which uncertain model factors cause this undesirable model behavior.

\sphinxAtStartPar
\sphinxstyleemphasis{Model simplification}: Sensitivity analysis can also be used to identify factors or components of the model that appear to have limited effects on direct outputs or metrics of interest. Consider a model that has been developed in an organization for the purposes of a specific research question and is later used in the context of a different application. Some processes represented in significant detail might no longer be of the same importance while consuming significant data or computational resources, as different outputs might be pertinent to the new application. Sensitivity analysis can be used to identify unimportant model components and simplify them to nominal values and reduced model forms. Model complexity and computational costs can therefore be reduced, and, by extension, monetary investments.

\sphinxAtStartPar
\sphinxstyleemphasis{Model refinement}: Alternatively, sensitivity analysis can reveal the factors or processes that are highly influential to the outputs or metrics of interest, by assessing their relative importance. In the context of model evaluation, this can inform which model components warrant additional investigation or measurement so the uncertainty surrounding them and the resulting model outputs or metrics of interest can be reduced.

\sphinxAtStartPar
\sphinxstyleemphasis{Exploratory modeling}: When sufficient credence has been established in the model, sensitivity analysis can be applied to a host of other inquiries. Inferences about the factors and processes that most (or least) control a model’s outputs of interest can be extrapolated to the real system they represent and be used in a heuristic manner to inform model\sphinxhyphen{}based inferences. On this foundation, a model paired with the advanced techniques presented in this text can be used to “discover” decision relevant and highly consequential outcomes (i.e., scenario discovery, (Bankes, 1993; Bryant and Lempert, 2010)).


\section{Sensitivity Analysis Applications for Model Evaluation and Fidelity Testing}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-applications-for-model-evaluation-and-fidelity-testing}}
\sphinxAtStartPar
Elucidation of the specific type of problem faced shapes the specific objectives of applying a sensitivity analysis, as well as methods and tools most appropriate and defensible for each application setting (Saltelli et al., 2004; Saltelli and Tarantola, 2002). The three most common sensitivity analysis applications (Factor Prioritization, Factor Fixing, and Factor Mapping) are presented below, but the reader should be aware that other uses have been proposed in the literature (e.g., (Anderson et al., 2014; Borgonovo, 2010)).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor prioritization}: This sensitivity analysis application type  (also referred to as factor ranking) refers to when one would like to identify the uncertain factors which, when fixed to their true value, would lead to the greatest reduction in output variability (Saltelli et al., 2008). Information from this type of analysis can be crucial to model improvement as these factors can become the focus of future measurement campaigns or numerical experiments so that uncertainty in the model output can be reduced. Fig. 4 (a) shows the effects of three uncertain variables (X1, X2, and X3) on the variance of output Y.V(E(Y|Xi)) indicates the variance in Y if factor Xi is left to vary freely while all other factors remain fixed to nominal values. In this case, factor X2 makes the largest contribution to the variability of output Y and it should therefore be prioritized. In the context of risk analysis, factor prioritization can be used to reduce output variance to below a given tolerable threshold (also known as variance cutting). As the number of factors of focus and their degree of interactions increases, the computational experiments and analysis techniques increase in their demands as well as sophistication (this is further elaborated in the Global versus Local Sensitivity and the Design of Experiments sections).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor fixing}: Conversely, sensitivity analysis used in this mode (also referred to as factor screening) aims to identify the model components that have a negligible effect or make no significant contributions to the variability of the outputs or metrics of interest (usually referred to as non\sphinxhyphen{}influential; Saltelli et al., 2008). In the stylized example of Fig. 4 (a), X1 makes the smallest contribution to the variability of output Y suggesting that the uncertainty in its value could be negligible and the factor itself fixed in subsequent model executions. Eliminating these factors or processes in the model or fixing them to a nominal value can help reduce model complexity as well as the unnecessary computational burden of subsequent model runs, results processing, or other sensitivity analyses (the fewer uncertain factors considered, the fewer runs are necessary to illuminate their effects on the output). Significance of the outcome can be gauged in a variety of manners, depending on the application. For instance, if applying a variance\sphinxhyphen{}based method, a minimum threshold value of contribution to the variance could be considered as a significance ‘cutoff’, and factors with indices below that value can be considered non\sphinxhyphen{}influential. Nb: conclusions about factor fixing should be made based on total\sphinxhyphen{}order effects, i.e., considering all the effects a factor has, individually and in interaction with other factors (explained in more detail in the Variance\sphinxhyphen{}based methods section).

\sphinxAtStartPar
\sphinxstyleemphasis{Factor mapping}: Finally, factor mapping can be used to pinpoint which values of uncertain factors lead to model outputs within a given range of the output space (Saltelli et al., 2008). In the context of model diagnostics, it is possible that the model’s output changes in ways considered impossible based on the represented processes, or other observed evidence. In this situation, factor mapping can be used to identify which uncertain model factors cause this undesirable model behavior by ‘filtering’ model runs that are considered ‘non\sphinxhyphen{}behavioral’ (Edwards et al., 2011; Pianosi et al., 2016; Spear and Hornberger, 1980). In Fig. 4 (b), region B of the output space denotes the set of behavioral model outcomes, which can be traced back to input space X (e.g., using Monte Carlo Filtering or pre\sphinxhyphen{}calibration).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{figure4_factor_mapping}.png}
\caption{Factor prioritization, factor fixing and factor mapping settings of sensitivity analysis.}\label{\detokenize{3_sensitivity_analysis_the_basics:id2}}\end{figure}


\section{Sensitivity Analysis Applications for Exploratory Modeling and Scenario Discovery}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-applications-for-exploratory-modeling-and-scenario-discovery}}
\sphinxAtStartPar
The language used above reflects a use of sensitivity analysis for model fidelity evaluation and improvement (top right panel in Fig. 3). However, as previously mentioned, when a model has been established as a sufficiently accurate representation of the system, sensitivity analysis can produce additional inferences (bottom right panel in Fig. 3). For instance, under the factor mapping use, the analyst can now focus on undesirable system states and discover which factors are most responsible for them: for instance, “population growth of above 25\% would be responsible for unacceptably high energy demands”. Factor prioritization and factor fixing can be used to make equivalent inferences, such as “growing populations and increasing temperatures are the leading factors for changing energy demands” (prioritizing of factors) or “changing dietary needs are inconsequential to increasing energy demands for this region” (a factor that can be fixed in subsequent model runs). All these inferences hinge on the assumption that the real system’s stakeholders consider the model states faithful enough representations of system states. As elaborated in the Perspectives on model evaluation section, this view on sensitivity analysis is founded on a relativist perspective on modeling, which tends to place more value on model usefulness rather than strict accuracy of representation in terms of error. As such, sensitivity analysis performed with decision\sphinxhyphen{}making relevance in mind will focus on model outputs or metrics that are consequential and decision relevant (e.g., energy demand in the examples above).


\section{Design of Experiments}
\label{\detokenize{3_sensitivity_analysis_the_basics:design-of-experiments}}
\sphinxAtStartPar
Before embarking on any sensitivity analysis, the first element that needs to be clarified is the uncertainty space of the model (Helton et al., 2006; Pianosi et al., 2016). In other words, how many and which factors making up the mathematical model are considered uncertain and can potentially affect the model output and the inferences drawn from it. Uncertain factors can be model parameters, model structures, inputs, or alternative model resolution levels (scales), all of which can be assessed through the tools presented in this text. Depending on the kind of factor, its variability can be elicited through various means: expert opinion, values reported in the literature, its physical meaning (e.g., population values in a city can never be negative), or through the use of more formal UQ methods, elaborated in later sections of this text. The model uncertainty space represents the entire space of variability present in each of the uncertain factors of a model. The complexity of most real\sphinxhyphen{}world models means that the response function, y=g(x), mapping inputs to outputs, is hardly ever available in an analytical form and therefore analytically computing the sensitivity of the output to each uncertain factor becomes impossible. In these cases, sensitivity analysis is only feasible through numerical procedures that employ different strategies to sample the uncertainty space and calculate sensitivity indices.
A sampling strategy is often referred to as a design of experiments and represents a methodological choice made before conducting any sensitivity analysis. Experimental design was first introduced by (Fisher, 1960) in the context of laboratory or field\sphinxhyphen{}based experiments. Its application in sensitivity analysis is similar to setting up a physical experiment in that it is used to discover the behavior of a system under specific conditions. An ideal design of experiments should provide a framework for the extraction of all plausible information about the impact of each factor on the output of the numerical model. The design of experiments is used to set up a simulation platform with the minimum computational cost to answer specific questions that cannot be readily drawn from the data through analytical or common data mining techniques. Models representing coupled human\sphinxhyphen{}natural systems usually have a large number of inputs, state variables and parameters, but not all of them exert fundamental control over the numerical process, despite their uncertainty, nor have substantial impacts on the model output, either independently or through their interactions. Each factor influences the model output in different ways that need to be discovered. For example, the influence of a parameter on model output can be linear or non\sphinxhyphen{}linear and can be continuous or only be active during specific times or at particular states of the system (Herman et al., 2013; Massmann et al., 2014). An effective and efficient design of experiments allows the analyst to explore these complex relationships and evaluate different behaviors of the model for various scientific questions (Van Schepdael et al., 2016).
There are a few different approaches to the design of experiments. The selection of design is closely related to the chosen sensitivity analysis approach, which is in turn shaped by the research motivations, scientific questions, and computational constraints at hand (additional discussion of this can be found at the end of the Sensitivity Analysis Methods section). For example, in a sensitivity analysis using perturbation and derivatives methods, the model input parameters vary from their nominal values one at a time, something that the design of experiments needs to reflect. If, instead, one were to perform sensitivity analysis using a multiple\sphinxhyphen{}starts perturbation method, the design of experiments needs to consider that multiple points across the factor space are used. The design of experiments specifically defines two key characteristics of samples that are fed to the numerical model: the number of samples and the range of each factor.
Generally, sampling can be performed randomly or by applying a stratifying approach. In random sampling, such as Monte Carlo (Metropolis and Ulam, 1949), samples are randomly generated by a pseudo\sphinxhyphen{}random number generator with an a\sphinxhyphen{}priori assumption about the distribution of parameters and their possible ranges. Random seeds can also be used to ensure consistency and higher control over the random process. However, this method could leave some holes in the parameter space and cause clustering in some spaces, especially for a large number of parameters (Norton, 2015). Most sampling strategies use stratified sampling to mitigate these disadvantages. Stratified sampling techniques divide the domain of each factor into subintervals, often of equal lengths. From each subinterval, an equal number of samples is drawn randomly, or based on the specific locations within the subintervals (Saltelli et al., 2008). The rest of this section overviews some of the most commonly used designs of experiments. Table 1 summarizes the designs discussed.


\begin{savenotes}\sphinxattablestart
\centering
\sphinxcapstartof{table}
\sphinxthecaptionisattop
\sphinxcaption{Summary of designs of experiments overviewed in this section. * Depends on the sample size.}\label{\detokenize{3_sensitivity_analysis_the_basics:id3}}
\sphinxaftertopcaption
\begin{tabulary}{\linewidth}[t]{|T|T|T|}
\hline
\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Design of experiments}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Factor interactions considered}
&\sphinxstyletheadfamily 
\sphinxAtStartPar
\sphinxstyleemphasis{Treatment of factor domains}
\\
\hline
\sphinxAtStartPar
One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (OAT)
&
\sphinxAtStartPar
No \sphinxhyphen{} main effects only
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\sphinxAtStartPar
Full Factorial Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects
&
\sphinxAtStartPar
Discrete (levels)
\\
\hline
\sphinxAtStartPar
Fractional Factorial Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} only lower\sphinxhyphen{}order effects*
&
\sphinxAtStartPar
Discrete (levels)
\\
\hline
\sphinxAtStartPar
Latin Hypercube (LH) Sampling
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects*
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\sphinxAtStartPar
Quasi\sphinxhyphen{}Random Sampling with Low\sphinxhyphen{}Discrepancy Sequences
&
\sphinxAtStartPar
Yes \sphinxhyphen{} including total effects*
&
\sphinxAtStartPar
Continuous (distributions)
\\
\hline
\end{tabulary}
\par
\sphinxattableend\end{savenotes}


\subsection{One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (OAT)}
\label{\detokenize{3_sensitivity_analysis_the_basics:one-at-a-time-oat}}
\sphinxAtStartPar
In this approach, only one model factor is changed at a time while all others are kept fixed across each iteration in a sampling sequence. The OAT method assumes that model factors of focus are linearly independent (i.e., there are no interactions) and can analyze how factors individually influence model outputs or metrics of interest. While highly popular given its ease of implementation, OAT  is ultimately highly limited in its exploration of a model’s sensitivities (Saltelli and Annoni, 2010). It is primarily used with local sensitivity techniques with similar criticisms: applying this sampling scheme on a system with nonlinear and interactive processes will miss important information on the effect uncertain factors have on the model. OAT samplings can be repeated multiple times in a more sophisticated manner and across different locations of the parameter space to overcome some of these challenges, which would increase computational costs and negate the main reasons for its selection.


\subsection{Full and Fractional Factorial Sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:full-and-fractional-factorial-sampling}}
\sphinxAtStartPar
In full factorial sampling, each factor is treated as being discrete, by considering two or more levels (or intervals). The sampling process then generates samples within each possible combination of levels, corresponding to each parameter. This scheme produces a more comprehensive sampling of the factors’ variability space, as it accounts for all candidate combinations of factor levels (Fig. 6 (a)). If the number of levels is the same across all factors, the number of generated samples is estimated using nk, where n is the number of levels and k is the number of factors. For example, Fig. 6 (a) presents a full factorial sampling of three uncertain factors (x1, x2, and x3), each considered as having four discrete levels. The total number of samples necessary for such an experiment is 43=64. As the number of factors increases, the number of simulations necessary can also grow exponentially, making full factorial sampling computationally burdensome (Fig. 6 (b)). As a result, literature has commonly applied full factorial sampling at only two levels per factor, typically the two extremes (Montgomery, 2017). This significantly reduces computational burden but is only considered appropriate in cases where factors can indeed only assume two discrete values (e.g., when testing the effects of epistemic uncertainty and comparing between model structure A and model structure B). In the case of physical parameters on continuous distributions (e.g., when considering the effects of measurement uncertainty in a temperature sensor), discretizing the range of a factor to only extreme levels can bias its estimated importance.
Fractional factorial design is a widely used alternative to full factorial that allows the analyst to significantly reduce the number of simulations by confounding the main effects of a factor with its interactive effects (Saltelli et al., 2008). In other words, if one can reasonably assume that higher\sphinxhyphen{}order interactions are negligible, information about the main effects and lower\sphinxhyphen{}order interactions can be obtained using a fraction of the full factorial design. Traditionally, fractional factorial design has also been limited to two levels (Montgomery, 2017), referred to as Fractional Factorial designs 2k\sphinxhyphen{}p (Box and Hunter, 1961). Recently, Generalized Fractional Factorial designs have also been proposed that allow for the structured generation of samples at more than two levels per factor (Surowiec et al., 2017). Consider a case where the modeling team dealing with the problem in Fig. 6 (a) cannot afford to perform 64 simulations of their model. They can afford 32 runs for their experiment and instead decide to fractionally sample the variability space of their factors. A potential design of such a sampling strategy is presented in Fig. 6 (c).

\begin{figure}[htbp]
\centering
\capstart

\noindent\sphinxincludegraphics[width=500\sphinxpxdimen]{{figure5_alternative_designs}.png}
\caption{Alternative designs of experiments and their computational costs for three uncertain factors (x1, x2, and x3). (a) Full factorial design sampling of three factors at four levels, at a total of 64 samples; (b) exponential growth of necessary number of samples when applying full factorial design at four levels; (c) fractional factorial design of three factors at four levels, at a total of 32 samples; and (d) Latin Hypercube sample of three factors with uniform distributions, at a total of 32 samples.}\label{\detokenize{3_sensitivity_analysis_the_basics:id4}}\end{figure}


\subsection{Latin Hypercube (LH) Sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:latin-hypercube-lh-sampling}}
\sphinxAtStartPar
Latin hypercube sampling (McKay et al., 1979) is one of the most common methods in space\sphinxhyphen{}filling experimental designs. With this sampling technique, for N uncertain factors, an N\sphinxhyphen{}dimensional hypercube is generated, with each factor divided into an equal number of levels depending on the sample to be generated. Equal numbers of samples are then randomly generated at each level, across all factors. In this manner, LH design guarantees sampling from every level of the variability space and without any overlaps. When the number of samples generated is much larger than the number of uncertain factors, LH sampling can be very effective in examining the effects of each factor (Saltelli et al., 2008). LH sampling is an attractive technique, because it guarantees a diverse coverage of the space, through the use of subintervals, without being constrained to discrete levels for each factor \sphinxhyphen{} compare Fig. 6 (c) with Fig. 6 (d) for the same number of samples.

\sphinxAtStartPar
LH sampling is less effective when the number of samples is not much larger than the number of uncertain factors, and the effects of each factor cannot be appropriately distinguished. The samples between factors can also be highly correlated, biasing any subsequent sensitivity analysis results. To address this, the sampling scheme can be modified to control for the correlation in parameters while maximizing the information derived. An example of such modification is through the use of orthogonal arrays (Tang, 1993).


\subsection{Low\sphinxhyphen{}Discrepancy Sequences}
\label{\detokenize{3_sensitivity_analysis_the_basics:low-discrepancy-sequences}}
\sphinxAtStartPar
Low\sphinxhyphen{}discrepancy sequences is another sampling technique that employs a pseudo\sphinxhyphen{}random generator for Monte Carlo sampling (Dalal et al., 2008; Zaremba, 1968). These quasi\sphinxhyphen{}Monte Carlo methods eliminate potential gaps and clusters between samples by minimizing discrepancy when generating uniformly distributed random samples within the hypercube. The discrepancy property is mathematically measured by characterizing the lumpiness of a sequence of samples in a multidimensional space, which results in evenly distributed samples (Dalal et al., 2008). Discrepancy can be quantitatively measured using the deviations of sampled points from the uniform distribution (Kucherenko et al., 2015). Low\sphinxhyphen{}discrepancy sequences ensure that the number of samples in any subspace of the variability hypercube is approximately the same. This is not something guaranteed by LH sampling, and even though the design can be improved through optimization with various criteria, it is limited to small sample sizes and low dimensions (Iooss et al., 2010; Jin et al., 2008; Kucherenko et al., 2015; Morris and Mitchell, 1995; Park, 1994). The Sobol sequence (Sobol, 1976; Sobol’, 1967), one of the most widely used sampling techniques, utilizes the low\sphinxhyphen{}discrepancy approach to uniformly fill the sampled factor space. A core advantage of this style of sampling is that it takes far fewer samples (i.e., simulations) to attain a much lower level of error in estimating model output statistics (e.g., the mean and variance of outputs).


\subsection{Other types of sampling}
\label{\detokenize{3_sensitivity_analysis_the_basics:other-types-of-sampling}}
\sphinxAtStartPar
The sampling techniques mentioned so far are general sampling methods useful for a variety of applications beyond sensitivity analysis. There are however techniques that have been developed for specific sensitivity analysis methods. Examples of these methods include the Morris One\sphinxhyphen{}At\sphinxhyphen{}a\sphinxhyphen{}Time (Morris, 1991), Fourier Amplitude Sensitivity Test (FAST; (Cukier et al., 1973)), Extended FAST (Saltelli et al., 1999), and Extended Sobol methods (Saltelli, 2002). For example, the Morris sampling strategy builds a number of trajectories (usually referred to as repetitions and denoted by r) in the input space each composed of N+1 factor points, where N is the number of uncertain factors. The first point of the trajectory is selected randomly and the subsequent N points are generated by moving one factor at a time by a fixed amount. Each factor is perturbed once along the trajectory, while the starting points of all of the trajectories are randomly and uniformly distributed. Several variations of this strategy also exist in the literature; for more details on each approach and their differences the reader is directed to (Pianosi et al., 2016).


\subsection{Synthetic generation of input time series}
\label{\detokenize{3_sensitivity_analysis_the_basics:synthetic-generation-of-input-time-series}}
\sphinxAtStartPar
Models often have input time series or processes with strong temporal and/or spatial correlations (e.g., streamflow, energy demand, price of commodities, etc.) that, while they might not immediately come to mind as factors to be examined in sensitivity analysis, can be treated as such. Synthetic input time series are used for a variety of reasons, for example, when observations are not available or are limited, or when past observations are not considered sufficiently representative to capture rare or extreme events of interest (Herman et al., 2016; Milly et al., 2008). Synthetic generation of input time series provides a valuable tool to consider non\sphinxhyphen{}stationarity and incorporate potential stressors, such as climate change impacts into input time series (Borgomeo et al., 2015). For example, a century of record will be insufficient to capture very high impact rare extreme events (e.g., persistent multi\sphinxhyphen{}year droughts). A large body of statistical literature exists focusing on the topics of synthetic weather (Herrera et al., 2017; Wilks and Wilby, 1999) and streamflow (Lamontagne and Stedinger, 2018; Medda and Bhar, 2019) generation that provides a rich suite of approaches for developing history\sphinxhyphen{}informed, well\sphinxhyphen{}characterized stochastic process models to better estimate rare individual or compound (hot, severe drought) extremes. It is beyond the scope of this text to review these methods. Readers are encouraged to explore the studies cited above in this section as well as the following publications for discussions and comparisons of these methods: (Borgomeo et al., 2015; Herman et al., 2016; Kirsch et al., 2013; Loucks and Beek, 2017; Steinschneider et al., 2015; Vogel, 2017; Vogel and Stedinger, 1988). The use of these methods for the purposes of exploratory modeling, especially in the context of well\sphinxhyphen{}characterized versus deep uncertainty, is further discussed in the Consequential Scenarios section.


\section{Sensitivity Analysis Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:sensitivity-analysis-methods}}
\sphinxAtStartPar
In this section, we describe some of the most widely applied sensitivity analysis methods along with their mathematical definitions. We also provide a detailed discussion on applying each method, as well as a comparison of and their features and limitations.


\subsection{Derivative\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:derivative-based-methods}}
\sphinxAtStartPar
Derivative\sphinxhyphen{}based methods explore how model outputs are affected by perturbations in a single model input around a particular input value. These methods are local and are performed using OAT sampling. For simplicity of mathematical notations, let us assume that the model \sphinxstyleemphasis{g(X)} only returns one output. Following Borgonovo (2008) and Pianosi et al. (2016), the sensitivity index, \sphinxstyleemphasis{S}$_{\text{i}}$, of the model’s \sphinxstyleemphasis{i}\sphinxhyphen{}th input factor, \sphinxstyleemphasis{x}$_{\text{i}}$, can be measured using the partial derivative evaluated at a nominal value, \(\underline x\), of the vector of inputs:
\begin{equation*}
\begin{split}S_i (\underline x) = \frac{\partial g}{\partial x} |_{\underline x{^c{_i}}}\end{split}
\end{equation*}
\sphinxAtStartPar
where \sphinxstyleemphasis{c}$_{\text{i}}$ is the scaling factor.


\subsection{Elementary Effect Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:elementary-effect-methods}}

\subsection{Regression\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:regression-based-methods}}

\subsection{Regional Sensitivity Analysis}
\label{\detokenize{3_sensitivity_analysis_the_basics:regional-sensitivity-analysis}}

\subsection{Variance\sphinxhyphen{}based Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:variance-based-methods}}

\subsection{Analysis of Variance (ANOVA)}
\label{\detokenize{3_sensitivity_analysis_the_basics:analysis-of-variance-anova}}

\subsection{Moment\sphinxhyphen{}Independent (Density\sphinxhyphen{}Based) Methods}
\label{\detokenize{3_sensitivity_analysis_the_basics:moment-independent-density-based-methods}}

\section{How To Choose A Sensitivity Analysis Method: Model Traits And Dimensionality}
\label{\detokenize{3_sensitivity_analysis_the_basics:how-to-choose-a-sensitivity-analysis-method-model-traits-and-dimensionality}}
\sphinxAtStartPar
Figure 8 presents a graphical synthesis of the methods overviewed in this section, with regards to their appropriateness of application based on the complexity of the model at hand and the computational limits on the number of model evaluations afforded. The bars below each method also indicate the sensitivity analysis purposes they are most appropriate to address, which are in turn a reflection of the motivations and research questions the sensitivity analysis is called to address. Computational intensity is measured as a multiple of the number of model factors that are considered uncertain (d). Increasing model complexity mandates that more advanced sensitivity analysis methods are applied to address potential nonlinearities, factor interactions and discontinuities. Such methods can only be performed at increasing computational expense. For example, computationally cheap linear regression should not be used to assess factors’ importance if the model cannot be proven linear and the factors independent, because important relationships will invariably be missed (recall the example in Fig. 5). When computational limits do constrain applications to make simplified assumptions and sensitivity techniques, any conclusions in such cases should be delivered with clear statements of the appropriate caveats.

\sphinxAtStartPar
The reader should also be aware that the estimates of computational intensity that are given here are indicative of magnitude and would vary depending on the sampling technique, model complexity and the level of information being asked. For example, a Sobol sensitivity analysis typically requires a sample of size n × d+2 to produce first\sphinxhyphen{} and total\sphinxhyphen{}order indices, where d is the number of uncertain factors and n is a scaling factor, selected ad hoc, depending on model complexity (Saltelli, 2002a). The scaling factor n is typically set to at least 1000, but it should most appropriately be set on the basis of index convergence. In other words, a prudent analyst would perform the analysis several times with increasing n and observe at what level the indices converge to stable values (Nossent et al., 2011). The level should be the minimum sample size used in subsequent sensitivity analyses of the same system. Furthermore, if the analyst would like to better understand the degrees of interaction between factors, requiring second\sphinxhyphen{}order indices, the sample size would have to increase to n × 2d+2 (Saltelli, 2002a).


\section{Software Toolkits}
\label{\detokenize{3_sensitivity_analysis_the_basics:software-toolkits}}

\chapter{Sensitivity Analysis: Diagnostic \& Exploratory Modeling}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:sensitivity-analysis-diagnostic-exploratory-modeling}}\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling::doc}}

\section{Understanding Errors: What Is Controlling Model Performance?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:understanding-errors-what-is-controlling-model-performance}}

\section{Consequential Dynamics: What is Controlling Model Behaviors of Interest?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-dynamics-what-is-controlling-model-behaviors-of-interest}}

\section{Consequential Scenarios: What is Controlling Consequential Outcomes?}
\label{\detokenize{4_sensitivity_analysis_diagnostic_and_exploratory_modeling:consequential-scenarios-what-is-controlling-consequential-outcomes}}

\chapter{Uncertainty Quantification: The Basics}
\label{\detokenize{5_uncertainty_quantification_the_basics:uncertainty-quantification-the-basics}}\label{\detokenize{5_uncertainty_quantification_the_basics::doc}}
\sphinxAtStartPar
As described in the previous sections, uncertainty characterization (UC) can be defined as exploratory modeling where alternative hypotheses for the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path\sphinxhyphen{}dependent changes in the form and function of systems are explored (Marchau et al., 2019). UC exploratory modeling has a consistent focus on the assumptions, structural model forms, alternative parameterizations, and input data sets that are used to characterize the behavioral space of one or more models. The focus of UC is not to exactly quantify and predict probabilistic likelihoods for all possible quantities, but instead to inform which modeling choices yield the most consequential behavioral changes or outcomes, especially when considering deeply uncertain, scenario\sphinxhyphen{}informed projections (Moallemi et al., 2020b; Walker et al., 2013).

\sphinxAtStartPar
In comparison, uncertainty quantification (UQ) refers to the representation of uncertainties using probability distributions. The act of quantification requires specific assumptions about distributional forms and likelihoods, which may be more or less justified depending on prior information about the system or model behavior (Frankignoul and Hasselmann, 1977; Zellner and Tian, 1964). Without this justification, alternative specifications may yield substantially different inferences.


\section{Why is Uncertainty Quantification Important for Understanding MultiSector System Dynamics?}
\label{\detokenize{5_uncertainty_quantification_the_basics:why-is-uncertainty-quantification-important-for-understanding-multisector-system-dynamics}}

\section{Uncertainty Quantification for Exploratory Modeling}
\label{\detokenize{5_uncertainty_quantification_the_basics:uncertainty-quantification-for-exploratory-modeling}}

\section{Bayesian Uncertainty Quantification}
\label{\detokenize{5_uncertainty_quantification_the_basics:bayesian-uncertainty-quantification}}

\section{Uncertainty Quantification Under (Deep) Uncertainty}
\label{\detokenize{5_uncertainty_quantification_the_basics:uncertainty-quantification-under-deep-uncertainty}}

\section{Integrating Model Diagnostics and Uncertainty Quantification}
\label{\detokenize{5_uncertainty_quantification_the_basics:integrating-model-diagnostics-and-uncertainty-quantification}}

\chapter{Uncertainty Quantification: A Tool For Capturing Risks \& Extremes}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:uncertainty-quantification-a-tool-for-capturing-risks-extremes}}\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes::doc}}

\section{Understanding Risk: How Probable Are Extreme Events?}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:understanding-risk-how-probable-are-extreme-events}}

\section{Understanding Tails: Statistical Modeling of Extreme Events}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:understanding-tails-statistical-modeling-of-extreme-events}}

\section{How to Choose an Appropriate Method?}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:how-to-choose-an-appropriate-method}}

\section{How to Select a Prior Distribution?}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:how-to-select-a-prior-distribution}}

\section{Posterior Predictive Checking}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:posterior-predictive-checking}}

\section{Model Selection and Comparison}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:model-selection-and-comparison}}

\section{What are Common Methods?}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:what-are-common-methods}}
\sphinxAtStartPar
There are many  methods to quantify  uncertainty. Each  method has advantages and disadvantages for a particular analysis. Here we focus on parametric uncertainty quantification, as a discussion of structural uncertainty quantification is beyond  the scope of this review. Moreover, we prefer to think about structural uncertainty from the perspective of exploratory modeling and deep uncertainty, rather than from the perspective of quantification and selection or averaging.

\sphinxAtStartPar
Uncertainty quantification methods can be broadly classified as Markov Chain Monte Carlo (MCMC) approaches, particle\sphinxhyphen{}based approaches, and emulation\sphinxhyphen{}based approaches, though there are some hybrid methods. Several of the most common approaches for uncertainty quantification are described below. In all cases, the computational and conceptual challenges associated with parametric uncertainty quantification grow rapidly with the number of model parameters. As noted in the prior sections, sensitivity analyses are useful for dimensionality reduction prior to conducting parametric uncertainty quantification. Both factor fixing and factor prioritization can be used to limit the number of parameters which are treated as uncertain.


\subsection{Scenario Discovery}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:scenario-discovery}}

\subsection{Pre\sphinxhyphen{}calibration/GLUE}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:pre-calibration-glue}}

\subsection{Markov Chain Monte Carlo}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:markov-chain-monte-carlo}}
\sphinxAtStartPar
Markov chain Monte Carlo (MCMC) is a “gold standard” approach to full uncertainty quantification. MCMC refers to a category of algorithms which systematically sample from a target distribution (in this case, the posterior distribution) by constructing a Markov chain. MCMC algorithms rely on the mixing properties of the resulting Markov chain to guarantee asymptotic convergence to the posterior distribution, as the chain is constructed so that the posterior is its stationary distribution. It should be stressed that this guarantee exists only asymptotically. Studies use heuristics to test for signs of misconvergence and to assess the skill of the approximation (xx)

\sphinxAtStartPar
MCMC algorithms begin with the choice of some initial value for the Markov chain. This value can be randomly determined, or can be some other quantity such as a maximum likelihood or maximum a posteriori estimates. While the Markov chain will eventually converge to the posterior regardless of the choice of initial value, the amount of time required to escape the transient dynamics of the Markov chain is dependent on this value. Typically, transient samples are discarded as burn\sphinxhyphen{}in, as they may skew the sample distribution if the burn\sphinxhyphen{}in is relatively long compared to the number of iterations spent exploring the posterior, though this practice is not universal and has been questioned by some statisticians (Geyer, 2011). However, when not discarding the transient area, the chain must be run for a larger number of iterations to ensure that these samples do not bias the sample distribution.

\sphinxAtStartPar
Diagnosing the convergence of the Markov chain to the posterior is more art than science, relying on heuristics and judgement. One example heuristic is to run many Markov chains from different initial conditions, ideally well\sphinxhyphen{}dispersed across the parameter space; one may be able to conclude that the chains have not yet converged if the resulting marginal parameter distributions are sufficiently different when plotted. The Gelman\sphinxhyphen{}Rubin diagnostic formalizes this idea by comparing the within\sphinxhyphen{}chain and pooled variances of multiple chains (Gelman and Rubin, 1992). The ratio of these two quantities, called the potential scale reduction factor, can diagnose a lack of convergence if it is sufficiently far from 1 (typically using a threshold such as 1.1 or 1.05). Thus, it is generally good practice to use several MCMC runs to facilitate the diagnoses of non\sphinxhyphen{}convergence.

\sphinxAtStartPar
Another key value is the effective sample size (ESS). Due to the Markovian property, the samples obtained using MCMC are autocorrelated, and therefore not independent. As a result, the number of samples obtained using MCMC are not directly useful when interpreting the extent of exploration (or computing quantities such as the Monte Carlo standard error (Flegal et al., 2008)). For example, it may not be appropriate to draw inferences about tail properties for a small  ESS.

\sphinxAtStartPar
Many MCMC algorithms exist, with varying strengths and weaknesses, discussed below. For example, some require more tuning to improve the ESS than others. All of these algorithms involve the evaluation of the model at various parameter settings. Once a Markov chain is constructed and deemed to suitably represent the posterior distribution, parameter values can be sampled from it with replacement as a proxy for directly sampling from the posterior.


\subsubsection{Metropolis\sphinxhyphen{}Hastings}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:metropolis-hastings}}

\subsubsection{Gibbs Sampling}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:gibbs-sampling}}

\subsubsection{Hamiltonian Monte Carlo}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:hamiltonian-monte-carlo}}

\subsection{Particle\sphinxhyphen{}based Methods}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:particle-based-methods}}

\section{What are Example Software Implementations?}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:what-are-example-software-implementations}}
\sphinxAtStartPar
There exist many software platforms to implement uncertainty assessment. Each implementation is built upon a specific programming language including, but not limited to R, Python, C++, Fortran, MATLAB, and Julia. Two key considerations are the user’s preferred programming language and the computer model’s native code. For instance, a computer model running in C++ may be better suited for a software implementation based on the same language. For inconsistencies, please see the discussion on wrappers below.

\sphinxAtStartPar
Here, we present an overview of popular packages inherent to R, Python, and Julia. The user is free to code the UQ implementation without incorporating these existing packages; however, it may require more effort to code the pertinent subroutines (e.g., MCMC and building surrogate models). Uncertainty quantification for computer models typically operates within the Bayesian framework (see What are Common Methods?). Each implementation includes a mechanism that enables Bayesian inference using MCMC, Gaussian process emulation, or Sequential Monte Carlo. We focus on a subset of the common approaches.


\subsection{Markov Chain Monte Carlo with the True Model}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:markov-chain-monte-carlo-with-the-true-model}}

\subsection{Markov Chain Monte Carlo with Surrogate Models}
\label{\detokenize{6_uncertainty_quantification_a_tool_for_capturing_risks_and_extremes:markov-chain-monte-carlo-with-surrogate-models}}

\chapter{Conclusion}
\label{\detokenize{7_conclusion:conclusion}}\label{\detokenize{7_conclusion::doc}}
\sphinxAtStartPar
As noted in the Introduction (Section 1.0), the computational and conceptual challenges of the multi\sphinxhyphen{}model, transdisciplinary workflows that characterize ambitious projects such as IM3 have limited UC and UQ analyses. Moreover, the very nature and purpose of modeling and diagnostic model evaluation can have very diverse philosophical framings depending on the disciplines involved (see Figure 1 and Section 2.0). The guidance provided in this text  can be used to frame consistent and rigorous experimental designs for better understanding the consequences and insights from our modeling choices when seeking to capture complex human\sphinxhyphen{}natural systems. The progression of sections of this text provide a thorough introduction of the concepts and definitions of diagnostic model evaluation, sensitivity analysis, UC, and UQ. In addition, we comprehensively discuss how specific modeling objectives and applications should guide the selection of appropriate techniques; broadly, these can include model diagnostics, in\sphinxhyphen{}depth analysis of the behavior of the abstracted system, and projections under conditions of deep uncertainty. This text also contains a detailed presentation of the main sensitivity analysis, UC, and UQ analysis methods and a discussion of their features and main limitations. Readers are also provided with an overview of computer tools and platforms that have been developed and could be considered in addressing IM3 scientific questions. The appendices of this text include a terminology glossary of the key concepts as well as example test cases and scripts to showcase various UC related capabilities.

\sphinxAtStartPar
Although we distinguish the UC and UQ model diagnostics, the reader should note that we suggest an overall consistent approach to both in this text by emphasizing “exploratory modeling” (see review add citation). Although data support, model complexity, and computational limits strongly distinguish the feasibility and appropriateness of the UC and UQ diagnostic tools (e.g., see Figure 18), we overall recommend that modelers view their work through the lens of cycles of learning. Iterative and deliberative exploration of model\sphinxhyphen{}based hypotheses and inferences for transdisciplinary teams is non\sphinxhyphen{}trivial and ultimately critical for mapping where innovations or insights are most consequential. Overall, we recommend approaching modeling with an openness to the diverse disciplinary perspectives such as those mirrored by the IM3 family of models in a progression from evaluating models relative to observed history to advanced formalized analyses to make inferences on multi\sphinxhyphen{}sector, multi\sphinxhyphen{}scale vulnerabilities and resilience. Exploratory modeling approaches can help fashion experiments with large numbers of alternative hypotheses on the co\sphinxhyphen{}evolutionary dynamics of influences, stressors, as well as path\sphinxhyphen{}dependent changes in the form and function of coupled human\sphinxhyphen{}natural systems (Weaver et al., 2013). This text guides the reader through the use of sensitivity analysis and uncertainty methods across the diverse perspectives that have shaped modern diagnostic and exploratory modeling.


\chapter{References}
\label{\detokenize{8_references:references}}\label{\detokenize{8_references::doc}}
\sphinxAtStartPar
Akaike, H., 1978. On the Likelihood of a Time Series Model. J. R. Stat. Soc. Ser. Stat. 27, 217\textendash{}235. \sphinxurl{https://doi.org/10.2307/2988185}
Akaike, H., 1974. A new look at the statistical model identification. IEEE Trans. Autom. Control 19, 716\textendash{}723. \sphinxurl{https://doi.org/10.1109/TAC.1974.1100705}
Anderson, B., Borgonovo, E., Galeotti, M., Roson, R., 2014. Uncertainty in Climate Change Modeling: Can Global Sensitivity Analysis Be of Help? Risk Anal. 34, 271\textendash{}293. \sphinxurl{https://doi.org/10.1111/risa.12117}
Annan, J.D., Hargreaves, J.C., 2004. Efficient parameter estimation for a highly chaotic system. Tellus A 56, 520\textendash{}526. \sphinxurl{https://doi.org/10.1111/j.1600-0870.2004.00073.x}
Bakker, A.M.R., Wong, T.E., Ruckert, K.L., Keller, K., 2017. Sea\sphinxhyphen{}level projections representing the deeply uncertain contribution of the West Antarctic ice sheet. Sci Rep 7, 3880. \sphinxurl{https://doi.org/10.1038/s41598-017-04134-5}
Bankes, S., 1993. Exploratory Modeling for Policy Analysis. Oper. Res. 41, 435\textendash{}449. \sphinxurl{https://doi.org/10.1287/opre.41.3.435}
Barlas, Y., 1996. Formal aspects of model validity and validation in system dynamics. Syst. Dyn. Rev. J. Syst. Dyn. Soc. 12, 183\textendash{}210.
Barlas, Y., Carpenter, S., 1990. Philosophical roots of model validation: Two paradigms. Syst. Dyn. Rev. 6, 148\textendash{}166. \sphinxurl{https://doi.org/10.1002/sdr.4260060203}
Bastos, L.S., O’Hagan, A., 2009. Diagnostics for Gaussian Process Emulators. Technometrics 51, 425\textendash{}438. \sphinxurl{https://doi.org/10.1198/TECH.2009.08019}
Batouli, M., Mostafavi, A., 2018. Multiagent Simulation for Complex Adaptive Modeling of Road Infrastructure Resilience to Sea\sphinxhyphen{}Level Rise. Comput.\sphinxhyphen{}Aided Civ. Infrastruct. Eng. 33, 393\textendash{}410. \sphinxurl{https://doi.org/10.1111/mice.12348}
Bayes, T., 1763. An Essay towards Solving a Problem in the Doctrine of Chance. Philos. Trans. R. Soc. Lond. 53, 370\textendash{}418.
Betancourt, M., 2017. A Conceptual Introduction to Hamiltonian Monte Carlo. ArXiv StatME.
Beven, K., 2002. Towards a coherent philosophy for modelling the environment. Proc. R. Soc. Lond. Ser. Math. Phys. Eng. Sci. 458, 2465\textendash{}2484. \sphinxurl{https://doi.org/10.1098/rspa.2002.0986}
Beven, K., 1993. Prophecy, reality and uncertainty in distributed hydrological modelling. Adv. Water Resour., Research Perspectives in Hydrology 16, 41\textendash{}51. \sphinxurl{https://doi.org/10.1016/0309-1708(93)90028-E}
Beven, K., Binley, A., 1992. The future of distributed models: Model calibration and uncertainty prediction. Hydrol. Process. 6, 279\textendash{}298. \sphinxurl{https://doi.org/10.1002/hyp.3360060305}
Beven, K., Freer, J., 2001. Equifinality, data assimilation, and uncertainty estimation in mechanistic modelling of complex environmental systems using the GLUE methodology. J Hydrol.
Beven, K.J., 2018. On hypothesis testing in hydrology: Why falsification of models is still a really good idea. WIREs Water 5, e1278. \sphinxurl{https://doi.org/10.1002/wat2.1278}
Bobée, B., 1975. The Log Pearson type 3 distribution and its application in hydrology. Water Resour Res 11, 681\textendash{}689. \sphinxurl{https://doi.org/10.1029/WR011i005p00681}
Bobée, B., Rasmussen, P.F., 1995. Recent advances in flood frequency analysis. Rev Geophys 33, 1111\textendash{}1116. \sphinxurl{https://doi.org/10.1029/95RG00287}
Borgomeo, E., Farmer, C.L., Hall, J.W., 2015. Numerical rivers: A synthetic streamflow generator for water resources vulnerability assessments. Water Resour. Res. 51, 5382\textendash{}5405. \sphinxurl{https://doi.org/10.1002/2014WR016827}
Borgonovo, E., 2010. Sensitivity analysis with finite changes: An application to modified EOQ models. Eur. J. Oper. Res. 200, 127\textendash{}138.
Borgonovo, E., Plischke, E., 2016. Sensitivity analysis: A review of recent advances. Eur. J. Oper. Res. 248, 869\textendash{}887. \sphinxurl{https://doi.org/10.1016/j.ejor.2015.06.032}
Boukouvalas, A., Sykes, P., Cornford, D., Maruri\sphinxhyphen{}Aguilar, H., 2014. Bayesian Precalibration of a Large Stochastic Microsimulation Model. IEEE Trans Intell Transp Syst 15, 1337\textendash{}1347. \sphinxurl{https://doi.org/10.1109/tits.2014.2304394}
Box, G.E., Hunter, J.S., 1961. The 2 k—p fractional factorial designs. Technometrics 3, 311\textendash{}351.
Bryant, B.P., Lempert, R.J., 2010. Thinking inside the box: A participatory, computer\sphinxhyphen{}assisted approach to scenario discovery. Technol. Forecast. Soc. Change 77, 34\textendash{}49. \sphinxurl{https://doi.org/10.1016/j.techfore.2009.08.002}
Budescu, D.V., Broomell, S.B., Lempert, R.J., Keller, K., 2014. Aided and unaided decisions with imprecise probabilities in the domain of losses. EURO J. Decis. Process. 2, 31\textendash{}62. \sphinxurl{https://doi.org/10.1007/s40070-013-0023-4}
Bürkner, P.\sphinxhyphen{}C., Gabry, J., Vehtari, A., 2019. Approximate leave\sphinxhyphen{}future\sphinxhyphen{}out cross\sphinxhyphen{}validation for Bayesian time series models. ArXiv StatME.
Burnham, K.P., Anderson, D.R., 2004. Multimodel Inference: Understanding AIC and BIC in Model Selection. Sociol Methods Res 33, 261\textendash{}304. \sphinxurl{https://doi.org/10.1177/0049124104268644}
Burnham, K.P., Anderson, D.R., 2001. Kullback\sphinxhyphen{}Leibler information as a basis for strong inference in ecological studies. Wildl Res 28, 111\textendash{}119. \sphinxurl{https://doi.org/10.1071/wr99107}
Cao, Y., Li, B., 2019. Assessing models for estimation and methods for uncertainty quantification for spatial return levels. Environmetrics 30, e2508.
Cash, D.W., Clark, W.C., Alcock, F., Dickson, N.M., Eckley, N., Guston, D.H., Jäger, J., Mitchell, R.B., 2003. Knowledge systems for sustainable development. Proc. Natl. Acad. Sci. 100, 8086\textendash{}8091.
Ciccazzo, A., Pillo, G.D., Latorre, V., 2014. Support vector machines for surrogate modeling of electronic circuits. Neural Comput. Appl. 24, 69\textendash{}76. \sphinxurl{https://doi.org/10.1007/s00521-013-1509-5}
Coles, S., Bawa, J., Trenner, L., Dorazio, P., 2001. An introduction to statistical modeling of extreme values. Springer.
Cooke, R.M., 1991. Experts in uncertainty:  Opinion and subjective probability in science, Experts in uncertainty:  Opinion and subjective probability in science. Oxford University Press, New York, NY, US.
Coronese, M., Lamperti, F., Keller, K., Chiaromonte, F., Roventini, A., 2019. Evidence for sharp increase in the economic damages of extreme natural disasters. Proc Natl Acad Sci U A 201907826. \sphinxurl{https://doi.org/10.1073/pnas.1907826116}
Cukier, R.I., Fortuin, C.M., Shuler, K.E., Petschek, A.G., Schaibly, J.H., 1973. Study of the sensitivity of coupled reaction systems to uncertainties in rate coefficients. I Theory. J. Chem. Phys. 59, 3873\textendash{}3878. \sphinxurl{https://doi.org/10.1063/1.1680571}
Currin, C., Mitchell, T., Morris, M., Ylvisaker, D., 1991. Bayesian Prediction of Deterministic Functions, with Applications to the Design and Analysis of Computer Experiments. J Am Stat Assoc 86, 953\textendash{}963. \sphinxurl{https://doi.org/10.1080/01621459.1991.10475138}
Dalal, I.L., Stefan, D., Harwayne\sphinxhyphen{}Gidansky, J., 2008. Low discrepancy sequences for Monte Carlo simulations on reconfigurable platforms, in: 2008 International Conference on Application\sphinxhyphen{}Specific Systems, Architectures and Processors. Presented at the 2008 International Conference on Application\sphinxhyphen{}Specific Systems, Architectures and Processors, pp. 108\textendash{}113. \sphinxurl{https://doi.org/10.1109/ASAP.2008.4580163}
DeConto, R.M., Pollard, D., 2016. Contribution of Antarctica to past and future sea\sphinxhyphen{}level rise. Nature 531, 591\textendash{}597. \sphinxurl{https://doi.org/10.1038/nature17145}
Del Moral, P., Doucet, A., Jasra, A., 2006. Sequential Monte Carlo samplers. J R Stat Soc Ser. B Stat Methodol 68, 411\textendash{}436. \sphinxurl{https://doi.org/10.1111/j.1467-9868.2006.00553.x}
Demarta, S., McNeil, A.J., 2005. The t copula and related copulas. Int. Stat. Rev. 73, 111\textendash{}129.
Doucet, A., Freitas, N., Gordon, N., 2001. An Introduction to Sequential Monte Carlo Methods. Seq. Monte Carlo Methods Pract. 3\textendash{}14. \sphinxurl{https://doi.org/10.1007/978-1-4757-3437-9\_1}
Doucet, A., Godsill, S., Andrieu, C., 2000. On sequential Monte Carlo sampling methods for Bayesian filtering. Stat Comput 10, 197\textendash{}208. \sphinxurl{https://doi.org/10.1023/A:1008935410038}
Eason, J., Cremaschi, S., 2014. Adaptive sequential sampling for surrogate model generation with artificial neural networks. Comput. Chem. Eng. 68, 220\textendash{}232. \sphinxurl{https://doi.org/10.1016/j.compchemeng.2014.05.021}
Edwards, N.R., Cameron, D., Rougier, J., 2011. Precalibrating an intermediate complexity climate model. Clim Dyn 37, 1469\textendash{}1482. \sphinxurl{https://doi.org/10.1007/s00382-010-0921-0}
Eker, S., Rovenskaya, E., Obersteiner, M., Langan, S., 2018. Practice and perspectives in the validation of resource management models. Nat. Commun. 9, 1\textendash{}10. \sphinxurl{https://doi.org/10.1038/s41467-018-07811-9}
El Adlouni, S., Bobée, B., Ouarda, T.B.M.J., 2008. On the tails of extreme event distributions in hydrology. J Hydrol 355, 16\textendash{}33. \sphinxurl{https://doi.org/10.1016/j.jhydrol.2008.02.011}
Ellsberg, D., 1961. Risk, Ambiguity, and the Savage Axioms. Q J Econ 75, 643\textendash{}669. \sphinxurl{https://doi.org/10.2307/1884324}
Elsawah, S., Filatova, T., Jakeman, A.J., Kettner, A.J., Zellner, M.L., Athanasiadis, I.N., Hamilton, S.H., Axtell, R.L., Brown, D.G., Gilligan, J.M., Janssen, M.A., Robinson, D.T., Rozenberg, J., Ullah, I.I.T., Lade, S.J., 2020. Eight grand challenges in socio\sphinxhyphen{}environmental systems modeling. Socio\sphinxhyphen{}Environ. Syst. Model. 2, 16226\textendash{}16226. \sphinxurl{https://doi.org/10.18174/sesmo.2020a16226}
Evensen, G., 1994. Sequential data assimilation with a nonlinear quasi\sphinxhyphen{}geostrophic model using Monte Carlo methods to forecast error statistics. J. Geophys. Res. Oceans 99, 10143\textendash{}10162. \sphinxurl{https://doi.org/10.1029/94JC00572}
Ferdous, M.R., Wesselink, A., Brandimarte, L., Di Baldassarre, G., Rahman, M.M., 2019. The levee effect along the Jamuna River in Bangladesh. Water Int 44, 496\textendash{}519. \sphinxurl{https://doi.org/10.1080/02508060.2019.1619048}
Field, C.B., Barros, V.R., Dokken, D.J., Mach, K.J., Mastrandrea, M.D., Bilir, T.E., Chatterjee, M., Ebi, K.L., Estrada, Y.O., Genova, R.C., Girma, B., Kissel, E.S., Levy, A.N., MacCracken, S., Mastrandrea, P.R., White, L.L., 2014. Summary for Policymakers, in: Climate Change 2014: Impacts, Adaptation, and Vulnerability. Part A: Global and Sectoral Aspects. Contribution of Working Group II to the Fifth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press, Cambridge, United Kingdom, and New York, NY, USA, pp. 1\textendash{}32.
Fisher, R.A., 1960. The design of experiments. Oliver and Boyd, Edinburgh, Scotland, UK.
Flegal, J.M., Haran, M., Jones, G.L., 2008. Markov Chain Monte Carlo: Can We Trust the Third Significant Figure? Stat Sci 23, 250\textendash{}260. \sphinxurl{https://doi.org/10.1214/08-STS257}
Frankignoul, C., Hasselmann, K., 1977. Stochastic climate models, Part II Application to sea\sphinxhyphen{}surface temperature anomalies and thermocline variability. Tell’Us 29, 289\textendash{}305. \sphinxurl{https://doi.org/10.3402/tellusa.v29i4.11362}
Fuller, R.W., Wong, T.E., Keller, K., 2017. Probabilistic inversion of expert assessments to inform projections about Antarctic ice sheet responses. PLoS One 12, e0190115. \sphinxurl{https://doi.org/10.1371/journal.pone.0190115}
Gao, L., Bryan, B.A., Nolan, M., Connor, J.D., Song, X., Zhao, G., 2016. Robust global sensitivity analysis under deep uncertainty via scenario analysis. Environ. Model. Softw. 76, 154\textendash{}166. \sphinxurl{https://doi.org/10.1016/j.envsoft.2015.11.001}
Gelfand, A.E., Hills, S.E., Racine\sphinxhyphen{}Poon, A., Adrian F. M. Smith, 1990. Illustration of Bayesian Inference in Normal Data Models Using Gibbs Sampling. J Am Stat Assoc 85, 972\textendash{}985. \sphinxurl{https://doi.org/10.2307/2289594}
Gelman, A., Carlin, J.B., Stern, H.S., Dunson, D.B., Vehtari, A., Rubin, D.B., 2013. Bayesian Data Analysis, Third. ed. CRC Press.
Gelman, A., Hwang, J., Vehtari, A., 2014. Understanding predictive information criteria for Bayesian models. Stat Comput 24, 997\textendash{}1016. \sphinxurl{https://doi.org/10.1007/s11222-013-9416-2}
Gelman, A., Meng, X.\sphinxhyphen{}L., Stern, H., 1996. Posterior predictive assessment of model fitness via realized discrepancies. Stat Sin 6, 733\textendash{}760.
Gelman, A., Rubin, D.B., 1992. Inference from Iterative Simulation Using Multiple Simulations. Stat Sci 7, 457\textendash{}511. \sphinxurl{https://doi.org/10.1214/ss/1177011136}
Gelman, A., Shalizi, C.R., 2013. Philosophy and the practice of Bayesian statistics. Br J Math Stat Psychol 66, 8\textendash{}38. \sphinxurl{https://doi.org/10.1111/j.2044-8317.2011.02037.x}
Gelman, A., Simpson, D., Betancourt, M., 2017. The Prior Can Often Only Be Understood in the Context of the Likelihood. Entropy 19, 555. \sphinxurl{https://doi.org/10.3390/e19100555}
Geman, S., Geman, D., 1984. Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. IEEE Trans Pattern Anal Mach Intell 6, 721\textendash{}741. \sphinxurl{https://doi.org/10.1109/tpami.1984.4767596}
Geyer, C.J., 2011. Introduction to Markov Chain Monte Carlo, in: Brooks, S., Gelman, A., Jones, G.L., Meng, X.\sphinxhyphen{}L. (Eds.), Handbook of Markov Chain Monte Carlo. Chapman \& Hall/CRC, Boca Raton, FL, pp. 3\textendash{}48.
Geyer, C.J., Johnson, L.T., 2013. mcmc: Markov Chain Monte Carlo. R Package Version 0 9\sphinxhyphen{}2 URL HttpCRAN R\sphinxhyphen{}Proj. Orgpackage Mcmc.
Ghanem, R.G., Spanos, P.D., 1991. Spectral stochastic finite\sphinxhyphen{}element formulation for reliability analysis. J Eng Mech 117, 2351\textendash{}2372.
Goodman, J., Weare, J., 2010. Ensemble samplers with affine invariance. Commun. Appl. Math. Comput. Sci. 5, 65\textendash{}80. \sphinxurl{https://doi.org/10.2140/camcos.2010.5.65}
Gordon, N.J., Salmond, D.J., Smith, A.F.M., 1993. Novel approach to nonlinear/non\sphinxhyphen{}Gaussian Bayesian state estimation. IEE Proc. F Radar Signal Process. 140, 107\textendash{}113. \sphinxurl{https://doi.org/10.1049/ip-f-2.1993.0015}
Gorissen, D., De Tommasi, L., Crombecq, K., Dhaene, T., 2009. Sequential modeling of a low noise amplifier with neural networks and active learning. Neural Comput. Appl. 18, 485\textendash{}494. \sphinxurl{https://doi.org/10.1007/s00521-008-0223-1}
Gupta, H.V., Clark, M.P., Vrugt, J.A., Abramowitz, G., Ye, M., 2012. Towards a comprehensive assessment of model structural adequacy. Water Resour. Res. 48. \sphinxurl{https://doi.org/10.1029/2011WR011044}
Gupta, H.V., Sorooshian, S., Yapo, P.O., 1998. Toward improved calibration of hydrologic models: Multiple and noncommensurable measures of information. Water Resour. Res. 34, 751\textendash{}763. \sphinxurl{https://doi.org/10.1029/97WR03495}
Gupta, H.V., Wagener, T., Liu, Y., 2008. Reconciling theory with observations: elements of a diagnostic approach to model evaluation. Hydrol. Process. Int. J. 22, 3802\textendash{}3813.
Hadjimichael, A., Quinn, J., Reed, P., 2020. Advancing Diagnostic Model Evaluation to Better Understand Water Shortage Mechanisms in Institutionally Complex River Basins. Water Resour. Res. 56, e2020WR028079. \sphinxurl{https://doi.org/10.1029/2020WR028079}
Haer, T., Husby, T.G., Botzen, W.J.W., Aerts, J.C.J.H., 2020. The safe development paradox: An agent\sphinxhyphen{}based model for flood risk under climate change in the European Union. Glob Env. Change 60, 102009. \sphinxurl{https://doi.org/10.1016/j.gloenvcha.2019.102009}
Haimes, Y.Y., 2018. Risk Modeling of Interdependent Complex Systems of Systems: Theory and Practice. Risk Anal. 38, 84\textendash{}98. \sphinxurl{https://doi.org/10.1111/risa.12804}
Hall, J.W., Lempert, R.J., Keller, K., Hackbarth, A., Mijere, C., McInerney, D.J., 2012. Robust climate policies under uncertainty: A comparison of robust decision making and info\sphinxhyphen{}gap methods. Risk Anal. 32, 1657\textendash{}1672. \sphinxurl{https://doi.org/10.1111/j.1539-6924.2012.01802.x}
Hamm, N.A.S., Hall, J.W., Anderson, M.G., 2006. Variance\sphinxhyphen{}based sensitivity analysis of the probability of hydrologically induced slope instability. Comput. Geosci. 32, 803\textendash{}817. \sphinxurl{https://doi.org/10.1016/j.cageo.2005.10.007}
Han, Y., Ash, K., Mao, L., Peng, Z.\sphinxhyphen{}R., 2020. An agent\sphinxhyphen{}based model for community flood adaptation under uncertain sea\sphinxhyphen{}level rise. Clim Change. \sphinxurl{https://doi.org/10.1007/s10584-020-02802-6}
Hastings, W.K., 1970. Monte Carlo Sampling Methods Using Markov Chains and Their Applications. Biometrika 57, 97\textendash{}109. \sphinxurl{https://doi.org/10.2307/2334940}
Helbing, D., 2013. Globally networked risks and how to respond. Nature 497, 51\textendash{}59. \sphinxurl{https://doi.org/10.1038/nature12047}
Helton, J.C., Johnson, J.D., Sallaberry, C.J., Storlie, C.B., 2006. Survey of sampling\sphinxhyphen{}based methods for uncertainty and sensitivity analysis. Reliab. Eng. Syst. Saf., The Fourth International Conference on Sensitivity Analysis of Model Output (SAMO 2004) 91, 1175\textendash{}1209. \sphinxurl{https://doi.org/10.1016/j.ress.2005.11.017}
Herman, J.D., Reed, P.M., Wagener, T., 2013. Time\sphinxhyphen{}varying sensitivity analysis clarifies the effects of watershed model formulation on model behavior. Water Resour. Res. 49, 1400\textendash{}1414. \sphinxurl{https://doi.org/10.1002/wrcr.20124}
Herman, J.D., Zeff, H.B., Lamontagne, J.R., Reed, P.M., Characklis, G.W., 2016. Synthetic Drought Scenario Generation to Support Bottom\sphinxhyphen{}Up Water Supply Vulnerability Assessments. J. Water Resour. Plan. Manag. 142, 04016050. \sphinxurl{https://doi.org/10.1061/(ASCE)WR.1943-5452.0000701}
Herrera, M., Natarajan, S., Coley, D.A., Kershaw, T., Ramallo\sphinxhyphen{}González, A.P., Eames, M., Fosas, D., Wood, M., 2017. A review of current and future weather data for building simulation. Build. Serv. Eng. Res. Technol. 38, 602\textendash{}627. \sphinxurl{https://doi.org/10.1177/0143624417705937}
Hoffman, M.D., Gelman, A., 2014. The No\sphinxhyphen{}U\sphinxhyphen{}Turn Sampler: Adaptively Setting Path Lengths in Hamiltonian Monte Carlo. J Mach Learn Res 15, 1593\textendash{}1623.
Houtekamer, P.L., Mitchell, H.L., 1998. Data Assimilation Using an Ensemble Kalman Filter Technique. Mon. Weather Rev. 126, 796\textendash{}811. \sphinxurl{https://doi.org/10.1175/1520-0493(1998})126\textless{}0796:DAUAEK\textgreater{}2.0.CO;2
Hu, L., Keller, C.A., Long, M.S., Sherwen, T., Auer, B., Da Silva, A., Nielsen, J.E., Pawson, S., Thompson, M.A., Trayanov, A.L., Travis, K.R., Grange, S.K., Evans, M.J., Jacob, D.J., 2018. Global simulation of tropospheric chemistry at 12.5 km resolution: performance and evaluation of the GEOS\sphinxhyphen{}Chem chemical module (v10\sphinxhyphen{}1) within the NASA GEOS Earth system model (GEOS\sphinxhyphen{}5 ESM). Geosci Model Dev 11, 4603\textendash{}4620. \sphinxurl{https://doi.org/10.5194/gmd-11-4603-2018}
Hu, Y.\sphinxhyphen{}M., Hendry, M., Heng, I.S., 2014. Efficient Exploration of Multi\sphinxhyphen{}Modal Posterior Distributions. ArXiv Astro\sphinxhyphen{}PhIM.
Hurrell, J.W., Holland, M.M., Gent, P.R., Ghan, S., Kay, J.E., Kushner, P.J., Lamarque, J.\sphinxhyphen{}F., Large, W.G., Lawrence, D., Lindsay, K., Lipscomb, W.H., Long, M.C., Mahowald, N., Marsh, D.R., Neale, R.B., Rasch, P., Vavrus, S., Vertenstein, M., Bader, D., Collins, W.D., Hack, J.J., Kiehl, J., Marshall, S., 2013. The Community Earth System Model: A Framework for Collaborative Research. Bull Am Meteorol Soc 94, 1339\textendash{}1360. \sphinxurl{https://doi.org/10.1175/BAMS-D-12-00121.1}
Hurvich, C.M., Tsai, C.\sphinxhyphen{}L., 1989. Regression and time series model selection in small samples. Biometrika 76, 297\textendash{}307.
Iooss, B., Boussouf, L., Feuillard, V., Marrel, A., 2010. Numerical studies of the metamodel fitting and validation processes. Int. J. Adv. Syst. Meas. 3, 11\textendash{}21.
Jaynes, E.T., 2003. Probability Theory: The Logic of Science. Cambridge University Press.
Jin, R., Chen, W., Sudjianto, A., 2008. An Efficient Algorithm for Constructing Optimal Design of Computer Experiments. Presented at the ASME 2003 International Design Engineering Technical Conferences and Computers and Information in Engineering Conference, American Society of Mechanical Engineers Digital Collection, pp. 545\textendash{}554. \sphinxurl{https://doi.org/10.1115/DETC2003/DAC-48760}
Johnson, D.R., Fischbach, J.R., Ortiz, D.S., 2013. Estimating Surge\sphinxhyphen{}Based Flood Risk with the Coastal Louisiana Risk Assessment Model. J Coast Res 109\textendash{}126. \sphinxurl{https://doi.org/10.2112/si\_67\_8}
Kandlikar, M., Risbey, J., Dessai, S., 2005. Representing and communicating deep uncertainty in climate\sphinxhyphen{}change assessments. C R Geosci 337, 443\textendash{}455.
Kasmalkar, I.G., Serafin, K.A., Miao, Y., Bick, I.A., Ortolano, L., Ouyang, D., Suckale, J., 2020. When floods hit the road: Resilience to flood\sphinxhyphen{}related traffic disruption in the San Francisco Bay Area and beyond. Sci Adv 6, eaba2423. \sphinxurl{https://doi.org/10.1126/sciadv.aba2423}
Kass, R.E., Raftery, A.E., 1995. Bayes Factors. J Am Stat Assoc 90, 773\textendash{}795. \sphinxurl{https://doi.org/10.1080/01621459.1995.10476572}
Keller, K., McInerney, D., 2008. The dynamics of learning about a climate threshold. Clim Dyn 30, 321\textendash{}332. \sphinxurl{https://doi.org/10.1007/s00382-007-0290-5}
Kennedy, M.C., O’Hagan, A., 2001. Bayesian calibration of computer models. J R Stat Soc Ser. B Stat Methodol 63, 425\textendash{}464. \sphinxurl{https://doi.org/10.1111/1467-9868.00294}
Kirsch, B.R., Characklis, G.W., Zeff, H.B., 2013. Evaluating the Impact of Alternative Hydro\sphinxhyphen{}Climate Scenarios on Transfer Agreements: Practical Improvement for Generating Synthetic Streamflows. J. Water Resour. Plan. Manag. 139, 396\textendash{}406. \sphinxurl{https://doi.org/10.1061/(ASCE)WR.1943-5452.0000287}
Kleindorfer, G.B., O’Neill, L., Ganeshan, R., 1998. Validation in simulation: Various positions in the philosophy of science. Manag. Sci. 44, 1087\textendash{}1099.
Koutsoyiannis, D., 2004. Statistics of extremes and estimation of extreme rainfall: I. Theoretical investigation/Statistiques de valeurs extrêmes et estimation de précipitations extrêmes: I. Recherche théorique. Hydrol. Sci. J. 49.
Kraan, B.C., Cooke, R.M., 2000. Uncertainty in compartmental models for hazardous materials \sphinxhyphen{} a case study. J Hazard Mater 71, 253\textendash{}268. \sphinxurl{https://doi.org/10.1016/S0304-3894(99)00082-5}
Kucherenko, S., Albrecht, D., Saltelli, A., 2015. Exploring multi\sphinxhyphen{}dimensional spaces: a Comparison of Latin Hypercube and Quasi Monte Carlo Sampling Techniques. ArXiv150502350 Stat.
Kullback, S., Leibler, R.A., 1951. On Information and Sufficiency. Ann Math Stat 22, 79\textendash{}86. \sphinxurl{https://doi.org/10.1214/aoms/1177729694}
Kumar, P., 2011. Typology of hydrologic predictability. Water Resour. Res. 47. \sphinxurl{https://doi.org/10.1029/2010WR009769}
Kwakkel, J.H., Walker, W.E., Haasnoot, M., 2016. Coping with the Wickedness of Public Policy Problems: Approaches for Decision Making under Deep Uncertainty. J. Water Resour. Plan. Manag. 142, 01816001. \sphinxurl{https://doi.org/10.1061/(ASCE)WR.1943-5452.0000626}
Lamontagne, J.R., Reed, P.M., Link, R., Calvin, K.V., Clarke, L.E., Edmonds, J.A., 2018. Large Ensemble Analytic Framework for Consequence\sphinxhyphen{}Driven Discovery of Climate Change Scenarios. Earths Future 6, 488\textendash{}504. \sphinxurl{https://doi.org/10.1002/2017EF000701}
Lamontagne, J.R., Stedinger, J.R., 2018. Generating Synthetic Streamflow Forecasts with Specified Precision. J. Water Resour. Plan. Manag. 144, 04018007. \sphinxurl{https://doi.org/10.1061/(ASCE)WR.1943-5452.0000915}
Landeg, O., Whitman, G., Walker\sphinxhyphen{}Springett, K., Butler, C., Bone, A., Kovats, S., 2019. Coastal flooding and frontline health care services: challenges for flood risk resilience in the English health care system. J Health Serv Res Policy 24, 219\textendash{}228. \sphinxurl{https://doi.org/10.1177/1355819619840672}
Lee, B.S., Haran, M., Fuller, R., Pollard, D., Keller, K., 2020. A Fast Particle\sphinxhyphen{}Based Approach for Calibrating a 3\sphinxhyphen{}D Model of the Antarctic Ice Sheet. Ann. Appl. Stat. in the press.
Lee, B.S., Haran, M., Keller, K., 2017. Multidecadal Scale Detection Time for Potentially Increasing Atlantic Storm Surges in a Warming Climate. Geophys Res Lett 44, 10,617\sphinxhyphen{}10,623. \sphinxurl{https://doi.org/10.1002/2017GL074606}
Lempert, R.J., 2002. A new decision sciences for complex systems. Proc. Natl. Acad. Sci. 99, 7309\textendash{}7313. \sphinxurl{https://doi.org/10.1073/pnas.082081699}
Liu, J., West, M., 2001. Combined Parameter and State Estimation in Simulation\sphinxhyphen{}Based Filtering, in: Doucet, A., de Freitas, N., Gordon, N. (Eds.), Sequential Monte Carlo Methods in Practice. Springer New York, New York, NY, pp. 197\textendash{}223. \sphinxurl{https://doi.org/10.1007/978-1-4757-3437-9\_10}
Liu, X., Guillas, S., 2017. Dimension Reduction for Gaussian Process Emulation: An Application to the Influence of Bathymetry on Tsunami Heights. SIAMASA J Uncertain. Quantif. 5, 787\textendash{}812. \sphinxurl{https://doi.org/10.1137/16M1090648}
Lorenz, E.N., 1963. Deterministic Nonperiodic Flow. J Atmos Sci 20, 130\textendash{}141. \sphinxurl{https://doi.org/10.1175/1520-0469(1963})020\textless{}0130:DNF\textgreater{}2.0.CO;2
Loucks, D.P., Beek, E. van, 2017. Water Resource Systems Planning and Management: An Introduction to Methods, Models, and Applications. Springer International Publishing.
Lukacs, P.M., Burnham, K.P., Anderson, D.R., 2010. Model selection bias and Freedman’s paradox. Ann Inst Stat Math 62, 117\textendash{}125. \sphinxurl{https://doi.org/10.1007/s10463-009-0234-4}
Makowski, D., Wallach, D., Tremblay, M., 2002. Using a Bayesian approach to parameter estimation; comparison of the GLUE and MCMC methods. Agronomie 22, 191\textendash{}203.
Maniyar, D.M., Cornford, D., Boukouvalas, A., 2007. Dimensionality Reduction in the Emulator Setting (No. NCRG/2007/005). Neural Computing Research Group.
Marchau, V.A.W.J., Walker, W.E., Bloemen, P.J.T.M., Popper, S.W. (Eds.), 2019. Decision Making under Deep Uncertainty: From Theory to Practice. Springer International Publishing. \sphinxurl{https://doi.org/10.1007/978-3-030-05252-2}
Martin, A.D., 2010. MCMCpack: Markov chain Monte Carlo (MCMC) Package. R package version 1.0\sphinxhyphen{}7. HttpCRAN R\sphinxhyphen{}Proj. Orgpackage MCMCpack.
Massmann, C., Wagener, T., Holzmann, H., 2014. A new approach to visualizing time\sphinxhyphen{}varying sensitivity indices for environmental model diagnostics across evaluation time\sphinxhyphen{}scales. Environ. Model. Softw. 51, 190\textendash{}194. \sphinxurl{https://doi.org/10.1016/j.envsoft.2013.09.033}
Matthews, P., 1993. A slowly mixing Markov chain with implications for Gibbs sampling. Stat Probab Lett 17, 231\textendash{}236. \sphinxurl{https://doi.org/10.1016/0167-7152(93)90172-F}
McInerney, D., Keller, K., 2008. Economically optimal risk reduction strategies in the face of uncertain climate thresholds. Clim Change 91, 29\textendash{}41. \sphinxurl{https://doi.org/10.1007/s10584-006-9137-z}
McKay, M.D., Beckman, R.J., Conover, W.J., 1979. A Comparison of Three Methods for Selecting Values of Input Variables in the Analysis of Output from a Computer Code. Technometrics 21, 239\textendash{}245. \sphinxurl{https://doi.org/10.2307/1268522}
Medda, S., Bhar, K.K., 2019. Comparison of single\sphinxhyphen{}site and multi\sphinxhyphen{}site stochastic models for streamflow generation. Appl. Water Sci. 9, 67.
Metropolis, N., Rosenbluth, A.W., Rosenbluth, M.N., Teller, A.H., Teller, E., 1953. Equation of State Calculations by Fast Computing Machines. J Chem Phys 21, 1087\textendash{}1092. \sphinxurl{https://doi.org/10.1063/1.1699114}
Metropolis, N., Ulam, S., 1949. The Monte Carlo Method. J. Am. Stat. Assoc. 44, 335\textendash{}341. \sphinxurl{https://doi.org/10.1080/01621459.1949.10483310}
Milly, P.C.D., Betancourt, J., Falkenmark, M., Hirsch, R.M., Kundzewicz, Z.W., Lettenmaier, D.P., Stouffer, R.J., 2008. Stationarity is dead: Whither water management? Science 319, 573\textendash{}574. \sphinxurl{https://doi.org/10.1126/science.1151915}
Moallemi, E.A., Kwakkel, J., de Haan, F.J., Bryan, B.A., 2020a. Exploratory modeling for analyzing coupled human\sphinxhyphen{}natural systems under uncertainty. Glob. Environ. Change 65, 102186. \sphinxurl{https://doi.org/10.1016/j.gloenvcha.2020.102186}
Moallemi, E.A., Zare, F., Reed, P.M., Elsawah, S., Ryan, M.J., Bryan, B.A., 2020b. Structuring and evaluating decision support processes to enhance the robustness of complex human\textendash{}natural systems. Environ. Model. Softw. 123, 104551. \sphinxurl{https://doi.org/10.1016/j.envsoft.2019.104551}
Montgomery, D.C., 2017. Design and analysis of experiments. John Wiley \& Sons.
Morgan, M.G., Keith, D.W., 2008. Improving the way we think about projecting future energy use and emissions of carbon dioxide. Clim Change 90, 189\textendash{}215. \sphinxurl{https://doi.org/10.1007/s10584-008-9458-1}
Morris, M.D., 1991. Factorial Sampling Plans for Preliminary Computational Experiments. Technometrics 33, 161\textendash{}174. \sphinxurl{https://doi.org/10.1080/00401706.1991.10484804}
Morris, M.D., Mitchell, T.J., 1995. Exploratory designs for computational experiments. J. Stat. Plan. Inference 43, 381\textendash{}402. \sphinxurl{https://doi.org/10.1016/0378-3758(94)00035-T}
National Research Council, 2014. Convergence: facilitating transdisciplinary integration of life sciences, physical sciences, engineering, and beyond. National Academies Press, Washington, D.C., USA.
Naylor, T.H., Finger, J.M., 1967. Verification of computer simulation models. Manag. Sci. 14, B\sphinxhyphen{}92.
Neal, R.M., 2011. MCMC Using Hamiltonian Dynamics, in: Brooks, S., Gelman, A., Jones, G.L., Meng, X.\sphinxhyphen{}L. (Eds.), Handbook of Markov Chain Monte Carlo, Handbooks of Modern Statistical Methods. CRC Press, Boca Raton, FL, pp. 113\textendash{}162.
Nearing, G.S., Ruddell, B.L., Bennett, A.R., Prieto, C., Gupta, H.V., 2020. Does Information Theory Provide a New Paradigm for Earth Science? Hypothesis Testing. Water Resour. Res. 56, e2019WR024918. \sphinxurl{https://doi.org/10.1029/2019WR024918}
Nelder, J.A., Wedderburn, R.W.M., 1972. Generalized Linear Models. J R Stat Soc Ser A 135, 370\textendash{}384. \sphinxurl{https://doi.org/10.2307/2344614}
Norton, J., 2015. An introduction to sensitivity assessment of simulation models. Environ. Model. Softw. 69, 166\textendash{}174. \sphinxurl{https://doi.org/10.1016/j.envsoft.2015.03.020}
Oddo, P.C., Lee, B.S., Garner, G.G., Srikrishnan, V., Reed, P.M., Forest, C.E., Keller, K., 2017. Deep Uncertainties in Sea\sphinxhyphen{}Level Rise and Storm Surge Projections: Implications for Coastal Flood Risk Management. Risk Anal. \sphinxurl{https://doi.org/10.1111/risa.12888}
O’Neill, B.C., Crutzen, P., Grübler, A., Duong, M.H., Keller, K., Kolstad, C., Koomey, J., Lange, A., Obersteiner, M., Oppenheimer, M., Pepper, W., Sanderson, W., Schlesinger, M., Treich, N., Ulph, A., Webster, M., Wilson, C., 2006. Learning and climate change. Clim Policy 6, 585\textendash{}589. \sphinxurl{https://doi.org/10.1080/14693062.2006.9685623}
Oppenheimer, M., O’Neill, B.C., Webster, M., 2008. Negative learning. Clim Change 89, 155\textendash{}172. \sphinxurl{https://doi.org/10.1007/s10584-008-9405-1}
Oreskes, N., Shrader\sphinxhyphen{}Frechette, K., Belitz, K., 1994. Verification, Validation, and Confirmation of Numerical Models in the Earth Sciences. Science 263, 641\textendash{}646. \sphinxurl{https://doi.org/10.1126/science.263.5147.641}
Park, J.\sphinxhyphen{}S., 1994. Optimal Latin\sphinxhyphen{}hypercube designs for computer experiments. J. Stat. Plan. Inference 39, 95\textendash{}111. \sphinxurl{https://doi.org/10.1016/0378-3758(94)90115-5}
Pianosi, F., Beven, K., Freer, J., Hall, J.W., Rougier, J., Stephenson, D.B., Wagener, T., 2016. Sensitivity analysis of environmental models: A systematic review with practical workflow. Environ. Model. Softw. 79, 214\textendash{}232. \sphinxurl{https://doi.org/10.1016/j.envsoft.2016.02.008}
Pollard, D., DeConto, R.M., 2012. Description of a hybrid ice sheet\sphinxhyphen{}shelf model, and application to Antarctica. Geosci. Model Dev. 5, 1273\textendash{}1295. \sphinxurl{https://doi.org/10.5194/gmd-5-1273-2012}
Pruett, W.A., Hester, R.L., 2016. The Creation of Surrogate Models for Fast Estimation of Complex Model Outcomes. PLOS ONE 11, e0156574. \sphinxurl{https://doi.org/10.1371/journal.pone.0156574}
Ragulina, G., Reitan, T., 2017. Generalized extreme value shape parameter and its nature for extreme precipitation using long time series and the Bayesian approach. Hydrol. Sci. J. 62, 863\textendash{}879.
Rakovec, O., Hill, M.C., Clark, M.P., Weerts, A.H., Teuling, A.J., Uijlenhoet, R., 2014. Distributed Evaluation of Local Sensitivity Analysis (DELSA), with application to hydrologic models. Water Resour. Res. 50, 409\textendash{}426. \sphinxurl{https://doi.org/10.1002/2013WR014063}
Rasmussen, D.J., Buchanan, M.K., Kopp, R.E., Oppenheimer, M., 2020. A flood damage allowance framework for coastal protection with deep uncertainty in sea‐level rise. Earths Future. \sphinxurl{https://doi.org/10.1029/2019EF001340}
Robert, C.P., 2007. The Bayesian choice: from decision\sphinxhyphen{}theoretic foundations to computational implementation, 2nd ed. Springer, New York.
Rohmer, J., Le Cozannet, G., Manceau, J.\sphinxhyphen{}C., 2019. Addressing ambiguity in probabilistic assessments of future coastal flooding using possibility distributions. Clim Change. \sphinxurl{https://doi.org/10.1007/s10584-019-02443-4}
Ruckert, K.L., Shaffer, G., Pollard, D., Guan, Y., Wong, T.E., Forest, C.E., Keller, K., 2017. Assessing the Impact of Retreat Mechanisms in a Simple Antarctic Ice Sheet Model Using Bayesian Calibration. PLoS One 12, e0170052. \sphinxurl{https://doi.org/10.1371/journal.pone.0170052}
Ruckert, K.L., Srikrishnan, V., Keller, K., 2019. Characterizing the deep uncertainties surrounding coastal flood hazard projections: A case study for Norfolk, VA. Sci Rep 9, 11373. \sphinxurl{https://doi.org/10.1038/s41598-019-47587-6}
Sacks, J., Welch, W.J., Mitchell, T.J., Wynn, H.P., 1989. Design and Analysis of Computer Experiments. Stat Sci 4, 409\textendash{}423.
Saltelli, A., 2002. Making best use of model evaluations to compute sensitivity indices. Comput. Phys. Commun. 145, 280\textendash{}297.
Saltelli, A., Aleksankina, K., Becker, W., Fennell, P., Ferretti, F., Holst, N., Li, S., Wu, Q., 2019. Why so many published sensitivity analyses are false: A systematic review of sensitivity analysis practices. Environ. Model. Softw. 114, 29\textendash{}39. \sphinxurl{https://doi.org/10.1016/j.envsoft.2019.01.012}
Saltelli, A., Annoni, P., 2010. How to avoid a perfunctory sensitivity analysis. Environ. Model. Softw. 25, 1508\textendash{}1517. \sphinxurl{https://doi.org/10.1016/j.envsoft.2010.04.012}
Saltelli, A., Funtowicz, S., 2014. When all models are wrong. Issues Sci. Technol. 30, 79\textendash{}85.
Saltelli, A., Ratto, M., Andres, T., Campolongo, F., Cariboni, J., Gatelli, D., Saisana, M., Tarantola, S., 2008. Global Sensitivity Analysis: The Primer, 1 edition. ed. Wiley\sphinxhyphen{}Interscience, Chichester, England; Hoboken, NJ.
Saltelli, A., Stark, P.B., Becker, W., Stano, P., 2015. Climate models as economic guides scientific challenge or quixotic quest? Issues Sci. Technol. 31, 79\textendash{}84.
Saltelli, A., Tarantola, S., 2002. On the relative importance of input factors in mathematical models: safety assessment for nuclear waste disposal. J. Am. Stat. Assoc. 97, 702\textendash{}709.
Saltelli, A., Tarantola, S., Campolongo, F., Ratto, M., 2004. Sensitivity Analysis in Practice: A Guide to Assessing Scientific Models. John Wiley \& Sons.
Saltelli, A., Tarantola, S., Chan, K.P.\sphinxhyphen{}S., 1999. A Quantitative Model\sphinxhyphen{}Independent Method for Global Sensitivity Analysis of Model Output. Technometrics 41, 39\textendash{}56. \sphinxurl{https://doi.org/10.1080/00401706.1999.10485594}
Schlather, M., 2002. Models for stationary max\sphinxhyphen{}stable random fields. Extremes 5, 33\textendash{}44.
Schwarz, G., 1978. Estimating the Dimension of a Model. Ann Stat 6, 461\textendash{}464. \sphinxurl{https://doi.org/10.1214/aos/1176344136}
Shafii, M., Tolson, B., Matott, L.S., 2014. Uncertainty\sphinxhyphen{}based multi\sphinxhyphen{}criteria calibration of rainfall\sphinxhyphen{}runoff models: a comparative study. Stoch Env. Res Risk Assess 28, 1493\textendash{}1510. \sphinxurl{https://doi.org/10.1007/s00477-014-0855-x}
Shibata, R., 1989. Statistical Aspects of Model Selection, in: Willems, J.C. (Ed.), From Data to Model. Springer Berlin Heidelberg, Berlin, Heidelberg, pp. 215\textendash{}240. \sphinxurl{https://doi.org/10.1007/978-3-642-75007-6\_5}
Sibley Heather M., Vroman Noah D., Shewbridge Scott E., 2017. Quantitative Risk\sphinxhyphen{}Informed Design of Levees. Geo\sphinxhyphen{}Risk 2017 76\textendash{}90. \sphinxurl{https://doi.org/10.1061/9780784480717.008}
Sminchisescu, C., Welling, M., Hinton, G., 2003. A mode\sphinxhyphen{}hopping MCMC sampler. Technical Report CSRG\sphinxhyphen{}478, University of Toronto, submitted to Machine ….
Sobol, I.M., 2001. Global sensitivity indices for nonlinear mathematical models and their Monte Carlo estimates. Math. Comput. Simul. 55, 271\textendash{}280. \sphinxurl{https://doi.org/10.1016/S0378-4754(00)00270-6}
Sobol, I.M., 1976. Uniformly distributed sequences with an additional uniform property. USSR Comput. Math. Math. Phys. 16, 236\textendash{}242.
Sobol’, I.M., 1967. On the distribution of points in a cube and the approximate evaluation of integrals. USSR Comput. Math. Math. Phys. 7, 86\textendash{}112. \sphinxurl{https://doi.org/10.1016/0041-5553(67)90144-9}
Spear, R.C., Hornberger, G.M., 1980. Eutrophication in peel inlet—II. Identification of critical uncertainties via generalized sensitivity analysis. Water Res. 14, 43\textendash{}49. \sphinxurl{https://doi.org/10.1016/0043-1354(80)90040-8}
Spiegelhalter, D.J., Best, N.G., Carlin, B.P., Van Der Linde, A., 2002. Bayesian measures of model complexity and fit. J R Stat Soc Ser. B Stat Methodol 64, 583\textendash{}639. \sphinxurl{https://doi.org/10.1111/1467-9868.00353}
Srikrishnan, V, Alley, R.B., Keller, K., 2019. Investing in Science and Using the Results to Improve Climate Risk Management. EOS.
Srikrishnan, Vivek, Guan, Y., Tol, R.S.J., Keller, K., 2019. Fossil fuel resources, decarbonization, and economic growth drive the feasibility of Paris climate targets. ArXiv StatAP.
Sriver, R.L., Lempert, R.J., Wikman\sphinxhyphen{}Svahn, P., Keller, K., 2018. Characterizing uncertain sea\sphinxhyphen{}level rise projections to support investment decisions. PLoS One 13, e0190641. \sphinxurl{https://doi.org/10.1371/journal.pone.0190641}
Stedinger, J.R., Vogel, R.M., Lee, S.U., Batchelder, R., 2008. Appraisal of the generalized likelihood uncertainty estimation (GLUE) method: APPRAISAL OF THE GLUE METHOD. Water Resour Res 44, 191. \sphinxurl{https://doi.org/10.1029/2008WR006822}
Steinschneider, S., Wi, S., Brown, C., 2015. The integrated effects of climate and hydrologic uncertainty on future flood risk assessments. Hydrol. Process. 29, 2823\textendash{}2839. \sphinxurl{https://doi.org/10.1002/hyp.10409}
Stone, M., 1977. An Asymptotic Equivalence of Choice of Model by Cross\sphinxhyphen{}Validation and Akaike’s Criterion. J R Stat Soc Ser. B Stat Methodol 39, 44\textendash{}47.
Surowiec, I., Vikström, L., Hector, G., Johansson, E., Vikström, C., Trygg, J., 2017. Generalized Subset Designs in Analytical Chemistry. Anal. Chem. 89, 6491\textendash{}6497. \sphinxurl{https://doi.org/10.1021/acs.analchem.7b00506}
Tang, B., 1993. Orthogonal Array\sphinxhyphen{}Based Latin Hypercubes. J. Am. Stat. Assoc. 88, 1392\textendash{}1397. \sphinxurl{https://doi.org/10.1080/01621459.1993.10476423}
Tang, Y., Reed, P., Wagener, T., Werkhoven, K. van, 2007. Comparing sensitivity analysis methods to advance lumped watershed model identification and evaluation. Hydrol. Earth Syst. Sci. 11, 793\textendash{}817. \sphinxurl{https://doi.org/10.5194/hess-11-793-2007}
Toulmin, S., 1977. From form to function: philosophy and history of science in the 1950s and now. Daedalus 143\textendash{}162.
Urban, N.M., Keller, K., 2010. Probabilistic hindcasts and projections of the coupled climate, carbon cycle and Atlantic meridional overturning circulation system: A Bayesian fusion of century\sphinxhyphen{}scale observations with a simple model. Tellus A 62, 737\textendash{}750.
Van Oort, N., 2011. Service reliability and urban public transport design.
Van Schepdael, A., Carlier, A., Geris, L., 2016. Sensitivity Analysis by Design of Experiments, in: Geris, L., Gomez\sphinxhyphen{}Cabrero, D. (Eds.), Uncertainty in Biology: A Computational Modeling Approach, Studies in Mechanobiology, Tissue Engineering and Biomaterials. Springer International Publishing, Cham, pp. 327\textendash{}366. \sphinxurl{https://doi.org/10.1007/978-3-319-21296-8\_13}
van Vuuren, D.P., Edmonds, J., Kainuma, M., Riahi, K., Thomson, A., Hibbard, K., Hurtt, G.C., Kram, T., Krey, V., Lamarque, J.\sphinxhyphen{}F., Masui, T., Meinshausen, M., Nakicenovic, N., Smith, S.J., Rose, S.K., 2011. The representative concentration pathways: an overview. Clim Change 109, 5. \sphinxurl{https://doi.org/10.1007/s10584-011-0148-z}
Vega‐Westhoff, B., Sriver, R.L., Hartin, C.A., Wong, T.E., Keller, K., 2019. Impacts of Observational Constraints Related to Sea Level on Estimates of Climate Sensitivity. Earths Future. \sphinxurl{https://doi.org/10.1029/2018EF001082}
Vehtari, A., Gelman, A., Gabry, J., 2017. Practical Bayesian model evaluation using leave\sphinxhyphen{}one\sphinxhyphen{}out cross\sphinxhyphen{}validation and WAIC. Stat Comput 27, 1413\textendash{}1432.
Vihola, M., 2012. Robust adaptive Metropolis algorithm with coerced acceptance rate. Stat Comput 22, 997\textendash{}1008. \sphinxurl{https://doi.org/10.1007/s11222-011-9269-5}
Vogel, R.M., 2017. Stochastic watershed models for hydrologic risk management. Water Secur. 1, 28\textendash{}35. \sphinxurl{https://doi.org/10.1016/j.wasec.2017.06.001}
Vogel, R.M., Stedinger, J.R., 1988. The value of stochastic streamflow models in overyear reservoir design applications. Water Resour. Res. 24, 1483\textendash{}1490. \sphinxurl{https://doi.org/10.1029/WR024i009p01483}
Vousdoukas, M.I., Bouziotas, D., Giardino, A., Bouwer, L.M., Mentaschi, L., Voukouvalas, E., Feyen, L., 2018. Understanding epistemic uncertainty in large\sphinxhyphen{}scale coastal flood risk assessment for present and future climates. Nat Hazards Earth Syst Sci 18, 2127\textendash{}2142. \sphinxurl{https://doi.org/10.5194/nhess-18-2127-2018}
Vrugt, J.A., Beven, K.J., 2018. Embracing equifinality with efficiency: Limits of Acceptability sampling using the DREAM(LOA) algorithm. J Hydrol 559, 954\textendash{}971. \sphinxurl{https://doi.org/10.1016/j.jhydrol.2018.02.026}
Walker, W., Haasnoot, M., Kwakkel, J., Walker, W.E., Haasnoot, M., Kwakkel, J.H., 2013. Adapt or Perish: A Review of Planning Approaches for Adaptation under Deep Uncertainty. Sustainability 5, 955\textendash{}979. \sphinxurl{https://doi.org/10.3390/su5030955}
Walker, W.E., Harremoës, P., Rotmans, J., Sluijs, J.P. van der, Asselt, M.B.A. van, Janssen, P., Krauss, M.P.K. von, 2003. Defining Uncertainty: A Conceptual Basis for Uncertainty Management in Model\sphinxhyphen{}Based Decision Support. Integr. Assess. 4, 5\textendash{}17. \sphinxurl{https://doi.org/10.1076/iaij.4.1.5.16466}
Walker, W.E., Lempert, R.J., Kwakkel, J.H., 2013. Deep Uncertainty, in: Gass, S.I., Fu, M.C. (Eds.), Encyclopedia of Operations Research and Management Science. Springer US, pp. 395\textendash{}402. \sphinxurl{https://doi.org/10.1007/978-1-4419-1153-7\_1140}
Watanabe, S., 2010. Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory. J Mach Learn Res 11, 3571\textendash{}3594.
Weaver, C.P., Lempert, R.J., Brown, C., Hall, J.A., Revell, D., Sarewitz, D., 2013. Improving the contribution of climate model information to decision making: the value and demands of robust decision frameworks. Wiley Interdiscip. Rev. Clim. Change 4, 39\textendash{}60. \sphinxurl{https://doi.org/10.1002/wcc.202}
White, D.D., Wutich, A., Larson, K.L., Gober, P., Lant, T., Senneville, C., 2010. Credibility, salience, and legitimacy of boundary objects: water managers’ assessment of a simulation model in an immersive decision theater. Sci. Public Policy 37, 219\textendash{}232. \sphinxurl{https://doi.org/10.3152/030234210X497726}
Wilks, D.S., Wilby, R.L., 1999. The weather generation game: a review of stochastic weather models. Prog. Phys. Geogr. 23, 329\textendash{}357.
Wirtz, D., Nowak, W., 2017. The rocky road to extended simulation frameworks covering uncertainty, inversion, optimization and control. Environ. Model. Softw. 93, 180\textendash{}192. \sphinxurl{https://doi.org/10.1016/j.envsoft.2016.10.003}
Wolff, C., Nikoletopoulos, T., Hinkel, J., Vafeidis, A.T., 2020. Future urban development exacerbates coastal exposure in the Mediterranean. Sci Rep 10, 14420. \sphinxurl{https://doi.org/10.1038/s41598-020-70928-9}
Wong, T.E., Keller, K., 2017. Deep Uncertainty Surrounding Coastal Flood Risk Projections: A Case Study for New Orleans. Earths Future 5, 1015\textendash{}1026. \sphinxurl{https://doi.org/10.1002/2017EF000607}
Wong, T.E., Klufas, A., Srikrishnan, V., Keller, K., 2018. Neglecting model structural uncertainty underestimates upper tails of flood hazard. Env. Res Lett 13, 074019. \sphinxurl{https://doi.org/10.1088/1748-9326/aacb3d}
Xian, S., Lin, N., Kunreuther, H., 2017. Optimal house elevation for reducing flood\sphinxhyphen{}related losses. J Hydrol 548, 63\textendash{}74. \sphinxurl{https://doi.org/10.1016/j.jhydrol.2017.02.057}
Xiu, D., Karniadakis, G.E., 2002. The Wiener\textendash{}Askey Polynomial Chaos for Stochastic Differential Equations. SIAM J. Sci. Comput. 24, 619\textendash{}644. \sphinxurl{https://doi.org/10.1137/S1064827501387826}
Zarekarizi, M., Srikrishnan, V., Keller, K., 2020. Neglecting Uncertainties Biases House\sphinxhyphen{}Elevation Decisions to Manage Riverine Flood Risks. Nat Commun. \sphinxurl{https://doi.org/10.1038/s41467-020-19188-9}
Zaremba, S.K., 1968. The mathematical basis of Monte Carlo and quasi\sphinxhyphen{}Monte Carlo methods. SIAM Rev. 10, 303\textendash{}314.
Zellner, A., Tian, G.C., 1964. Bayesian analysis of the regression model with autocorrelated errors. J Am Stat Assoc 763\textendash{}778.
Zhang, X., Che, L., Shahidehpour, M., Alabdulwahab, A.S., Abusorrah, A., 2015. Reliability\sphinxhyphen{}based optimal planning of electricity and natural gas interconnections for multiple energy hubs. IEEE Trans Smart Grid 8, 1658\textendash{}1667.
Zscheischler, J., Martius, O., Westra, S., Bevacqua, E., Raymond, C., Horton, R.M., van den Hurk, B., AghaKouchak, A., Jézéquel, A., Mahecha, M.D., Maraun, D., Ramos, A.M., Ridder, N.N., Thiery, W., Vignotto, E., 2020. A typology of compound weather and climate events. Nat. Rev. Earth Environ. 1\textendash{}15. \sphinxurl{https://doi.org/10.1038/s43017-020-0060-z}


\chapter{Glossary}
\label{\detokenize{9_glossary:glossary}}\label{\detokenize{9_glossary::doc}}

\chapter{Indices and tables}
\label{\detokenize{index:indices-and-tables}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{genindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{modindex}

\item {} 
\sphinxAtStartPar
\DUrole{xref,std,std-ref}{search}

\end{itemize}



\renewcommand{\indexname}{Index}
\printindex
\end{document}