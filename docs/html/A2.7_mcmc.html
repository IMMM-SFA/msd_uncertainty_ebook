
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Model Calibration with Markov chain Monte Carlo Tutorial &#8212; Addressing Uncertainty in MultiSector Dynamics Research  documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=0a66277c" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>window.MathJax = {"tex": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true}, "options": {"ignoreHtmlClass": "tex2jax_ignore|mathjax_ignore|document", "processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'A2.7_mcmc';</script>
    <script src="_static/custom.js?v=d380ae45"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Z9WMRS23CZ"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-Z9WMRS23CZ');
</script>

  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
  
    <p class="title logo__title">Addressing Uncertainty in MultiSector Dynamics Research  documentation</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface.html">Preface</a></li>
<li class="toctree-l1"><a class="reference internal" href="citation.html">Suggested Citation</a></li>
<li class="toctree-l1"><a class="reference internal" href="acknowledgements.html">Acknowledgements</a></li>
<li class="toctree-l1"><a class="reference internal" href="code_of_conduct.html">Code of Conduct</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contribution Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Contents</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_introduction.html">1. Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="2_diagnostic_modeling_overview_and_perspectives.html">2. Diagnostic Modeling Overview and Perspectives</a></li>
<li class="toctree-l1"><a class="reference internal" href="3_sensitivity_analysis_the_basics.html">3. Sensitivity Analysis: The Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_sensitivity_analysis_diagnostic_and_exploratory_modeling.html">4. Sensitivity Analysis: Diagnostic &amp; Exploratory Modeling</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_conclusion.html">5. Conclusion</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="A1_Uncertainty_Quantification.html">A. Uncertainty Quantification</a></li>
<li class="toctree-l1"><a class="reference internal" href="A2_Jupyter_Notebooks.html">B. Jupyter Notebook Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="A3_plotting_code.html">C. Plotting Code Samples</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Glossary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="6_glossary.html">Glossary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">References</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="R.Bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/IMMM-SFA/msd_uncertainty_ebook" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/IMMM-SFA/msd_uncertainty_ebook/issues/new?title=Issue%20on%20page%20%2FA2.7_mcmc.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/A2.7_mcmc.rst" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.rst</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Calibration with Markov chain Monte Carlo Tutorial</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-calibration-with-markov-chain-monte-carlo">Model Calibration with Markov chain Monte Carlo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#note-if-you-are-running-this-notebook-locally-uncomment-the-following-code-to-install-the-required-package-data"><strong>NOTE:</strong> If you are running this notebook locally, uncomment the following code to install the required package data:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mcmc-for-model-calibration-and-uncertainty-quantification">MCMC for Model Calibration and Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uncertainty-quantification">Bayesian Uncertainty Quantification</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rejection-sampling">Rejection Sampling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo">Markov chain Monte Carlo</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hymod-calibration">HYMOD Calibration</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distributions">Prior Distributions</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings">Metropolis-Hastings</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-next-steps">Challenges and Next Steps</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-for-using-mcmc">Tips for Using MCMC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="model-calibration-with-markov-chain-monte-carlo-tutorial">
<h1>Model Calibration with Markov chain Monte Carlo Tutorial<a class="headerlink" href="#model-calibration-with-markov-chain-monte-carlo-tutorial" title="Link to this heading">#</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<div class="line-block">
<div class="line">Run the tutorial interactively:  <a class="reference external" href="https://uc-ebook.msdlive.org/user-redirect/lab/tree/notebooks/mcmc.ipynb">MCMC Notebook</a>.</div>
<div class="line">Please be aware that notebooks can take a couple minutes to launch.</div>
<div class="line">To run the notebooks yourself, download the files <a class="reference external" href="https://github.com/IMMM-SFA/msd_uncertainty_ebook/tree/main/notebooks">here</a> and use these <a class="reference external" href="https://github.com/IMMM-SFA/msd_uncertainty_ebook/blob/main/pyproject.toml">requirements</a>.</div>
</div>
</div>
<div class="admonition-community-contribution admonition">
<p class="admonition-title">Community Contribution</p>
<div class="line-block">
<div class="line">This tutorial was contributed by the community. Use the citation below in additional to the main citation when referencing this code:</div>
<div class="line"><br /></div>
<div class="line"><em>Srikrishnan, V. (2025). Model Calibration with Markov Chain Monte Carlo Tutorial (v1.0.0). MSD-LIVE Data Repository. https://doi.org/10.57931/2565322</em></div>
</div>
</div>
<section id="model-calibration-with-markov-chain-monte-carlo">
<h2>Model Calibration with Markov chain Monte Carlo<a class="headerlink" href="#model-calibration-with-markov-chain-monte-carlo" title="Link to this heading">#</a></h2>
<p>The purpose of this tutorial is to demonstrate how to use Markov chain
Monte Carlo (MCMC) to calibrate a model. By calibration, we mean the
selection of model parameters (and, when relevant, structures). This
tutorial notebook will build on the HYMOD sensitivity analysis notebook.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">math</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">random</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">msdbook</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">msdbook.hymod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">msdbook.package_data</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_hymod_input_file</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set seed for reproducibility</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<section id="note-if-you-are-running-this-notebook-locally-uncomment-the-following-code-to-install-the-required-package-data">
<h3><strong>NOTE:</strong> If you are running this notebook locally, uncomment the following code to install the required package data:<a class="headerlink" href="#note-if-you-are-running-this-notebook-locally-uncomment-the-following-code-to-install-the-required-package-data" title="Link to this heading">#</a></h3>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># from msdbook.install_supplement import install_package_data</span>
<span class="c1"># install_package_data()</span>
</pre></div>
</div>
<section id="mcmc-for-model-calibration-and-uncertainty-quantification">
<h4>MCMC for Model Calibration and Uncertainty Quantification<a class="headerlink" href="#mcmc-for-model-calibration-and-uncertainty-quantification" title="Link to this heading">#</a></h4>
<section id="bayesian-uncertainty-quantification">
<h5>Bayesian Uncertainty Quantification<a class="headerlink" href="#bayesian-uncertainty-quantification" title="Link to this heading">#</a></h5>
<p>A common goal in model development and diagnostics is <em>calibration</em>, or
the identification of model structures and parameters which are
consistent with data. While models can be calibrated through hand-tuning
parameters or minimizing simple error metrics such as
root-mean-square-error (RMSE), these approaches can underrepresent the
probabilistic nature of the data-generating process, as well as the
potential for multiple model configurations to be consistent with the
data. Probabilistic uncertainty quantification, which is the topic of
this notebook, can address these concerns.</p>
<p>The notion that different model configurations can be consistent with
data to different degrees is related to the Bayesian interpretation of
probability as representing the degree of belief in an outcome. Bayesian
uncertainty quantification has two characteristics:</p>
<ol class="arabic">
<li><p>Obtaining probability distributions over model structures and/or
parameter values <span class="math notranslate nohighlight">\(\theta\)</span> reflecting consistency with prior
beliefs <span class="math notranslate nohighlight">\(p(\theta)\)</span> and data <span class="math notranslate nohighlight">\(y\)</span>. These probabilities
represent the <em>posterior</em> probability emerging from Bayes’ Rule,</p>
<div class="math notranslate nohighlight">
\[p(\theta | y) \propto p(y |\theta) p(\theta),\]</div>
<p>where <span class="math notranslate nohighlight">\(p(y | \theta)\)</span> is the <em>likelihood</em> of seeing the data
given the parameterization <span class="math notranslate nohighlight">\(\theta\)</span>. The likelihood captures
the probability model by which the data is observed and can include
bias terms, observation errors, or other influences.</p>
</li>
<li><p>The use of prior distributions. Priors are classically thought of as
means to express beliefs about admissible or plausible values, but
they can also be used to limit the degree of overfitting by requiring
more data to force more extreme parameter estimates.</p></li>
</ol>
<p>The fundamental challenge is that of sampling from the posterior
probability distribution, which may not have a nice representation. MCMC
is one family of approaches to solving that challenge.</p>
</section>
<section id="rejection-sampling">
<h5>Rejection Sampling<a class="headerlink" href="#rejection-sampling" title="Link to this heading">#</a></h5>
<p>Many methods for drawing samples (or simulating) from “non-standard”
distributions rely on an accept-reject approach, where samples are
generated from an easier-to-simulate <em>proposal</em> distribution and are
probabilistically accepted or rejected based on the relative probability
density between the proposal and the target distribution.</p>
<p>To illustrate this accept-reject concept, we can use an algorithm called
<em>rejection sampling</em>. Rejection sampling involves simulation of
independent and identically-distributed (i.i.d.) samples from a proposal
distribution which “covers” the target distribution. More specifically,
let <span class="math notranslate nohighlight">\(\pi(x)\)</span> be the (known) density of the target distribution,
and let <span class="math notranslate nohighlight">\(g(x)\)</span> be a density which satisfies
<span class="math notranslate nohighlight">\(\pi(x) &lt; M g(x)\)</span> where <span class="math notranslate nohighlight">\(1 &lt; M &lt; \infty\)</span>; in other words.
This covering property is essential to ensure that values are proposed
across the entire range of <span class="math notranslate nohighlight">\(\pi\)</span> with positive probability (called
the <em>support</em>). We can see an example of this in the figure below.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the target mixture model pdf.</span>
<span class="c1"># This represents a 50/50 mixture of N(-1, 0.75) and N(1, 0.4).</span>
<span class="k">def</span><span class="w"> </span><span class="nf">mixture_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.75</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.4</span><span class="p">)</span>

<span class="c1"># Create an array of x values from -5 to 5 with a step of 0.01.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># Set the number of samples and the constant M for rejection sampling.</span>
<span class="n">nsamp</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">M</span> <span class="o">=</span> <span class="mf">2.5</span>

<span class="c1"># Draw nsamp samples from the proposal distribution (Normal(0, 1.5)).</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nsamp</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="n">nsamp</span><span class="p">)</span>

<span class="c1"># Calculate the proposal density g and target density f at y.</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">mixture_pdf</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># Acceptance criterion: u &lt; f / (M * g)</span>
<span class="n">keep_samp</span> <span class="o">=</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="p">(</span><span class="n">f</span> <span class="o">/</span> <span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">g</span><span class="p">))</span>
<span class="n">accepted</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">keep_samp</span><span class="p">]</span>

<span class="c1"># Estimate density using Gaussian KDE.</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">accepted</span><span class="p">)</span>
<span class="n">y_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">accepted</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">accepted</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">200</span><span class="p">)</span>

<span class="c1"># Plot the target mixture model and the proposal distribution.</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mixture_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Target&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">2.5</span> <span class="o">*</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Proposal (M=2.5)&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">accepted</span><span class="p">,</span> <span class="n">bins</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Kept Samples&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mixture_pdf</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True Target&#39;</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_vals</span><span class="p">,</span> <span class="n">kde</span><span class="p">(</span><span class="n">y_vals</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Sampled Density&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$x$&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_9_0.png" src="_images/mcmc_9_0.png" />
</figure>
<p>The rejection sampling algorithm is then:</p>
<ol class="arabic simple">
<li><p>Simulate <span class="math notranslate nohighlight">\(Y_i \sim g(x)\)</span>;</p></li>
<li><p>Simulate <span class="math notranslate nohighlight">\(U_i \sim \text{Uniform}(0, 1)\)</span>.</p></li>
<li><p>Accept <span class="math notranslate nohighlight">\(Y_i\)</span> if <span class="math notranslate nohighlight">\(U_i &lt;= \pi(Y_i) / Mg(Y_i)\)</span>.</p></li>
</ol>
<p>In other words, <span class="math notranslate nohighlight">\(Y\)</span> is accepted as a sample from <span class="math notranslate nohighlight">\(\pi(x)\)</span>
with probability <span class="math notranslate nohighlight">\(\rho = \pi(x) / Mg(x)\)</span>. As a result of this
procedure, the proposals <span class="math notranslate nohighlight">\((Y_i, U_i)\)</span> are uniformly distributed
over the area under the curve of <span class="math notranslate nohighlight">\(g(x)\)</span>, and the rejection
procedure results in the accepted samples being uniformly distributed
over the area under the curve of <span class="math notranslate nohighlight">\(\pi(x)\)</span>, as desired.</p>
<p>An illustration of rejection sampling can be seen below.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set parameters</span>
<span class="n">nsamp</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">M</span> <span class="o">=</span> <span class="mf">3.5</span>

<span class="c1"># Generate nsamp samples from Uniform(0, 1) for u and y</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nsamp</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">nsamp</span><span class="p">)</span>

<span class="c1"># Compute the Beta(5, 10) pdf at each y value</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

<span class="c1"># Determine which samples to keep: condition (M * u) &lt; f</span>
<span class="n">keep_samp</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">*</span> <span class="n">u</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">f</span>

<span class="c1"># Create the figure with the desired size.</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Rejection Sampling Efficiency&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># First plot</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">3.5</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$X$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>

<span class="c1"># Plot the Beta(5, 10) density line</span>
<span class="n">x_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x_vals</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;black&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Beta(5,10)&#39;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)):</span>
    <span class="k">if</span> <span class="n">keep_samp</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;o&#39;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;blue&#39;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">marker</span> <span class="o">=</span> <span class="s1">&#39;x&#39;</span>
        <span class="n">color</span> <span class="o">=</span> <span class="s1">&#39;red&#39;</span>
    <span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">M</span> <span class="o">*</span> <span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="n">marker</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>

<span class="c1"># Second plot</span>
<span class="n">accepted</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">keep_samp</span><span class="p">]</span>
<span class="c1"># Create a density estimate using Gaussian KDE.</span>
<span class="n">kde</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">gaussian_kde</span><span class="p">(</span><span class="n">accepted</span><span class="p">)</span>

<span class="c1"># Define x values for the density plot.</span>
<span class="n">x1_vals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">density_vals</span> <span class="o">=</span> <span class="n">kde</span><span class="p">(</span><span class="n">x1_vals</span><span class="p">)</span>

<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x1_vals</span><span class="p">,</span> <span class="n">density_vals</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">r</span><span class="s1">&#39;$X$&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Sample Density Estimate&quot;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_11_0.png" src="_images/mcmc_11_0.png" />
</figure>
<p>There are several downsides and practical challenges associated with
rejection sampling, which helps motivate the use of Markov chain Monte
Carlo methods, such as the Metropolis-Hastings algorithm. In particular,
the expected value of the acceptance rate is approximately <span class="math notranslate nohighlight">\(1/M\)</span>,
which means choosing a proposal density that minimizes <span class="math notranslate nohighlight">\(M\)</span> while
still covering <span class="math notranslate nohighlight">\(\pi\)</span> is valuable. However, this can be challenging
for complex target distributions or, in particular, high-dimensional
distributions.</p>
</section>
<section id="markov-chain-monte-carlo">
<h5>Markov chain Monte Carlo<a class="headerlink" href="#markov-chain-monte-carlo" title="Link to this heading">#</a></h5>
<p>Markov chain Monte Carlo (MCMC) is a family of algorithms to sample from
(almost) arbitrary probability distributions. The underlying idea is to
construct a Markov chain of samples whose stationary distribution is the
same as the target distribution <span class="math notranslate nohighlight">\(\pi\)</span>. That the target
distribution is the <em>stationary</em> distribution of the constructed chain
is important for <a class="reference internal" href="#tips-for-using-mcmc"><span class="std std-ref">diagnostics</span></a>.</p>
<p>While there are many MCMC algorithms, the most fundamental is the
<strong>Metropolis-Hastings algorithm</strong>. We will focus on the
Metropolis-Hastings algorithm in this tutorial, as it makes the MCMC
procedure and the impacts of choices transparent, though <a class="reference internal" href="#challenges-and-next-steps"><span class="std std-ref">other
approaches</span></a> can scale better.</p>
<p>The Metropolis-Hastings algorithm relies on an accept-reject step to
ensure that the resulting Markov transition probabilities have the right
properties to ensure convergence to the target distribution <span class="math notranslate nohighlight">\(\pi\)</span>.
This requires the specification of a <em>proposal distribution</em> <span class="math notranslate nohighlight">\(q\)</span>.</p>
<p>0. Start from an initial parameter value</p>
<div class="math notranslate nohighlight">
\[x_0.\]</div>
<p>Given</p>
<div class="math notranslate nohighlight">
\[X_t = x_t:\]</div>
<p>1. Generate</p>
<div class="math notranslate nohighlight">
\[Y_t \sim q(y | x_t);\]</div>
<p>2. Set</p>
<div class="math notranslate nohighlight">
\[X_{t+1} = Y_t\]</div>
<p>with probability</p>
<div class="math notranslate nohighlight">
\[\rho(x_t, Y_t)\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\rho(x, y) = \min \left\{\frac{\pi(y)}{\pi(x)}\frac{q(x | y)}{q(y | x)}, 1\right\},\]</div>
<p>else set</p>
<div class="math notranslate nohighlight">
\[X_{t+1} = x_t.\]</div>
<p>Often the proposal distribution is chosen to be symmetric,
<span class="math notranslate nohighlight">\(q(y | x) = q(x | y)\)</span>, so the accept-reject probability
<span class="math notranslate nohighlight">\(\rho(x, y) = \min\{\pi(y)/\pi(x), 1\}\)</span>. We will look later at the
impact of choices of <span class="math notranslate nohighlight">\(q\)</span> and some adaptive approaches.</p>
<p>We can visualize how the algorithm works in practice with the figure
below. The impact of the accept-reject step is that proposals which
increase the target probability relative to the current value
<span class="math notranslate nohighlight">\((\pi(Y_t) &gt; \pi(X_t)\)</span>, as in the top panel) will always be
accepted, while proposals which decrease the target probability (as in
the bottom panel) will be accepted based on the ratio of
<span class="math notranslate nohighlight">\(\pi(Y_t) / \pi(X_t)\)</span>. In this case, the probability of accepting
the proposal of <span class="math notranslate nohighlight">\(y\)</span> is approximately 0.3. If the proposal is
accepted, <span class="math notranslate nohighlight">\(X_{t+1} = Y_t\)</span> and the new proposal is centered on
<span class="math notranslate nohighlight">\(Y_t\)</span>, while if it is rejected, <span class="math notranslate nohighlight">\(X_{t+1} = x_t\)</span> and the
value is repeated in the resulting Markov chain.</p>
<figure class="align-default" id="id1">
<img alt="_images/mh-1.png" src="_images/mh-1.png" />
<figcaption>
<p><span class="caption-text">Metropolis-Hastings step where the proposal is always accepted as it has higher probability according to the target density <span class="math notranslate nohighlight">\(\pi\)</span> than the current value</span><a class="headerlink" href="#id1" title="Link to this image">#</a></p>
</figcaption>
</figure>
<figure class="align-default" id="id2">
<img alt="_images/mh-2.png" src="_images/mh-2.png" />
<figcaption>
<p><span class="caption-text">Metropolis-Hastings step where the proposal may not be accepted as it has lower probability according to the target density <span class="math notranslate nohighlight">\(\pi\)</span> than the current value. In this case, <span class="math notranslate nohighlight">\(\pi(y) / \pi(x) \approx 0.30\)</span>, so the proposal will be accepted with probability 30%.</span><a class="headerlink" href="#id2" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>The sequential accept-reject step and the localization of the proposal
density on the current sample <span class="math notranslate nohighlight">\(X_t\)</span> is what results in the
autocorrelation of the Markov chain, which has implications for the use
of the resulting samples for Monte Carlo estimation and simulation.
Namely, the <em>effective sample size</em></p>
<div class="math notranslate nohighlight">
\[N_\text{eff} = \frac{N}{1 + 2 \sum_{i=1}^\infty \rho_i},\]</div>
<p>is always less than <span class="math notranslate nohighlight">\(N\)</span>, and can be dramatically smaller if the
resulting chain has very high autocorrelation. <span class="math notranslate nohighlight">\(N_\text{eff}\)</span> is
the value that should be used to estimate the Monte Carlo standard error
for any resulting estimatation.</p>
<p>However, this autocorrelation across the samples is a potentially small
price to pay for the flexibility of MCMC. The local proposals mean that
there is no need to find a “general” covering distribution, as in
rejection sampling, which allows the Metropolis-Hastings algorithm to be
practical in higher dimensions and for distributions with unexpected
features such as multi-modality.</p>
<p>In code form, the Metropolis-Hastings algorithm looks like this.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Inputs:</span>
<span class="c1">#   - num_iter: Int, number of iterations to run Metropolis_Hastings algorithm</span>
<span class="c1">#   - proposal_sd: List or vector of proposal standard deviations, corresponding to each parameter</span>
<span class="c1">#   - p0: initial parameter vector</span>
<span class="c1">#   - logposterior: function to calculate the log-posterior for a given parameter vector</span>
<span class="c1"># Outputs:</span>
<span class="c1">#   - parameters: matrix of sampled parameters, num_iter x num_parameters</span>
<span class="c1">#   - lp: vector of log-posterior values for the sampled parameters</span>
<span class="c1">#   - accept_rate: Float of the percentage of proposals which were accepted.</span>

<span class="k">def</span><span class="w"> </span><span class="nf">metropolis</span><span class="p">(</span><span class="n">num_iter</span><span class="p">,</span> <span class="n">proposal_sd</span><span class="p">,</span> <span class="n">p0</span><span class="p">,</span> <span class="n">logposterior</span><span class="p">):</span>
    <span class="c1"># Initialize our lists for sampled parameters and log-posterior values</span>
    <span class="c1"># Create empty array</span>
    <span class="n">parameters</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">p0</span><span class="p">)))</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">num_iter</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># Set initial values</span>
    <span class="n">parameters</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">p0</span>
    <span class="n">lp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">logposterior</span><span class="p">(</span><span class="n">p0</span><span class="p">)</span>
    <span class="c1"># Set up proposal covariance matrix</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">Covariance</span><span class="o">.</span><span class="n">from_diagonal</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">proposal_sd</span><span class="p">))</span>
    <span class="n">acceptances</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_iter</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="c1"># Propose a new state</span>
        <span class="n">proposal</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">)</span>
        <span class="c1"># Calculate the acceptance probability</span>
        <span class="n">lp_proposal</span> <span class="o">=</span> <span class="n">logposterior</span><span class="p">(</span><span class="n">proposal</span><span class="p">)</span>
        <span class="n">p_accept</span> <span class="o">=</span> <span class="n">lp_proposal</span> <span class="o">-</span> <span class="n">lp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">p_accept</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">min</span><span class="p">([</span><span class="n">p_accept</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
        <span class="n">u</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">uniform</span><span class="o">.</span><span class="n">rvs</span><span class="p">()</span>
        <span class="c1"># Accept with probability p_accept</span>
        <span class="k">if</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">p_accept</span><span class="p">):</span>
            <span class="c1"># Add the proposed parameter to the end of the list `parameters`</span>
            <span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">proposal</span>
            <span class="c1"># Add the corresponding posterior score to the end of that list too</span>
            <span class="n">acceptances</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="n">lp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lp_proposal</span>
        <span class="c1"># Reject with probability 1-p_accept</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Add another copy of the current parameter value to the end of the list `parameters`</span>
            <span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="p">:]</span>
            <span class="c1"># Add the corresponding posterior score to the end of that list too</span>
            <span class="n">lp</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">lp</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Calculate the acceptance rate; this is a useful diagnostic</span>
    <span class="n">accept_rate</span> <span class="o">=</span> <span class="n">acceptances</span> <span class="o">/</span> <span class="n">num_iter</span>
    <span class="c1"># Leave off the initial value but return the rest</span>
    <span class="k">return</span> <span class="n">parameters</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">lp</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">accept_rate</span>
</pre></div>
</div>
</section>
</section>
<section id="hymod-calibration">
<h4>HYMOD Calibration<a class="headerlink" href="#hymod-calibration" title="Link to this heading">#</a></h4>
<p>Let’s look at how well HYMOD with some default parameters explain the
streamflow data. This example may take a while to converge; HYMOD is
sufficiently complex (both computationally and in terms of dynamics)
that this “naive” approach to MCMC is relatively slow on a local
machine. We will discuss some alternative approaches for this category
of models in Section 3 (Diagnostics).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># load the Leaf River HYMOD input file</span>
<span class="n">leaf_data</span> <span class="o">=</span> <span class="n">load_hymod_input_file</span><span class="p">()</span>

<span class="c1"># extract the first eleven years of data</span>
<span class="n">leaf_data</span> <span class="o">=</span> <span class="n">leaf_data</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">4015</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Leaf River Data structure:&#39;</span><span class="p">)</span>

<span class="c1"># There are only three columns in the file including precipitation, potential evapotranspiration, and streamflow</span>
<span class="n">leaf_data</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Leaf</span> <span class="n">River</span> <span class="n">Data</span> <span class="n">structure</span><span class="p">:</span>
</pre></div>
</div>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Precip</th>
      <th>Pot_ET</th>
      <th>Strmflw</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>4.60</td>
      <td>0.29</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>4.31</td>
      <td>0.24</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.0</td>
      <td>4.33</td>
      <td>0.21</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.0</td>
      <td>4.78</td>
      <td>0.19</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.0</td>
      <td>2.91</td>
      <td>0.18</td>
    </tr>
  </tbody>
</table>
</div><p>Let’s look at how well the model performs with some default parameter
values.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># assign input parameters to generate a baseline simulated streamflow</span>
<span class="n">Nq</span> <span class="o">=</span> <span class="mi">3</span>  <span class="c1"># Number of quickflow routing tanks</span>
<span class="n">Kq</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Quickflow routing tanks&#39; rate parameter</span>
<span class="n">Ks</span> <span class="o">=</span>  <span class="mf">0.001</span> <span class="c1"># Slowflow routing tank&#39;s rate parameter</span>
<span class="n">Alp</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="c1"># Quick/slow split parameter</span>
<span class="n">Huz</span> <span class="o">=</span> <span class="mi">100</span> <span class="c1"># Maximum height of soil moisture accounting tank</span>
<span class="n">B</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="c1"># Scaled distribution function shape parameter</span>

<span class="c1"># Note that the number of years is 11. One year of model warm-up and ten years are used for actual simulation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">msdbook</span><span class="o">.</span><span class="n">hymod</span><span class="o">.</span><span class="n">hymod</span><span class="p">(</span><span class="n">Nq</span><span class="p">,</span> <span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="o">=</span><span class="mi">4015</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">msdbook</span><span class="o">.</span><span class="n">hymod</span><span class="o">.</span><span class="n">plot_observed_vs_simulated_streamflow</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">leaf_data</span><span class="p">,</span> <span class="n">hymod_dict</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_22_0.png" src="_images/mcmc_22_0.png" />
</figure>
<p>We can see that this HYMOD parameterization generally does well, but
tends to underestimate the peak streamflows. Can we do better?</p>
<p>First, we need to specify a probability model for the data. To do this,
we can write the data <span class="math notranslate nohighlight">\(y_t\)</span> as the sum of the model output
<span class="math notranslate nohighlight">\(F(\theta_F; \mathbf{x}_t)\)</span> (where <span class="math notranslate nohighlight">\(\theta_F\)</span> is the
parameter vector and <span class="math notranslate nohighlight">\(\mathbf{x}_t\)</span> are the exogenous model
forcings) and the residuals <span class="math notranslate nohighlight">\(\mathbf{z}_t(\theta_z)\)</span>, where
<span class="math notranslate nohighlight">\(\theta_z\)</span> are the statistical parameters used to describe the
residual distribution. The residual probability model can be relatively
simple, such as the common assumption that <span class="math notranslate nohighlight">\(\mathbf{z}_t\)</span> are
independently distributed according to a Gaussian distribution, or can
be more complex, including auto-correlations, cross-correlations, and/or
combinations of systematic <em>model data-discrepancy</em> and independent
observation errors.</p>
<p>In this example, we will assume that the residuals are normally
distributed (on the log scale, since HYMOD predictions and streamflow
are non-negative), though in practice we would check this assumption by
fitting the model and looking at residual diagnostics, such as partial
autocorrelation and Q-Q plots. Since HYMOD can simulate zero streamflow,
which is not in the data, we will also include a strictly positive bias
term <span class="math notranslate nohighlight">\(\beta\)</span>. As a result, our probability model is</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{gather*}
\log(y_t) = \log(F(\theta_F; \mathbf{x}_t) + \beta) + z_t \\
z_t \sim \mathcal{N}(0, \sigma)
\end{gather*}\end{split}\]</div>
<p>This means that we need the following model and statistical parameters:</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Nq</span></code>: the number of quickflow routing tanks;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Kq</span></code>: the quickflow routing tanks’ rate parameter;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ks</span></code>: The slowflow routing tanks’ rate parameter;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Alp</span></code>: The quick/slow split parameter;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Huz</span></code>: The maximum height of soil moisture accounting tank;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">B</span></code>: The scaled distribution function scale parameter;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta</span></code>: Positive bias term, since HYMOD can produce zero simulated
streamflow;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sigma</span></code>: Standard deviation of the log-residual normal
distribution.</p></li>
</ol>
<section id="prior-distributions">
<h5>Prior Distributions<a class="headerlink" href="#prior-distributions" title="Link to this heading">#</a></h5>
<p>MCMC lets us sample from arbitrary probability distributions, including
Bayesian posterior distributions. One advantage of a Bayesian approach
to model calibration is that it lets us include prior information for
parameter values, which can help guide inferences towards
mechanistically reasonable values. In the absence of firm prior
information about parameter values, we can check that prior
distributions result in reasonable simulations with a <em>prior predictive
check</em>. Let’s start with the following priors, which we assume are
independent across parameter.</p>
<ol class="arabic simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Kq</span></code>: <span class="math notranslate nohighlight">\(\text{LogNormal}(0.25, 0.5)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Ks</span></code>: <span class="math notranslate nohighlight">\(\text{LogNormal}(0.95, 0.003)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Alp</span></code>: <span class="math notranslate nohighlight">\(\text{Beta}(2, 2)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Huz</span></code>: <span class="math notranslate nohighlight">\(\mathcal{N}(100, 20)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">B</span></code>: <span class="math notranslate nohighlight">\(\text{LogNormal}(0.1, 1)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">beta</span></code>: <span class="math notranslate nohighlight">\(\text{LogNormal}(0.05, 0.5)\)</span>;</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sigma</span></code>: <span class="math notranslate nohighlight">\(\text{LogNormal}(0.5, 0.5)\)</span>.</p></li>
</ol>
<p>To conduct a prior predictive check, we will generate samples from these
distributions, evaluate the model (and add residuals), and then look at
the distribution of output (or output summary statistics) about which we
have some intuition about what are reasonable values. Note that we will
not explicitly compare these results to the data, we do not want to
overfit.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">1000</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">(</span><span class="n">array</span><span class="p">([</span> <span class="mf">13.</span><span class="p">,</span>  <span class="mf">59.</span><span class="p">,</span> <span class="mf">139.</span><span class="p">,</span> <span class="mf">255.</span><span class="p">,</span> <span class="mf">264.</span><span class="p">,</span> <span class="mf">159.</span><span class="p">,</span>  <span class="mf">85.</span><span class="p">,</span>  <span class="mf">21.</span><span class="p">,</span>   <span class="mf">4.</span><span class="p">,</span>   <span class="mf">1.</span><span class="p">]),</span>
 <span class="n">array</span><span class="p">([</span><span class="mf">0.43029764</span><span class="p">,</span> <span class="mf">0.44725484</span><span class="p">,</span> <span class="mf">0.46421203</span><span class="p">,</span> <span class="mf">0.48116923</span><span class="p">,</span> <span class="mf">0.49812642</span><span class="p">,</span>
        <span class="mf">0.51508362</span><span class="p">,</span> <span class="mf">0.53204081</span><span class="p">,</span> <span class="mf">0.54899801</span><span class="p">,</span> <span class="mf">0.5659552</span> <span class="p">,</span> <span class="mf">0.5829124</span> <span class="p">,</span>
        <span class="mf">0.59986959</span><span class="p">]),</span>
 <span class="o">&lt;</span><span class="n">BarContainer</span> <span class="nb">object</span> <span class="n">of</span> <span class="mi">10</span> <span class="n">artists</span><span class="o">&gt;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_25_1.png" src="_images/mcmc_25_1.png" />
</figure>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ndays</span> <span class="o">=</span> <span class="mi">4015</span>
<span class="n">nsamples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># generate prior samples</span>
<span class="n">Kq_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">Ks_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
<span class="n">Alp_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">Huz_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">B_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">beta_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">sigma_prior</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="p">(</span><span class="n">s</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>

<span class="n">Kq</span> <span class="o">=</span> <span class="n">Kq_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">Ks</span> <span class="o">=</span> <span class="n">Ks_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">Alp</span> <span class="o">=</span> <span class="n">Alp_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">Huz</span> <span class="o">=</span> <span class="n">Huz_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">B_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">beta_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma_prior</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">nsamples</span><span class="p">)</span>

<span class="c1"># preallocate output storage</span>
<span class="n">prior_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ndays</span><span class="p">,</span> <span class="n">nsamples</span><span class="p">))</span>

<span class="c1"># note that we include the error/noise in these simulations</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nsamples</span><span class="p">):</span>
    <span class="n">prior_out</span><span class="p">[:,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">msdbook</span><span class="o">.</span><span class="n">hymod</span><span class="o">.</span><span class="n">hymod</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">Kq</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Ks</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Alp</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">Huz</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="o">=</span><span class="n">ndays</span><span class="p">)[</span><span class="s1">&#39;Q&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sigma</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">ndays</span><span class="p">))</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># compute 90% prediction interval for each time step</span>
<span class="n">prior_q90</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">prior_out</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">strmflw_ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">leaf_data</span><span class="o">.</span><span class="n">Strmflw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">prior_q90</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">prior_q90</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">prior_q90</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Observations&#39;</span><span class="p">,</span> <span class="s1">&#39;Prior Predictive Median&#39;</span><span class="p">,</span> <span class="s1">&#39;90% Prior Predictive Interval&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_27_1.png" src="_images/mcmc_27_1.png" />
</figure>
<p>This looks reasonable as a starting point; we may not be capturing the
most extreme data in our 90% interval, but we also wouldn’t expect to,
and as none of our priors are uniform, we are not closing off the
possibility that the posteriors could be wider.</p>
</section>
<section id="metropolis-hastings">
<h5>Metropolis-Hastings<a class="headerlink" href="#metropolis-hastings" title="Link to this heading">#</a></h5>
<p>To implement the Metropolis-Hastings algorithm, we’ll start by writing
functions to compute the log-posterior of the probability model.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">log_prior</span><span class="p">(</span><span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Kq</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Ks</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.003</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Alp</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">Huz</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">20</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">beta</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="n">lp</span> <span class="o">+=</span> <span class="n">stats</span><span class="o">.</span><span class="n">lognorm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">sigma</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">lp</span>


<span class="k">def</span><span class="w"> </span><span class="nf">log_likelihood</span><span class="p">(</span><span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="p">):</span>
    <span class="n">hymod_out</span> <span class="o">=</span> <span class="n">msdbook</span><span class="o">.</span><span class="n">hymod</span><span class="o">.</span><span class="n">hymod</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="o">=</span><span class="n">ndays</span><span class="p">)[</span><span class="s1">&#39;Q&#39;</span><span class="p">]</span>
    <span class="n">residuals</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">leaf_data</span><span class="p">[</span><span class="s1">&#39;Strmflw&#39;</span><span class="p">])</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">hymod_out</span> <span class="o">+</span> <span class="n">beta</span><span class="p">)</span> <span class="c1"># compute residuals</span>
    <span class="n">ll</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">logpdf</span><span class="p">(</span><span class="n">residuals</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">sigma</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">ll</span>


<span class="k">def</span><span class="w"> </span><span class="nf">log_posterior</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">leaf_data</span><span class="o">=</span><span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="o">=</span><span class="mi">4015</span><span class="p">):</span>
    <span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">lp</span> <span class="o">=</span> <span class="n">log_prior</span><span class="p">(</span><span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">)</span>
    <span class="c1"># only evaluate the model if the log-prior &gt; -Inf</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">lp</span><span class="p">)</span> <span class="ow">and</span> <span class="n">lp</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
        <span class="n">ll</span> <span class="o">=</span> <span class="n">log_likelihood</span><span class="p">(</span><span class="n">Kq</span><span class="p">,</span> <span class="n">Ks</span><span class="p">,</span> <span class="n">Alp</span><span class="p">,</span> <span class="n">Huz</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="p">)</span>
        <span class="n">lp</span> <span class="o">+=</span> <span class="n">ll</span>
    <span class="k">return</span> <span class="n">lp</span>
</pre></div>
</div>
<p>Next, we’ll implement the Metropolis-Hastings algorithm. The number of
iterations is set to 100,000, which is needed for convergence. The
<code class="docutils literal notranslate"><span class="pre">metropolis()</span></code> function may take a long time to run (75-290 min), to
speed this up, reduce the <code class="docutils literal notranslate"><span class="pre">niter</span></code> parameter (ex. <code class="docutils literal notranslate"><span class="pre">niter</span> <span class="pre">=</span> <span class="pre">1000</span></code>).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">niter</span> <span class="o">=</span> <span class="mi">100000</span>

<span class="n">init_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">]])</span>
<span class="n">proposal_sd</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">]</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">metropolis</span><span class="p">(</span><span class="n">niter</span><span class="p">,</span> <span class="n">proposal_sd</span><span class="p">,</span> <span class="n">init_params</span><span class="p">,</span> <span class="n">log_posterior</span><span class="p">)</span>
</pre></div>
</div>
<p>What is the acceptance rate? Both too high and too low of an acceptance
rate suggest something is off with how our sampler is balancing
exploration and exploitation. The theoretical “ideal” is between 24-45%.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.27565</span>
</pre></div>
</div>
<p>To provide some evidence for convergence, let’s look at the traceplots.
We’ll look at a burn-in of 1/10 the number of iterations; this may need
to change depending on the number of iterations you run (e.g. if the
traceplot after the red vertical line appears to shift versus appearing
roughly stationary for the rest of the chain).</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parnames</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Kq&quot;</span><span class="p">,</span> <span class="s2">&quot;Ks&quot;</span><span class="p">,</span> <span class="s2">&quot;Alp&quot;</span><span class="p">,</span> <span class="s2">&quot;Huz&quot;</span><span class="p">,</span> <span class="s2">&quot;B&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;sigma&quot;</span><span class="p">]</span>
<span class="n">nburn</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">niter</span> <span class="o">/</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][:,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">nburn</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span> <span class="c1"># modify x to look at other burnin lengths</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">parnames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_37_1.png" src="_images/mcmc_37_1.png" />
</figure>
<p>We can see that we might have converged by 10,000 iterations (or
possibly earlier). We will discard the samples from before this point as
burn-in since they have an unrepresentative probability in the sampled
chain.</p>
<p>Let’s zoom in on the samples from after this point.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][(</span><span class="n">nburn</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="n">niter</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="n">parnames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">6</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Iteration&quot;</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_39_1.png" src="_images/mcmc_39_1.png" />
</figure>
<p>These chains look like a “hairy caterpillar”, which is the ideal pattern
for the chain to mix well and sample systematically throughout the
posterior distribution. If our proposal distribution had been too
narrow, we would have accepted many more samples, but the traceplot
above would look like a narrow line “dragging” slowly, instead of
bouncing around (the chain for <span class="math notranslate nohighlight">\(K_s\)</span> looks closest to this type fo
behavior). If it had been too wide, we would have rejected many more
samples, and the traceplot would have looked more like a city skyline,
as the sampler would have gotten stuck at the same value for a long
time.</p>
<p>The chains shown above <em>look</em> roughly stationary: there is no visual
evidence of large shifts in the distribution, such as jumps or changes
in the variance. However, the only guarantee that the Markov chain
produced by the Metropolis-Hastings algorithm will converge to the
target distribution is asymptotic (as the number of iterations
<span class="math notranslate nohighlight">\(n \to \infty\)</span>), and there is no mathematically-guaranteed rate of
convergence to guide our decision-making. Instead, we generally want to
be skeptical that our chain has converged to the target distribution and
to accumulate evidence contradicting our skepticism.</p>
<p>One quick check for convergence is to look at whether the distribution
of samples change between the first half of the post-burn-in chain and
its entirety. If the second half of the samples do not materially change
the distribution, that is evidence for convergence, as it suggests that
the later samples are drawn from the same distribution as the earlier
ones. On the other hand, if the two distributions differ, the later
samples are clearly not drawn from the same distribution as the first
samples, and it would be unclear that the chain has converged.</p>
<p>Let’s implement this check for <span class="math notranslate nohighlight">\(K_q\)</span> as an example. We can see
from the figure below that the two histograms look roughly similar,
which passes this convergence check.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][(</span><span class="n">nburn</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="nb">int</span><span class="p">(</span><span class="n">niter</span><span class="o">/</span><span class="mi">2</span><span class="p">),</span> <span class="n">i</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][(</span><span class="n">nburn</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="n">niter</span><span class="p">,</span> <span class="n">i</span><span class="p">],</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Kq&quot;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;Kq&quot;</span><span class="p">);</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Density&quot;</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_41_1.png" src="_images/mcmc_41_1.png" />
</figure>
<p>A more systematic generalization of this convergence check would involve
generating multiple chains starting at different initial conditions to
check that the chains reach roughly the same distribution, but we will
skip that for now.</p>
<p>Let’s look at the resulting parameter distributions.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">],</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">7</span><span class="p">):</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][(</span><span class="n">nburn</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span><span class="n">niter</span><span class="p">,</span> <span class="n">i</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="n">parnames</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
<span class="n">axs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;Count&quot;</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_43_1.png" src="_images/mcmc_43_1.png" />
</figure>
<p>Now, let’s simulate from the posterior distribution to see how well we
capture the observed streamflow.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">nsamp</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">range</span><span class="p">((</span><span class="n">nburn</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">niter</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="n">nsamp</span><span class="p">)</span>

<span class="c1"># simulate</span>
<span class="n">hymod_sim</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">ndays</span><span class="p">,</span> <span class="n">nsamp</span><span class="p">))</span>
<span class="k">for</span> <span class="n">index</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">idx</span><span class="p">):</span>
    <span class="n">hymod_sim</span><span class="p">[:,</span> <span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">msdbook</span><span class="o">.</span><span class="n">hymod</span><span class="o">.</span><span class="n">hymod</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">leaf_data</span><span class="p">,</span> <span class="n">ndays</span><span class="o">=</span><span class="n">ndays</span><span class="p">)[</span><span class="s1">&#39;Q&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span> <span class="o">+</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="mi">6</span><span class="p">])</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">ndays</span><span class="p">))</span>

<span class="c1"># compute quantiles</span>
<span class="n">hymod_q</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">hymod_sim</span><span class="p">,</span> <span class="p">[</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">strmflw_ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">12</span><span class="p">,</span><span class="mi">6</span><span class="p">])</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50</span><span class="p">])</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">leaf_data</span><span class="o">.</span><span class="n">Strmflw</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">hymod_q</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">),</span> <span class="n">hymod_q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">hymod_q</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">strmflw_ax</span><span class="o">.</span><span class="n">legend</span><span class="p">([</span><span class="s1">&#39;Observations&#39;</span><span class="p">,</span> <span class="s1">&#39;Posterior Predictive Median&#39;</span><span class="p">,</span> <span class="s1">&#39;90% Posterior Predictive Interval&#39;</span><span class="p">],</span> <span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper right&#39;</span><span class="p">);</span>
</pre></div>
</div>
<figure class="align-default">
<img alt="_images/mcmc_46_1.png" src="_images/mcmc_46_1.png" />
</figure>
<p>We can visually see that we fail to capture some of the extremes in the
90% projection interval. This is ok; we would expect about 10% of the
data to be outside of the interval if the model were well-calibrated. To
check, we can compute the <em>surprise index</em>, which is the fraction of
points outside of the projection interval.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">si</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="nb">sum</span><span class="p">([</span><span class="n">hymod_q</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">leaf_data</span><span class="o">.</span><span class="n">Strmflw</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;=</span> <span class="n">hymod_q</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">ndays</span><span class="p">)])</span> <span class="o">/</span> <span class="n">ndays</span><span class="p">)</span>
<span class="n">si</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">0.09464508094645085</span>
</pre></div>
</div>
<p>The surprise index is 9.4%, when we would expect it to be 10%. That’s
not bad (actually, it’s quite good), and means that the model is well
calibrated. If we wanted to dial the calibration in further (or if the
surprise index were far off, like 20% or 2%), we could change the priors
to be more or less restrictive as appropriate. This is somewhat of a
judgement call; there is no objectively acceptable threshold for
deviation from the target calibration level, but in general, being
within a few percentage points is acceptable.</p>
</section>
</section>
<section id="challenges-and-next-steps">
<h4>Challenges and Next Steps<a class="headerlink" href="#challenges-and-next-steps" title="Link to this heading">#</a></h4>
<p>Two of the main challenges in implementing MCMC are:</p>
<ol class="arabic simple">
<li><p>The complexity of the model. As MCMC can take hundreds of thousands
of model evaluations, small increases in computational expense can be
the difference in whether MCMC is feasible or not. Increasing number
of un- or weakly-correlated parameters (model or statistical) can
also pose problems, as these require more samples to fully explore
and capture the distribution. Since the Metropolis-Hastings algorithm
in particular is fundamentally serial (the need to burn in every
chain means there is only a weak benefit to parallelization), these
challenges are to some degree unavoidable without the use of a more
sophisticated algorithm.</p></li>
<li><p>Selection of the proposal distribution. The efficiency of the sampler
makes a big difference in the number of needed samples and the
<em>effective sample size</em> of the resulting chain. This can require a
lot of tuning and gets more complex as the number of parameters
increases.</p></li>
<li><p>Specification of the likelihood/probability model. We used a fairly
simple model for the HYMOD residuals, but for more complex settings,
the residuals may exhibit a high degree of spatial or temporal
autocorrelation or may be highly nonstationary. Developing the model
and writing down the likelihood function for the error process may be
intractable for some classes of models.</p></li>
</ol>
<p>The first two challenges can be addressed with more advanced methods
than those used here. Adaptive Metropolis-Hastings algorithms (such as
those included in the <code class="docutils literal notranslate"><span class="pre">adaptMCMC</span></code> R package or <code class="docutils literal notranslate"><span class="pre">AdaptiveMCMC</span></code> in
Julia) automatically tune the proposal distribution based on the
acceptance rate. Much more powerful algorithms such as Hamiltonian Monte
Carlo (used in the Stan family of packages, <code class="docutils literal notranslate"><span class="pre">pyMC3</span></code> in Python, and
<code class="docutils literal notranslate"><span class="pre">Turing</span></code> in Julia) use information about the gradient of the posterior
to sample very efficiently, though this often requires the ability to
automatically differentiate external simulation models, which may or may
not always be possible.</p>
<p>The third challenge is more fundamental (and general) for uncertainty
quantifican. When writing down a likelihood function is intractable,
Approximate Bayesian Computation (ABC) is a likelihood-free approach which
is based on comparing summary statistics, rather than computing the
posterior density.</p>
</section>
<section id="tips-for-using-mcmc">
<h4>Tips for Using MCMC<a class="headerlink" href="#tips-for-using-mcmc" title="Link to this heading">#</a></h4>
<p>In this tutorial, we saw how to implement the Metropolis-Hastings
algorithm for HYMOD. In order to use Metropolis-Hastings or other MCMC
algorithms to your problem, you will need to answer the following
questions:</p>
<ol class="arabic simple">
<li><p>Do you have a probability model for the data-generating process? This
could be a statistical model for the data or a model for the
discrepancy between a simulation model and the data. We often begin
with a relatively simple model (<em>e.g.</em> normally-distributed
residuals) and add complexity based on whether residual diagnostics
suggest that the probability model was appropriate. If you do not or
cannot write down an appropriate probability model, you could look at
likelihood-free methods such as Approximate Bayesian Computation
(ABC).</p></li>
<li><p>How complex is your inference problem? The more computationally
complex your model or the higher the dimensionality of the parameter
space, the longer MCMC will need to run to fully sample from the
posterior distribution. If your model is too complex, you could begin
with initial uncertainty characterization or sensitivity analyses to
evaluate the extent to which dimension reduction is possible, and you
could look into emulation or surrogate modeling methods. Using
Hamiltonian Monte Carlo methods are also an option if your model is
amenable to automatic differentiation.</p></li>
<li><p>How important is parametric uncertainty for your problem? If you’re
only interested in a point estimate of parameters, you could more
directly optimize the posterior density to find the maximum <em>a
posteriori</em> estimate instead of sampling from the posterior
distribution.</p></li>
</ol>
<p>If your answers to these questions suggest that MCMC is tractable and
useful for your problem, you should feel free to experiment with the
HYMOD example, including the number of iterations, the probability model
specification, and the proposal distribution. Just be aware that
increasing the number of iterations or making the probability model more
complex might make the notebook take longer to run.</p>
</section>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-calibration-with-markov-chain-monte-carlo">Model Calibration with Markov chain Monte Carlo</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#note-if-you-are-running-this-notebook-locally-uncomment-the-following-code-to-install-the-required-package-data"><strong>NOTE:</strong> If you are running this notebook locally, uncomment the following code to install the required package data:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mcmc-for-model-calibration-and-uncertainty-quantification">MCMC for Model Calibration and Uncertainty Quantification</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-uncertainty-quantification">Bayesian Uncertainty Quantification</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rejection-sampling">Rejection Sampling</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#markov-chain-monte-carlo">Markov chain Monte Carlo</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hymod-calibration">HYMOD Calibration</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#prior-distributions">Prior Distributions</a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#metropolis-hastings">Metropolis-Hastings</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#challenges-and-next-steps">Challenges and Next Steps</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#tips-for-using-mcmc">Tips for Using MCMC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022-current, Battelle Memorial Institute.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>