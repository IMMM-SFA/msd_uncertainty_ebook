{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Sensitivity Analysis of the HYMOD Model\n",
    "\n",
    "This tutorial has been developed to showcase some of the sensitivity analysis and uncertainty quantification concepts and tools established in the main text in the context of HYMOD, a rainfall-runoff model. The tutorial includes the following sections:\n",
    "\n",
    "#### Introduction to HYMOD and Sensitivity Analysis\n",
    "[1- Introduction to a simple hydrologic model (HYMOD)](#hymod) <br>\n",
    "[2- An overview of sensitivity analysis using SALib](#sensitivity)  <br>\n",
    "[3- Calculation of sensitivity analysis metrics](sa_metrics)  <br>\n",
    "\n",
    "#### Time-Varying Sensitivity Analysis\n",
    "[4- Time-varying sensitivity analysis](#TVSA)  <br>\n",
    "\n",
    "#### Ensemble-based Parametric Uncertainty\n",
    "[5- Generalized Likelihood Uncertainty Estimation (GLUE)](#GLUE)  <br>\n",
    "[6- Pre-Calibration](#precalibration) <br>\n",
    "\n",
    "<b> It is important to note that, </b> although in this tutorial we focus on HYMOD, which is a hydrologic model, it can also be thought of as an example of a model that abstracts complex non-linear systems. Therefore, many of the methods that we use in the tutorial can be applied to numerical models that simulate other complex non-linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to HYMOD and Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='hymod'></a> \n",
    "# 1- HYMOD \n",
    "\n",
    "HYMOD is a simple hydrologic model (rainfall-runoff model) that simulates key hydrologic fluxes such as infiltration, streamflow and evapotranspiration. The model has been originally developed and used for river flow forecasting. However, in the last two decades the model has been widely used to explore different sensitivity analysis (e.g., [Herman et al., 2013](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20124)), uncertainty quantification (e.g.,  [Smith et al., 2008](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2006WR005205)), and optimization (e.g., [Ye et al., 2014](https://www.sciencedirect.com/science/article/pii/S0022169414006362?casa_token=IRqE19Hkfa8AAAAA:_fXOqfwpzxMpchvu8_0njCe0Ok9H29Gyw2F46l9PzG9UVODDTUg6wIOSiyp6uybGevNVnZ7N)) concepts.\n",
    "\n",
    "There are two main assumptions in the model: <br>\n",
    "1) Rainfall is generated through infiltration excess overland flow. <br>\n",
    "2) Runoff generation can be formulated by the probability-distributed principle (Moore, 1985). Therefore the cumulative rate storage capacity $(F(C))$ can be calculated using the following relationship:\n",
    "\n",
    "$$F(C) = 1 - (1 - \\frac{C}{C_{MAX}})^{B_{exp}}$$\n",
    "\n",
    "where *$C$* is the water storage capacity; **$C_{MAX}$** is the parameter describing basin maximum water storage capacity (mm); and **$B_{exp}$** is the parameter describing the degree of spatial variability within the basin. \n",
    "\n",
    "\n",
    "The portion of precipitation that exceeds the water storage capacity is treated as runoff. The evapotranspiration is equal to potential evapotranspiration ($PET$) if enough water is available. Otherwise, it equals the available water storage.\n",
    "\n",
    "Then, based on a parameter $Alpha$, the runoff is divided into quick flow and slow flow, which are routed through three identical quick flow tanks $Q1, Q2, Q3$ and a parallel slow flow tank, respectively.\n",
    "\n",
    "The flow rates in the routing system are described by the resident time in the quick tanks $K_q$ (day) and the slow tank $K_s$ (day), respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](./figs/hymod_schematic.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vrugt et al., (2008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1-1 Model Parameters\n",
    "$C_{MAX}$: parameters describing basin maximum water storage capacity (mm)\n",
    "\n",
    "$B_{exp}$:      parameters describing the degree of spatial variability within the basin between 0 and Huz\n",
    "\n",
    "$Alp$:    Fraction of runoff contributing to quick flow \n",
    "\n",
    "$K_q$:     Quick flow residence time of linear infinite reservoir (the Kq values of all three linear reservoirs are the same)\n",
    "\n",
    "$K_s$:      Slow flow residence time of linear infinite reservoir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2 Input data\n",
    "\n",
    "The HYMOD model only requires precipitation and potential evapotranspiration as inputs. The Leaf River example that we use here is also a widely used test case of HYMOD. The dataset also includes observed runoff that we later use to evaluate the performace of each sensitvity analysis sample set. \n",
    "\n",
    "We can use the following to read the input file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msdbook\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load example data\n",
    "msdbook.install_package_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the HYMOD input file\n",
    "leaf_data = msdbook.load_hymod_input_file()\n",
    "\n",
    "# extract the first eleven years of data\n",
    "leaf_data = leaf_data.iloc[0:4015].copy()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are only three columns in the file including precipitation, potential evapotranspiration and  streamflow\n",
    "leaf_data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3 Baseline Model Simulation\n",
    "\n",
    "We can start our sensitivity analysis experiment with running HYMOD using its default parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign input parameters to generate a baseline simulated streamflow\n",
    "Nq = 3  # Number of quickflow routing tanks \n",
    "Kq = 0.5 # Quickflow routing tanks' rate parameter    \n",
    "Ks =  0.001 # Slowflow routing tank's rate parameter           \n",
    "Alp = 0.5 # Quick/slow split parameter   \n",
    "Huz = 100 # Maximum height of soil moisture accounting tank  \n",
    "B = 1.0 # Scaled distribution function shape parameter    \n",
    "\n",
    "# Note that the number of years is 11 years. One year of model warm-up and ten years are used for actual simulation\n",
    "model = msdbook.hymod(Nq, Kq, Ks, Alp, Huz, B, leaf_data, ndays=4015)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4 Model Outputs\n",
    "\n",
    "Model outputs include actual evapotranspiration, quick and fast streamflow, and combined runoff. In this tutorial we focus on the total daily runoff ($m^3/s$). We can use the following script to plot simulated streamflow against observed streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "**PP**:    Precipitation\n",
    "\n",
    "**ET**:    Evapotranspiration\n",
    "\n",
    "**OV**:    Runoff\n",
    "\n",
    "**Qq**:    Quick Flow\n",
    "\n",
    "**Qs**:    Slow Flow\n",
    "\n",
    "**QQ**:    Streamflow (Quick Flow + Slow Flow)\n",
    "\n",
    "**XHuz** and **XCuz**: Current moisture state of soil moisture accounting component (as depth XH or volume XC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the observed versus simulated streamflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-observed-vs-simulated-streamflow\">plot_observed_vs_simulated_streamflow</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = msdbook.plot_observed_vs_simulated_streamflow(df=leaf_data, hymod_dict=model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sensitivity'></a> \n",
    "# 2- Sensitivity Analysis\n",
    "\n",
    "Here we use the SALib Python library to explore how different HYMOD input parameters affect model streamflow simulations. For this exercise, we only use Sobol variance-based method. The following commands can be used to import SALib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SALib.sample import saltelli\n",
    "from SALib.analyze import sobol\n",
    "from SALib.analyze import delta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2 Model simulations for sensitivity analysis\n",
    "We first define the model input and their ranges. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem_hymod = {\n",
    "    'num_vars': 5,\n",
    "    'names': ['Kq', 'Ks', 'Alp', 'Huz', 'B'],\n",
    "    'bounds': [[0.1, 1],  # Kq\n",
    "               [0, 0.1],  # Ks\n",
    "               [0, 1],    # Alp\n",
    "               [0.1, 500],  # Huz\n",
    "               [0, 1.9]]  # B             \n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to sample and then run the model for each of the sample sets. We will load a sample that has already been created `param_values_hymod` for demonstration purposes.  The actual model simulation takes an extended period, so we also load the simulation data from a previous run. The following demonstrates how to conduct this analysis:\n",
    "```python\n",
    "\n",
    "# generate 256 samples. This is an arbitrary number. \n",
    "param_values_hymod = saltelli.sample(problem_hymod, 256)\n",
    "\n",
    "# dictionary to store outputs in\n",
    "d_outputs = {}\n",
    "\n",
    "# run simulation for each parameter sample\n",
    "for i in range(0, len(param_values_hymod)):\n",
    "    \n",
    "    # run model for each sensitivity analysis parameter sets\n",
    "    hymod_output = msdbook.hymod(Nq, \n",
    "                                 param_values_hymod[i, 0], \n",
    "                                 param_values_hymod[i, 1], \n",
    "                                 param_values_hymod[i, 2], \n",
    "                                 param_values_hymod[i, 3], \n",
    "                                 param_values_hymod[i, 4], \n",
    "                                 leaf_data, \n",
    "                                 ndays=4015)\n",
    "    \n",
    "    # store the simulated total flow discharge\n",
    "    d_outputs[f\"Q{i}\"] = hymod_output[\"Q\"]\n",
    "\n",
    "    \n",
    "Q_df_bw = pd.DataFrame(d_outputs)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously generated parameter values \n",
    "param_values_hymod = msdbook.load_hymod_params()\n",
    "\n",
    "# number of samples\n",
    "n_samples = len(param_values_hymod)\n",
    "\n",
    "# load previously generated hymod simulated outputs\n",
    "Q_df_bw = msdbook.load_hymod_simulation()\n",
    "\n",
    "# column names of each sample simulation number\n",
    "sample_column_names = [i for i in Q_df_bw.columns if i[0] == 'Q']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Warm-up\n",
    "A hydrological model such as HYMOD usually includes ordinary differential equations that are sensitive to their initial condition. They also have components in their underlying formulation that have long memory such that prior time steps can affect their current simulations. For example, soil moisture or groundwater can hold water for a long time and therefore they are often considered to exhibit a long memory. This can affect the partitioning of water to runoff and infiltration, while also controlling the generation of base flow. Therefore, it is important to have a reasonable initial value for them. To achieve this, hydrologists usually extend their simulation period and after the simulations, they remove that extended time period that has unreasonable groundwater or surface water values. This time period is called the warm-up time period.\n",
    "\n",
    "Here we extended our simulation for one year (from 10 years to 11 years) and we removed the first year of simulation, therefore our warm-up period is one year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude the first year of simulation from the simulations and reset the index\n",
    "Q_df = Q_df_bw.iloc[365:4015].copy().reset_index(drop=True)\n",
    "\n",
    "# exclude the first year of the input data and reset the index\n",
    "leaf_data = leaf_data.iloc[365:4015].copy().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3 Visual inspection of the model outputs\n",
    "\n",
    "Here we create a figure that shows HYMOD streamflow outputs under different sample sets, and compare them with the observed streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date columns to our simulation data frame; for this data our start date is 1/1/2000\n",
    "date_ts = pd.date_range(start='1/1/2000', periods=3650, freq='D')\n",
    "Q_df['date'] = date_ts\n",
    "Q_df['year'] = date_ts.year\n",
    "Q_df['month'] = date_ts.month\n",
    "Q_df['day'] = date_ts.day\n",
    "\n",
    "# aggregate the simulated observed streamflow to monthly mean\n",
    "df_sim_mth_mean = Q_df.groupby(['year', 'month'])[sample_column_names].mean()\n",
    "\n",
    "# do the same for the observed data\n",
    "date_ts = pd.date_range(start='1/1/2000', periods=len(leaf_data), freq='D')\n",
    "leaf_data['date'] = date_ts\n",
    "leaf_data['year'] = date_ts.year\n",
    "leaf_data['month'] = date_ts.month\n",
    "leaf_data['day'] = date_ts.day\n",
    "\n",
    "# aggregate the daily observed streamflow to monthly mean\n",
    "df_obs_mth_mean = leaf_data.groupby(['year', 'month']).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-observed-vs-sensitivity-streamflow\">plot_observed_vs_sensitivity_streamflow</a>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = msdbook.plot_observed_vs_sensitivity_streamflow(df_obs=df_obs_mth_mean, \n",
    "                                                     df_sim=df_sim_mth_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='sa_metrics'></a> \n",
    "# 3- Calculation of Sensitivity Analysis Indices\n",
    "\n",
    "There are different options to calculate sensitivity indices. The following section aggregates model streamflow outputs and calculates the sensitivity indices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1 Aggregated sensitivity analysis indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the simplest way of calculating sensitivity analysis metrics, however, averaging all model response can lead to loss of information that we further explore in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall aggregated indices\n",
    "Y = Q_df[sample_column_names].mean().to_numpy()\n",
    "\n",
    "# Perform analysis\n",
    "Si = delta.analyze(problem_hymod, param_values_hymod, Y, print_to_console=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('First order indices = ', Si['S1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Si['S1'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2 How do different performance metrics affect the results of our sensitivity analysis?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Streamflow has many different properties. In this section, we discuss how the selection of metrics can lead to fundamentally different sensitivity analysis results. For example, one can only focus on aggregated streamflow metrics such as mean (what has been presented so far), or only on extreme events such as drought or floods.\n",
    "\n",
    "Here we compare three different metrics:\n",
    "1- Mean error (ME) 2- Root Mean Square Error (RMSE) 3- Log-Root Mean Square Error (Log(RMSE))\n",
    "\n",
    "Each of these metrics focuses on a specific attribute of streamflow. For example, RMSE highlights the impacts of extreme flood events, while LOG(RMSE) focuses on model performance during low-flow events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate error metrics\n",
    "me = Q_df[sample_column_names].apply(lambda x: (x-leaf_data[\"Strmflw\"]), axis=0)\n",
    "mse = Q_df[sample_column_names].apply(lambda x: metrics.mean_squared_error(x, leaf_data[\"Strmflw\"]), axis=0)\n",
    "rmse = mse**(1/2)\n",
    "\n",
    "# add error metrics to a dictionary\n",
    "d_metrics = {'ME': me.mean().values,\n",
    "             'RMSE': rmse.values,\n",
    "             'LOG[RMSE]': np.log10(rmse.values)}\n",
    "\n",
    "# convert to a dataframe\n",
    "df_metrics_SA = pd.DataFrame(d_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following to calculate the SA indices for each metric and visualize it. Results are pre-loaded for efficiency.\n",
    "```python\n",
    "# performance analysis\n",
    "df_metric_sa_result = pd.DataFrame(np.zeros((3, 5)), columns=['Kq', 'Ks', 'Alp', 'Huz', 'B'])\n",
    "\n",
    "# conduct sensitivity analysis for each metric\n",
    "for index, i in enumerate(d_metrics.keys()):\n",
    "    \n",
    "    # get the data as a numpy array for the target metric\n",
    "    Y = d_metrics[i]\n",
    "    \n",
    "    # use the metric to conduct SA\n",
    "    Si = delta.analyze(problem_hymod, param_values_hymod, Y, print_to_console=False)\n",
    "    \n",
    "    # add the sensitivity indices to the output data frame\n",
    "    df_metric_sa_result.iloc[index, :] = Si['S1']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously ran simulation\n",
    "df_metric_sa_result = msdbook.load_hymod_metric_simulation()\n",
    "\n",
    "# view results\n",
    "df_metric_sa_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seaborn heatmap with required labels\n",
    "plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# labels for y-axis\n",
    "y_axis_labels = ['Mean Error', 'RSME', 'Log(RMSE)']\n",
    "\n",
    "# plot heatmap\n",
    "ax = sns.heatmap(df_metric_sa_result, yticklabels=y_axis_labels,  cmap='rocket')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that different goodness-of-fit metrics can produce different sensitivity indices. This is because streamflow time series have several dimensions and regimes (e.g., extreme high flow and low flow) and focusing on only one metric will neglect the sensitivity of other dimensions. \n",
    "\n",
    "Therefore, we can argue that a single goodness-of-fit measure will never be able to capture the entire response of model to different parametric combinations. For more discussion about this topic readers can refer to [Liu and Sun (2010)](https://www.sciencedirect.com/science/article/pii/S1574954110000580?via%3Dihub) and [Foglia et al., (2009)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR007255)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='TVSA'></a> \n",
    "# 4- Time-Varying Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hydrological processes are often state-dependent, meaning that their responses are affected by the time-varying condition that they are in. For example, rainfall-runoff processes are different in winter and summer. These processes are also different during wet years and dry years.\n",
    "\n",
    "Hydrological processes are also path-dependent, meaning that previous time-steps on the model affect the present and future simulation of different hydrologic components. To take these properties into account, we can zoom into different time periods to explore how the sensitivity of model parameters evolve in different time steps. This is referred to as time-varying sensitivity analysis.\n",
    "\n",
    "For more information about time-varying sensitivity analysis, readers can refer to [Herman et al. (2013)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1002/wrcr.20124) and [Xu et al. (2018)](https://link.springer.com/article/10.1007/s12206-018-0223-8)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-1 Sensitivity analysis indices for each month\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate simulated streamflow data to monthly time series\n",
    "df_sim_by_mth_mean = Q_df.groupby('month')[sample_column_names].mean()\n",
    "\n",
    "# aggregate observed streamflow data to monthly time series\n",
    "df_obs_by_mth_mean = leaf_data.groupby('month').mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following to calculate the SA indices for each month and visualize it. Results are pre-loaded for efficiency.\n",
    "```python\n",
    "# set up dataframes to store outputs\n",
    "df_mth_s1 = pd.DataFrame(np.zeros((12,5)), columns=['Kq', 'Ks', 'Alp', 'Huz', 'B'])\n",
    "df_mth_delta = df_mth_s1.copy()\n",
    "\n",
    "# iterate through each month\n",
    "for i in range(0, 12):\n",
    "    \n",
    "    # generate the simulation data\n",
    "    Y = df_sim_by_mth_mean.iloc[i, :].to_numpy()\n",
    "    \n",
    "    # run SA\n",
    "    Si = delta.analyze(problem_hymod, param_values_hymod, Y, print_to_console=False)\n",
    "    \n",
    "    # add to output dataframes\n",
    "    df_mth_s1.iloc[i, :] = np.maximum(Si['S1'], 0)\n",
    "    df_mth_delta.iloc[i, :] = np.maximum(Si['delta'], 0)  \n",
    "    \n",
    "# convert to arrays\n",
    "arr_mth_s1 = df_mth_s1.values\n",
    "arr_mth_delta = df_mth_delta.values\n",
    "```   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-order Indices\n",
    "The following can be used to visualize the time-varying first-order indices. The first order represents the direct impacts of a specific parameter on model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-monthly-heatmap\">plot_monthly_heatmap</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously ran data\n",
    "arr_mth_delta, arr_mth_s1 = msdbook.load_hymod_monthly_simulations()\n",
    "\n",
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_monthly_heatmap(arr_sim=arr_mth_s1.T,\n",
    "                                       df_obs=df_obs_by_mth_mean,\n",
    "                                       title='First Order - Mean Monthly SA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure demonstrates the first order sensitivity indices when the streamflow data are aggregated by month. The purple line represents the observed monthly discharge. The figure indicates that the first order indices are highest for B and Huz across all months and lowest for Alp, Ks, and Kq. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total-order indices\n",
    "We can also focus on the total order sensitivity index that includes first-order SA indices and interactions between parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_monthly_heatmap(arr_sim=arr_mth_delta.T,\n",
    "                                       df_obs=df_obs_by_mth_mean,\n",
    "                                       title='Total Order - Mean monthly SA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notably, the total order sensitivity results are different than the first order sensitivity results, which indicates that interactions between the parameters (particularly in regards to $Kq$, $Ks$, and $Alp$) contribute to variance in the HYMOD output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-2 Annual sensitivity analysis indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by year and get mean\n",
    "df_sim_by_yr_mean = Q_df.groupby(['year'])[sample_column_names].mean()\n",
    "\n",
    "# group input data and get mean\n",
    "df_obs_by_yr_mean = leaf_data.groupby(['year']).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate the sensitivity analysis indices for each individual year. This will allow us to understand if model control changes during different years. The following code first aggregates the outputs to annual time steps, and then calculates the SA indices.\n",
    "```python\n",
    "# set up dataframes to store outputs\n",
    "df_yr_s1 = pd.DataFrame(np.zeros((10, 5)), columns=['Kq', 'Ks', 'Alp', 'Huz', 'B'])\n",
    "df_yr_delta = df_yr_s1.copy()\n",
    "\n",
    "# iterate through each year\n",
    "for i in range(0, 10):\n",
    "    \n",
    "    # generate the simulation data\n",
    "    Y = df_sim_by_yr_mean.iloc[i, :].to_numpy()\n",
    "    \n",
    "    # run SA\n",
    "    Si = delta.analyze(problem_hymod, param_values_hymod, Y, print_to_console=False)\n",
    "    \n",
    "    # add to output dataframes\n",
    "    df_yr_s1.iloc[i, :] = np.maximum(Si['S1'], 0)\n",
    "    df_yr_delta.iloc[i, :] = np.maximum(Si['delta'], 0)  \n",
    "    \n",
    "# convert to arrays\n",
    "arr_yr_s1 = df_mth_s1.values\n",
    "arr_yr_delta = df_mth_delta.values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-order indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-annual-heatmap\">plot_annual_heatmap</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load previously ran data\n",
    "arr_yr_delta, arr_yr_s1 = msdbook.load_hymod_annual_simulations()\n",
    "\n",
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_annual_heatmap(arr_sim=arr_yr_s1.T, \n",
    "                                      df_obs=df_obs_by_yr_mean,\n",
    "                                      title='First Order - Mean Annual SA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order sensitivities at the annual scale are not unlike the first order monthly sensitivities. Once again, sensitivities vary across year and Huz and B are the most consequential parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total-order indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_annual_heatmap(arr_sim=arr_yr_delta.T, \n",
    "                                      df_obs=df_obs_by_yr_mean,\n",
    "                                      title='Total Order - Mean Annual SA and Observed flow')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our results indicate that sensitivity analysis indices vary in different years and now that interactions are included, the Kq, Ks, and Alp variables impact the sensitivity of the streamflow output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-3 Monthly time-varying sensitivity analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although time-varying sensitivity analysis at average monthly and average annual temporal resolutions is informative, TVSA is susceptible to the aggregation issue that we discussed earlier in section 3-2. To avoid that we can further discretize our time domain to zoom into individual months. This will provide us with even more information about model behavior and the sensitivity of different parameters in different states of the system. The block of code demonstrates how to implement the monthly TVSA.\n",
    "```python\n",
    "# set up dataframes to store outputs\n",
    "df_vary_s1 = pd.DataFrame(np.zeros((df_obs_mth_mean.shape[0], 5)), \n",
    "                          columns=['Kq', 'Ks', 'Alp', 'Huz', 'B'])\n",
    "\n",
    "df_vary_delta = df_vary_s1.copy()\n",
    "\n",
    "# iterate through each month\n",
    "for i in range(0, df_obs_mth_mean.shape[0]):\n",
    "    \n",
    "    # generate the simulation data\n",
    "    Y = df_sim_mth_mean.iloc[i, :].to_numpy()\n",
    "    \n",
    "    # run SA\n",
    "    Si = delta.analyze(problem_hymod, param_values_hymod, Y, print_to_console=False)\n",
    "    \n",
    "    # add to output dataframes\n",
    "    df_vary_s1.iloc[i, :] = np.maximum(Si['S1'], 0)\n",
    "    df_vary_delta.iloc[i, :] = np.maximum(Si['delta'], 0)  \n",
    "    \n",
    "# convert to arrays\n",
    "arr_vary_s1 = df_vary_s1.values\n",
    "arr_vary_delta = df_vary_delta.values\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First-order indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-varying-heatmap\">plot_varying_heatmap</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in previously ran data\n",
    "arr_vary_delta, arr_vary_s1 = msdbook.load_hymod_varying_simulations()\n",
    "\n",
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_varying_heatmap(arr_sim=arr_vary_s1.T, \n",
    "                                      df_obs=df_obs_mth_mean,\n",
    "                                      title='First Order - Time-Varying SA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to the TVSA when streamflow was aggregated, this figure suggests that Kq is indeed a relevant parameter for influencing streamflow output when individual months are considered.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total order - time varying sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot figure\n",
    "ax, ax2 = msdbook.plot_varying_heatmap(arr_sim=arr_vary_delta.T, \n",
    "                                      df_obs=df_obs_mth_mean,\n",
    "                                      title='Total Order - Time-Varying SA')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As above, the total order sensitivities further indicate the importance of Kq that is not apparent if aggregation is utilized. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble-based Parametric Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GLUE'></a> \n",
    "# 5- Generalized Likelihood Uncertainty Estimation (GLUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Generalized Likelihood Uncertainty Estimation (GLUE) is an uncertainty analysis algorithm that has been widely used in hydrologic studies. The main argument behind GLUE is rooted in model calibration and the concept of equifinality. Calibration of complex simulation tools such as hydrological models often produces more than one optimal or near-optimal solutions and these solutions have equivalent chances to be chosen [(Beven and Freer, 2001)](https://www.sciencedirect.com/science/article/abs/pii/S0022169401004218). This situation is called equifinality. GLUE provides a methodological framework to handle this problem and consider more than one optimal calibration set.\n",
    "\n",
    "GLUE usually includes the following steps [(Beven and Bineley, 1992)](https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.3360060305?casa_token=o2ooj-6wmC4AAAAA:WpVg1ysAtD59QbSpdHKX6IOjfjeHsOfqxCC6RvoXgiW6bDBRGNfdkOv-AH6h3WhT7-2mD4xmwzMi):\n",
    "\n",
    "1) Definition of a likelihood function  <br>\n",
    "2) Definition of ranges of parameters <br>\n",
    "3) Sensitivity analysis <br>\n",
    "4) Calculating likelihood (goodness-of-fit) values for each model simulation <br>\n",
    "5) Define a threshold and find sample sets that have higher likelihoods than the threshold <br>\n",
    "6) Visualize the sample sets <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-1 Calculation of GLUE metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Likelihood calculation (inverse error variance)\n",
    "There are various likelihood metrics that have been used in previous studies that use GLUE. A widely used example is inverse error variance (IEV; [Vrugt et al. 2009](https://link.springer.com/article/10.1007/s00477-008-0274-y) and [Beven and Bineley, 1992](https://onlinelibrary.wiley.com/doi/abs/10.1002/hyp.3360060305?casa_token=o2ooj-6wmC4AAAAA:WpVg1ysAtD59QbSpdHKX6IOjfjeHsOfqxCC6RvoXgiW6bDBRGNfdkOv-AH6h3WhT7-2mD4xmwzMi)): \n",
    "\n",
    "$$ IEV = {({\\sigma_{e}}^{2})}^{-T} = ({{\\frac{SSR}{n-2}}})^{-T} $$\n",
    "\n",
    "The other metric that can be used as an estimation of likelihood is normalized inverse error variance:\n",
    "$$ NIEV = \\frac{IEV}{\\sum_{i=1}^{n} IEV(i)} $$\n",
    "\n",
    "where *SSR* is the sum of squared residuals; $n$ is the number of samples; and $T$ is an arbitrary coefficient. Low $T$ values lead to equal weights placed on each sample set while higher $T$ values concentrate on the best parameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Vrugt et al (2008) : inverse error variance\n",
    "\n",
    "# T=0 means that we only select the best simulated values that are closer to the observed values \n",
    "#  T=infinity means that all sample sets have the same probability\n",
    "T= 0.70 \n",
    "\n",
    "# calculate metrics from Beven and Binley, 1992\n",
    "df_metrics_SA[\"SSR\"] = df_metrics_SA[\"RMSE\"]**2 * 3650\n",
    "df_metrics_SA[\"InverseErrorVariance\"] = (df_metrics_SA[\"SSR\"] / df_metrics_SA.shape[0])**(-T) \n",
    "df_metrics_SA[\"Normalized_IEV\"] = df_metrics_SA[\"InverseErrorVariance\"] / df_metrics_SA[\"InverseErrorVariance\"].sum()\n",
    "\n",
    "# convert array to dataframe\n",
    "param_values_hymod_df = pd.DataFrame(param_values_hymod, columns=['Kq', 'Ks', 'Alp', 'Huz', 'B'])\n",
    "\n",
    "# combine the metrics and param values dataframes and calculate combined metrics\n",
    "concat_df = pd.concat([df_metrics_SA, param_values_hymod_df], axis=1)\n",
    "concat_df[\"Ks_rescale\"] = concat_df[\"Ks\"] / 0.1\n",
    "concat_df[\"Huz_rescale\"] = concat_df[\"Huz\"] / 500\n",
    "concat_df[\"B_rescale\"] = concat_df[\"B\"] / 2\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "df_metrics_SA.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of likelihoods (inverse error variance) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of the inverse error variance\n",
    "h = sns.histplot(data=df_metrics_SA,\n",
    "                 x=\"InverseErrorVariance\", \n",
    "                 kde=True, \n",
    "                 bins=int(180/5),\n",
    "                 color = 'gold')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of normalized inverse error variance values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot and histogram of the normalized inverse error variance\n",
    "h = sns.histplot(data=df_metrics_SA,\n",
    "                 x=\"Normalized_IEV\", \n",
    "                 kde=True, \n",
    "                 bins=int(180/5), \n",
    "                 color='darkred')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selection of important sample sets and setting a threshold for physical/non-physical sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of important sample sets\n",
    "percentile = 95\n",
    "\n",
    "threshold = np.percentile(concat_df[\"InverseErrorVariance\"], percentile)\n",
    "print(f\"Threshold using the {percentile} percentile:  {threshold}\")\n",
    "\n",
    "# select values greater than the threshold\n",
    "selected_values_glue = concat_df[concat_df[\"InverseErrorVariance\"] > threshold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-2 Visual inspection of GLUE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data frame so that it may be used for plotting\n",
    "to_plot = pd.melt(selected_values_glue,\n",
    "                  id_vars=['ME'], \n",
    "                  value_vars=['Kq', 'Ks_rescale', 'Alp', 'Huz_rescale', 'B_rescale']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a plot with multiple panels of scatter plots where ME is the target metric\n",
    "g = sns.FacetGrid(to_plot, col=\"variable\")\n",
    "\n",
    "# map the scatter plots to the facet grid panels\n",
    "gf = g.map(sns.scatterplot, \"value\", \"ME\",  alpha=0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a plot with multiple panels of histogram plots where ME is the target metric\n",
    "g = sns.FacetGrid(to_plot, col=\"variable\")\n",
    "\n",
    "# map the plots to the facet grid panels\n",
    "gf = g.map(sns.histplot, \"value\", kde=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-3 Comment on the GLUE results\n",
    "\n",
    "Our results suggest that it is challenging to find an clear and interpretable relationship between different selected near-optimal sample sets at least by visual inspection. The main reason for this is that HYMOD includes a complex non-linear system of equations that is also affected by initial conditions and complexity of its input time series. Therefore, it does not have a clear control.\n",
    "\n",
    "Glue has been widely used in hydrology, the original paper has more than 5000 citations. However, the likelihood measure that GLUE uses is not actually a statistically sound likelihood metric and is in fact a goodness-of-fit measure. Therefore, it might not produce valid insights when dealing with situations of non-normality, heteroscedasticity, and serial correlation. For more on these issues reader can refer to [Stedinger et al., (2008)](https://agupubs.onlinelibrary.wiley.com/doi/full/10.1029/2008WR006822%4010.1002/%28ISSN%291944-7973.ASSESS1), [Mantovan and Todini, (2006)](https://www.sciencedirect.com/science/article/pii/S0022169406002162?casa_token=Ml8dhBrO5PkAAAAA:Ake1YuQo0OxK6BaaG-8wIdHa_kd4cuUpm7WiHBFur-G_DlRze6Z0_GkwWH3qHDLKwbJDO9mN), and [Beven And Binley, (2014)](https://onlinelibrary.wiley.com/doi/full/10.1002/hyp.10082)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='precalibration'></a> \n",
    "# 6- Pre-Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-calibration [(Edwards et al, 2010)](https://link.springer.com/article/10.1007/s00382-010-0921-0) is a simplified method to deal with uncertainty in complex environmental models. Pre-calibration can also be thought of as another method that tackles the shortcomings and conceptual challenges involved in calibration of complex environmental models. In pre-calibration instead of finding the best solutions, we focus on finding the sample sets that create outputs that are against the common understanding of the system. These parameter sets are called non-physical parameter sets. In other words, the probability that these parameters are among the best sample sets is zero or near zero and can be neglected in practice.\n",
    "\n",
    " \n",
    "Pre-calibration can include the following steps: <br>\n",
    "1) Sensitivity analysis <br>\n",
    "2) Definition of non-physical boundaries <br>\n",
    "3) Delineating regions in the output space which are non-physical (Implausible) <br>\n",
    "4) Map non-physical sets back to input space <br>\n",
    "5) Interpret the non-physical sample sets <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-1 Pre-calibration calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distribution of mean error, RMSE, and Log[RMSE] under different sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up figure and axis objects\n",
    "fig, axs = plt.subplots(nrows=3, figsize=(14,16))\n",
    "\n",
    "# axis 1 (first row in figure)\n",
    "a = sns.histplot(data=concat_df,\n",
    "                 x=\"RMSE\",\n",
    "                 ax=axs[0],\n",
    "                 kde=True,\n",
    "                 bins=int(180/5), \n",
    "                 color='peachpuff').set_title('Distribution of RMSE in Different Sample Sets')\n",
    "\n",
    "# axis 2 (second row in figure)\n",
    "b = sns.histplot(data=concat_df,\n",
    "                 x=\"ME\",\n",
    "                 ax=axs[1],\n",
    "                 kde=True,\n",
    "                 bins=int(180/5), \n",
    "                 color='lightgreen').set_title('Distribution of ME in Different Sample Sets') \n",
    "\n",
    "# axis 3 (third row in figure)\n",
    "c = sns.histplot(data=concat_df,\n",
    "                 x=\"LOG[RMSE]\",\n",
    "                 ax=axs[2], \n",
    "                 kde=True,\n",
    "                 bins=int(180/5),\n",
    "                 color = 'lightseagreen').set_title('Distribution of LOG[RMSE] in Different Sample Sets')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting a threshold for physical/non-physical sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection of physical/non-physical sample sets\n",
    "percentile = 95\n",
    "\n",
    "threshold_precal = np.percentile(concat_df[\"RMSE\"], percentile)\n",
    "print(f\"Threshold using the {percentile} percentile:  {threshold}\")\n",
    "\n",
    "# select values greater than the threshold\n",
    "selected_values_precal = concat_df[concat_df[\"RMSE\"] > threshold_precal]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual inspection of non-physical sample sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data frame so that it may be used for plotting\n",
    "to_plot_precal = pd.melt(selected_values_precal, \n",
    "                         id_vars=['ME'], \n",
    "                         value_vars=['Kq', 'Ks_rescale', 'Alp', 'Huz_rescale', 'B_rescale']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a plot with multiple panels of scatter plots where ME is the target metric\n",
    "g = sns.FacetGrid(to_plot_precal, col=\"variable\")\n",
    "\n",
    "# map the scatter plots to the facet grid panels\n",
    "gh = g.map(sns.scatterplot, \"value\", \"ME\",  alpha=0.7, color='plum')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a plot with multiple panels of histogram plots where ME is the target metric\n",
    "g = sns.FacetGrid(to_plot_precal, col=\"variable\")\n",
    "\n",
    "# map the plots to the facet grid panels\n",
    "gh = g.map(sns.histplot, \"value\", kde=True, color='plum')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the flow discharge provided by the ensemble of parameters sets from Pre-Calibration versus the observed flow data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-precalibration-flow\">plot_precalibration_flow</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean monthly indices \n",
    "Q_df_precal = pd.concat([Q_df.iloc[:, selected_values_precal.index], \n",
    "                         Q_df[['month' , 'year']]], \n",
    "                        axis=1) \n",
    "\n",
    "# calculate year, month mean\n",
    "df_precal_mth_mean = Q_df_precal.groupby(['year', 'month']).mean()\n",
    "\n",
    "# plot observed versus pre-calibration outputs\n",
    "ax = msdbook.plot_precalibration_flow(df_sim=df_precal_mth_mean,\n",
    "                                      df_obs=df_obs_mth_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following figure shows the flow discharge provided by the ensemble of parameters sets from both pre-calibration and GLUE and how these flows compare to the observed flow data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> View the source code used to create this plot here:  <a href=\"https://immm-sfa.github.io/msd_uncertainty_ebook/A3_plotting_code.html#plot-precalibration-glue\">plot_precalibration_glue</a>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_df_glue = pd.concat([Q_df.iloc[:, selected_values_glue.index], \n",
    "                       Q_df.loc[:,['month' , 'year']]], \n",
    "                      axis=1) \n",
    "\n",
    "# calculate year, month mean\n",
    "df_glue_mth_mean = Q_df_glue.groupby(['year', 'month']).mean()\n",
    "\n",
    "\n",
    "# plot observed versus pre-calibration and GLUE outputs\n",
    "ax = msdbook.plot_precalibration_glue(df_precal=df_precal_mth_mean,\n",
    "                                      df_glue=df_glue_mth_mean,\n",
    "                                      df_obs=df_obs_mth_mean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6-2 Comments on Pre-Calibration\n",
    "\n",
    "Although Pre-calibrations provides a helpful and relatively simple alternative for statistical inferences, it has some disadvantages. For example, it is often subjective and challenging to provide a threshold for non-physical model outputs. Moreover, as discussed earlier different goodness-of-fit metrics can produce distinct physical and non-physical sample sets. \n",
    "\n",
    "More information about pre-calibration can be found in [(Edwards et al., 2010)](https://link.springer.com/article/10.1007/s00382-010-0921-0), and  [Ruckert et al., (2017)](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0170052)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9.4_msd",
   "language": "python",
   "name": "py3.9.4_msd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
