{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Tutorial: Logistic Regression for Factor Mapping \n",
    "\n",
    "In this notebook, we will replicate a scenario discovery analysis performed in [Hadjimichael et al. (2020)](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2020EF001503).\n",
    "\n",
    "Planners in the the Upper Colorado River Basin (UCRB, shown in the figure below) are seeking to understand the vulnerability of water users to uncertainties stemming from climate change, population growth and water policy changes. The UCRB spans 25,682 km<sup>2</sup> in western Colorado and is home to approximately 300,000 residents and 1,012 km<sup>2</sup> of irrigated land. Several thousand irrigation ditches divert water from the main river and its tributaties for irrigation (shown as small black dots in the figure). Transmountain diversions of approximately 567,4000,000 m<sup>3</sup> per year are exported for irrigation, industrial and municipal uses in northern and eastern Colorado, serving the major population centers of Denver and Colorado Springs. These diversions are carried through tunnels, shown as large black dots in the figure.\n",
    "\n",
    "An important planning consideration is the water rights of each user, defined by seniority across all water uses (irrigation diversions, transboundary diversions, power plants etc.) in the basin. To assess the vulnerability of users with varying degrees of water rights seniority, planners simulate the system across an ensemble of scenarios using the state of Colorado's StateMod platform. The model simulates streamflow, diversions, instream demands, and reservoir operations. \n",
    "\n",
    "<img src=\"./figs/basin_map.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadjimichael et al. (2020) employ an exploratory analysis by simulating a large ensemble of plausible scenarios using StateMod and then identifying consequential decision-relevant combinations of uncertain factors, termed scenario discovery. Focusing on decision-relevant metrics, the scenario discovery is applied to the water shortages experienced by each individual user (i.e., not on a single basin-wide or sector-wide metric). For this training example, we'll be performing scenario discovery for three different water users: two irrigation users and one municipal user.\n",
    "\n",
    "## Let's get started!\n",
    "\n",
    "In this tutorial, we will be loading in data that has been produced in Hadjimichael et al. (2020). Before we start our analysis, we'll load the relevant Python libraries, example data, and information for the three users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'msdbook'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e4ac35aacd5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#import libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmsdbook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'msdbook'"
     ]
    }
   ],
   "source": [
    "#import libraries \n",
    "import msdbook\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load example data from Hadjimichael et al. (2020)\n",
    "msdbook.install_package_data()\n",
    "\n",
    "# Select the IDs for the three users that we will perform the analysis for\n",
    "all_IDs = ['7000550','7200799','3704614'] \n",
    "usernames = ['Medium seniority irrigation',\n",
    "             'Low seniority irrigation',\n",
    "             'Transbasin municipal diversion']\n",
    "nStructures = len(all_IDs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Step 1:  Load Latin hypercube sample and set up problem\n",
    "\n",
    "To examine regional vulnerability, we generate an ensemble of plausible future states of the worlds (SOWs) using Latin Hypercube Sampling. For this tutorial, we'll load a file containing 1,000 samples across 14 parameters. The sampled parameters encompass plausible changes to the future state of the basin, including changes to hydrology, water demands (irrigation, municipal & industry, transbasin), and institutional and environmental factors (environmental flows, reservoir storage, operation of the Shoshone Power Plant). These samples are taken from ranges identified in `param_bounds`. Below we load in the 1000 samples, the range of values that the samples can take for each parameter, and the parameter names. More information on what each parameter constitutes can be found in Table 1 of Hadjimichael et al., 2020. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load in the parameter samples \n",
    "LHsamples = msdbook.load_lhs_basin_sample()\n",
    "#Identify the bounds for each of the 14 parameters \n",
    "param_bounds = msdbook.load_basin_param_bounds()\n",
    "\n",
    "#Create an array of the parameter names\n",
    "param_names=['Irrigation demand multiplier','Reservoir loss','Transbasin demand multiplier',\n",
    "             'Municipal & industrial multiplier', 'Shoshone','Environmental flows',\n",
    "             'Evaporation change','Mean dry flow','Dry flow variance',\n",
    "             'Mean wet flow','Wet flow variance','Dry-dry probability',\n",
    "             'Wet-wet probability', 'Snowmelt shift']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define decision-relevant metrics for illustration\n",
    "Scenario discovery attempts to identify parametric regions that lead to 'success' and 'failure'. For this demonstration we'll be defining 'success' as states of the world where a shortage level doesn't exceed its historical frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run the logistic regression\n",
    "Logistic regression estimates the probability that a future SOW will be classified as a success or failure given a set of performance criteria. A logistic regression model is defined by:\n",
    "\n",
    "$$ln \\bigg (\\frac{p_i}{1-p_i} \\bigg ) = X^T_i \\beta$$\n",
    "\n",
    "where $p_i$ is the probability the performance in the $i^{th}$ SOW will be classified as a success, $X_i$ is the vector of covariates describing the $i^{th}$ SOW, and $\\beta$ is the vector of coefficients describing the relationship between the covariates and the response, which here will be estimated using maximum likelihood estimation. \n",
    "\n",
    "A logistic regression model was fit to the ensemble of SOWs using the performance criteria defined in step 2. Logistic regression modeling was conducted using the [Statsmodel Python](https://www.statsmodels.org/stable/index.html) package. The data required for the full analysis is too large to include in this tutorial, but results can be found in the data file loaded below.\n",
    "\n",
    "The results files contain the occurence of different shortage frequency and magnitude combinations under the experiment, in increments of 10, between 0 and 100. These combinations (100 for each user) are alternative decision-relevant metrics that can be used for scenario discovery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set arrays for shortage frequencies and magnitudes\n",
    "frequencies = np.arange(10, 110, 10)\n",
    "magnitudes = np.arange(10, 110, 10)\n",
    "realizations = 10\n",
    "\n",
    "# Load performance and pseudo r scores for each of the users\n",
    "results = [msdbook.load_user_heatmap_array(all_IDs[i]) / 100 for i in range(len(all_IDs))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Factor ranking\n",
    "To rank the importance of each uncertain factor, we utilize McFadden's psuedo-R<sup>2</sup>, a measure that quantifies the improvement of the model when utilizing each given predictor as compared to prediction using the mean of the data set:\n",
    "\n",
    "$$R^2_{McFadden}=1-\\frac{ln \\hat{L}(M_{full})}{ln \\hat{L}(M_{intercept})}$$\n",
    "\n",
    "Here $ln \\hat{L}(M_{full})$ is the log likelihood of the full model (including the predictor) and $ln \\hat{L}(M_{intercept})$ is the log likelihood of the intercept model (which predicts the mean probability of success across all SOWs). \n",
    "\n",
    "Higher values of McFadden's psuedo-R<sup>2</sup> indicate higher factor importance (when the likelihood of the full model approaches one, the ratio of the likelihood of the full model compared to the intercept model will get very small).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Load the pseudo-R^2 scores\n",
    "scores = [msdbook.load_user_pseudo_scores(all_IDs[i]) for i in range(len(all_IDs))]\n",
    "\n",
    "# Select indices of frequency and magnitudes that will be used for the visualization\n",
    "freq = [1,0,0]\n",
    "mag = [7,3,7]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Draw factor maps\n",
    "\n",
    "The McFadden's psuedo-R<sup>2</sup> scores files contain preliminary logistic regression results on parameter importance for each of these combinations. Using these psuedo-R<sup>2</sup> scores we will identify the two most important factors for each metric which we'll use to generate the final scenario discovery maps (note: there may be more than two important metrics for each user, but here we will demonstrate by mapping two)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# setup figure\n",
    "fig, axes = plt.subplots(3,1, figsize=(6,18), tight_layout=True)\n",
    "fig.patch.set_facecolor('white')\n",
    "\n",
    "for i in range(len(axes.flat)):\n",
    "    \n",
    "    ax = axes.flat[i]\n",
    "    \n",
    "    allSOWsperformance = results[i]\n",
    "    all_pseudo_r_scores = scores[i]\n",
    "    \n",
    "    # construct dataframe\n",
    "    dta = pd.DataFrame(data=np.repeat(LHsamples, realizations, axis = 0), columns=param_names)\n",
    "    dta['Success'] = allSOWsperformance[freq[i],mag[i],:]\n",
    "    \n",
    "    pseudo_r_scores = all_pseudo_r_scores[str(frequencies[freq[i]])+'yrs_'+str(magnitudes[mag[i]])+'prc'].values\n",
    "    top_predictors = np.argsort(pseudo_r_scores)[::-1][:2] #Sort scores and pick top 2 predictors\n",
    "    \n",
    "    # define color map for dots representing SOWs in which the policy\n",
    "    # succeeds (light blue) and fails (dark red)\n",
    "    dot_cmap = mpl.colors.ListedColormap(np.array([[227,26,28],[166,206,227]])/255.0)\n",
    "    \n",
    "    # define color map for probability contours\n",
    "    contour_cmap = mpl.cm.get_cmap('RdBu')\n",
    "    \n",
    "    # define probability contours\n",
    "    contour_levels = np.arange(0.0, 1.05,0.1)\n",
    "    \n",
    "    # define base values of the predictors\n",
    "    SOW_values = np.array([1,1,1,1,0,0,1,1,1,1,1,0,0,0]) # default parameter values for base SOW\n",
    "    base = SOW_values[top_predictors]\n",
    "    ranges = param_bounds[top_predictors]\n",
    "    \n",
    "    # define grid of x (1st predictor), and y (2nd predictor) dimensions\n",
    "    # to plot contour map over\n",
    "    xgrid = np.arange(param_bounds[top_predictors[0]][0],\n",
    "                      param_bounds[top_predictors[0]][1], np.around((ranges[0][1]-ranges[0][0])/500,decimals=4))\n",
    "    ygrid = np.arange(param_bounds[top_predictors[1]][0],\n",
    "                      param_bounds[top_predictors[1]][1], np.around((ranges[1][1]-ranges[1][0])/500,decimals=4))\n",
    "    all_predictors = [ dta.columns.tolist()[i] for i in top_predictors]\n",
    "    dta['Interaction'] = dta[all_predictors[0]]*dta[all_predictors[1]]\n",
    "    \n",
    "    # logistic regression here\n",
    "    predictor_list = [all_predictors[i] for i in [0,1]]\n",
    "    result = msdbook.fit_logit(dta, predictor_list)\n",
    "    \n",
    "    # plot contour map\n",
    "    contourset = msdbook.plot_contour_map(ax, result, dta, contour_cmap,\n",
    "                                          dot_cmap, contour_levels, xgrid,\n",
    "                                          ygrid, all_predictors[0], all_predictors[1], base)\n",
    "    \n",
    "    ax.set_title(usernames[i])\n",
    "    \n",
    "# set up colorbar\n",
    "cbar_ax = fig.add_axes([0.98, 0.15, 0.05, 0.7])\n",
    "cbar = fig.colorbar(contourset, cax=cbar_ax)\n",
    "cbar_ax.set_ylabel('Probability of Success', fontsize=16)\n",
    "cbar_ax.tick_params(axis='y', which='major', labelsize=12)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above demonstrates how different combinations of the uncertain factors lead to success or failure in different states of the world, which are denoted by the blue and red dots respectively. The probability of success and failure are further denoted by the contours in the figure. Several insights can be drawn from this figure.\n",
    "\n",
    "First, using metrics chosen to be decision-relevant (specific to each\n",
    "user) causes different factors to be identified as most important by this scenario-discovery\n",
    "exercise (the x- and y-axes for each of the subplots). In other words, depending\n",
    "on what the decision makers of this system want to prioritize they might choose to\n",
    "monitor different uncertain factors to track performance.\n",
    "\n",
    "Second, in the top panel, the two identified factors appear to also have an interactive\n",
    "effect on the metric used (shortages of a certain level and frequency in this example).\n",
    "In terms of scenario discovery, the Patient Rule Induction Method (PRIM) or Classification\n",
    "And Regression Trees (CART) would not be able to delineate this non-linear space and\n",
    "would therefore misclassify parameter combinations as 'desirable' when they were in\n",
    "fact undesirable, and vice versa.\n",
    "\n",
    "Lastly, logistic regression also produces contours of probability of success, i.e.\n",
    "different factor-value combinations are assigned different probabilities that a\n",
    "shortage level will be exceeded. This allows the decision makers to evaluate these\n",
    "insights while considering their risk aversion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips to Apply Scenario Discovery to Your Own Problem \n",
    "\n",
    "In this tutorial, we demonstrated how to perform a scenario discovery analysis for three different users in the UCRB. The analysis allowed us to determine which parameters that the users would be most affected by and to visualize how different ranges of these parameters lead to success and failure for different users. This framework can be applicable to any other application where it is of interest to characterize success and failure based on uncertain parameter ranges. In order to apply the same framework to your own problem:\n",
    "\n",
    "1. Choose sampling bounds for your parameters of interest, which will represent uncertainties that characterize your system. \n",
    "2. Generate samples for these parameters (this can be done using the `saltelli.sample` function or externally).\n",
    "3. Define what constitutes success and failure in your problem. In this tutorial, success was defined based on not surpassing the historical drought frequency. Choose a metric that is relevant to your problem and decision-makers that might be involved. If your model involves an optimization, you can also define metrics based on meeting certain values of these objectives.    \n",
    "3. Run the parameter sets through your model and calculate success and failure based on your metrics and across different users if applicable. This step will allow you to create the scatter plot part of the final figure.\n",
    "4. If it is of interest, the contours on the figure can be created by fitting the logistic regression model in a similiar manner as denoted in Steps 3 and 5 of the tutorial.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
